[
  {
    "id": 1,
    "question": "What is the mean of the numbers: 2, 4, 6, 8, 10?",
    "options": {
      "A": "4",
      "B": "6",
      "C": "8",
      "D": "10"
    },
    "answer": "B",
    "explanation": "The mean is the sum of all numbers divided by the count of numbers: (2+4+6+8+10)/5 = 30/5 = 6.",
    "topic": "Measures of Central Tendency",
    "difficulty": "Easy"
  },
  {
    "id": 2,
    "question": "What is the median of the numbers: 1, 3, 5, 7, 9?",
    "options": {
      "A": "1",
      "B": "3",
      "C": "5",
      "D": "7"
    },
    "answer": "C",
    "explanation": "The median is the middle value when the numbers are arranged in order. In this case, 5 is the middle value.",
    "topic": "Measures of Central Tendency",
    "difficulty": "Easy"
  },
  {
    "id": 3,
    "question": "What is the mode of the numbers: 2, 3, 3, 4, 5?",
    "options": {
      "A": "2",
      "B": "3",
      "C": "4",
      "D": "5"
    },
    "answer": "B",
    "explanation": "The mode is the number that appears most frequently in the dataset. Here, 3 appears twice.",
    "topic": "Measures of Central Tendency",
    "difficulty": "Easy"
  },
  {
    "id": 4,
    "question": "If you flip a fair coin, what is the probability of getting heads?",
    "options": {
      "A": "0",
      "B": "0.25",
      "C": "0.5",
      "D": "1"
    },
    "answer": "C",
    "explanation": "A fair coin has two equally likely outcomes (heads or tails), so the probability of heads is 1/2 or 0.5.",
    "topic": "Basic Probability",
    "difficulty": "Easy"
  },
  {
    "id": 5,
    "question": "What is a bar graph primarily used for?",
    "options": {
      "A": "Showing trends over time",
      "B": "Comparing different categories of data",
      "C": "Displaying parts of a whole",
      "D": "Illustrating relationships between two variables"
    },
    "answer": "B",
    "explanation": "Bar graphs are effective for comparing discrete categories or groups.",
    "topic": "Data Visualization",
    "difficulty": "Easy"
  },
  {
    "id": 6,
    "question": "What is a pie chart primarily used for?",
    "options": {
      "A": "Comparing different categories of data",
      "B": "Showing trends over time",
      "C": "Displaying parts of a whole",
      "D": "Illustrating relationships between two variables"
    },
    "answer": "C",
    "explanation": "Pie charts are best for showing the proportion of different categories relative to a whole.",
    "topic": "Data Visualization",
    "difficulty": "Easy"
  },
  {
    "id": 7,
    "question": "What is the range of the numbers: 10, 12, 15, 18, 20?",
    "options": {
      "A": "5",
      "B": "8",
      "C": "10",
      "D": "12"
    },
    "answer": "C",
    "explanation": "The range is the difference between the maximum and minimum values: 20 - 10 = 10.",
    "topic": "Measures of Dispersion",
    "difficulty": "Easy"
  },
  {
    "id": 8,
    "question": "True or False: The sum of probabilities of all possible outcomes in an event space is always 1.",
    "options": {
      "A": "True",
      "B": "False"
    },
    "answer": "A",
    "explanation": "This is a fundamental rule of probability. All possible outcomes must sum to 1.",
    "topic": "Basic Probability",
    "difficulty": "Easy"
  },
  {
    "id": 9,
    "question": "What is the primary purpose of descriptive statistics?",
    "options": {
      "A": "To make predictions about a population",
      "B": "To test hypotheses",
      "C": "To summarize and describe data",
      "D": "To determine cause and effect relationships"
    },
    "answer": "C",
    "explanation": "Descriptive statistics focus on organizing, summarizing, and presenting data in an informative way.",
    "topic": "Types of Statistics",
    "difficulty": "Easy"
  },
  {
    "id": 10,
    "question": "What is the primary purpose of inferential statistics?",
    "options": {
      "A": "To summarize large datasets",
      "B": "To create visual representations of data",
      "C": "To make inferences or predictions about a population based on a sample",
      "D": "To calculate measures of central tendency"
    },
    "answer": "C",
    "explanation": "Inferential statistics use sample data to draw conclusions or make predictions about a larger population.",
    "topic": "Types of Statistics",
    "difficulty": "Easy"
  },
  {
    "id": 11,
    "question": "Which of the following is an example of qualitative data?",
    "options": {
      "A": "Height in centimeters",
      "B": "Number of siblings",
      "C": "Hair color",
      "D": "Age in years"
    },
    "answer": "C",
    "explanation": "Qualitative data describes qualities or characteristics, like hair color, that cannot be measured numerically.",
    "topic": "Types of Data",
    "difficulty": "Easy"
  },
  {
    "id": 12,
    "question": "Which of the following is an example of quantitative data?",
    "options": {
      "A": "Favorite food",
      "B": "Marital status",
      "C": "Temperature in Celsius",
      "D": "Type of car"
    },
    "answer": "C",
    "explanation": "Quantitative data represents numerical values that can be measured, like temperature.",
    "topic": "Types of Data",
    "difficulty": "Easy"
  },
  {
    "id": 13,
    "question": "What is the relationship between a population and a sample?",
    "options": {
      "A": "A sample includes all members of a population.",
      "B": "A population is a subset of a sample.",
      "C": "A sample is a subset of a population.",
      "D": "They are unrelated concepts."
    },
    "answer": "C",
    "explanation": "A sample is a smaller, manageable group selected from the larger population for study.",
    "topic": "Sampling",
    "difficulty": "Easy"
  },
  {
    "id": 14,
    "question": "What defines a random sample?",
    "options": {
      "A": "Only specific individuals are chosen.",
      "B": "Every member of the population has an equal chance of being selected.",
      "C": "The researcher picks participants based on convenience.",
      "D": "Participants volunteer for the study."
    },
    "answer": "B",
    "explanation": "Random sampling ensures that each member has an equal probability of inclusion, reducing bias.",
    "topic": "Sampling",
    "difficulty": "Easy"
  },
  {
    "id": 15,
    "question": "What Greek letter typically symbolizes the population mean?",
    "options": {
      "A": "σ (sigma)",
      "B": "μ (mu)",
      "C": "ρ (rho)",
      "D": "β (beta)"
    },
    "answer": "B",
    "explanation": "The Greek letter μ (mu) is the standard notation for the population mean.",
    "topic": "Notation",
    "difficulty": "Easy"
  },
  {
    "id": 16,
    "question": "What symbol typically represents the sample mean?",
    "options": {
      "A": "μ (mu)",
      "B": "x̄ (x-bar)",
      "C": "s (s)",
      "D": "p (p)"
    },
    "answer": "B",
    "explanation": "The symbol x̄ (x-bar) is commonly used to denote the sample mean.",
    "topic": "Notation",
    "difficulty": "Easy"
  },
  {
    "id": 17,
    "question": "Which measure of central tendency is most affected by outliers?",
    "options": {
      "A": "Mean",
      "B": "Median",
      "C": "Mode",
      "D": "All are equally affected"
    },
    "answer": "A",
    "explanation": "The mean is calculated using all values, so extreme values (outliers) can significantly pull it in one direction.",
    "topic": "Measures of Central Tendency",
    "difficulty": "Easy"
  },
  {
    "id": 18,
    "question": "Which measure indicates how spread out the data points are from the mean?",
    "options": {
      "A": "Median",
      "B": "Mode",
      "C": "Standard Deviation",
      "D": "Range"
    },
    "answer": "C",
    "explanation": "Standard deviation quantifies the amount of variation or dispersion of a set of data values.",
    "topic": "Measures of Dispersion",
    "difficulty": "Easy"
  },
  {
    "id": 19,
    "question": "What is a dataset called if it has two modes?",
    "options": {
      "A": "Unimodal",
      "B": "Bimodal",
      "C": "Multimodal",
      "D": "Amodal"
    },
    "answer": "B",
    "explanation": "Bimodal means having two modes, i.e., two values that appear with the highest frequency.",
    "topic": "Measures of Central Tendency",
    "difficulty": "Easy"
  },
  {
    "id": 20,
    "question": "What is a frequency distribution?",
    "options": {
      "A": "A graph showing only the mean of data",
      "B": "A table or graph that shows how often each value or range of values occurs in a dataset",
      "C": "A calculation of the standard deviation",
      "D": "A list of raw data points"
    },
    "answer": "B",
    "explanation": "A frequency distribution summarizes the frequency of each distinct value or interval of values.",
    "topic": "Data Organization",
    "difficulty": "Easy"
  },
  {
    "id": 21,
    "question": "What type of graph is best suited for displaying the distribution of a quantitative variable?",
    "options": {
      "A": "Bar graph",
      "B": "Pie chart",
      "C": "Histogram",
      "D": "Line graph"
    },
    "answer": "C",
    "explanation": "Histograms show the frequency distribution of continuous data, grouping data into bins.",
    "topic": "Data Visualization",
    "difficulty": "Easy"
  },
  {
    "id": 22,
    "question": "Which formula calculates the mean of a set of numbers?",
    "options": {
      "A": "Highest value - Lowest value",
      "B": "Sum of all values / Number of values",
      "C": "Middle value when ordered",
      "D": "Most frequent value"
    },
    "answer": "B",
    "explanation": "The mean (average) is calculated by summing all values and dividing by the total count of values.",
    "topic": "Measures of Central Tendency",
    "difficulty": "Easy"
  },
  {
    "id": 23,
    "question": "How is the median found when there is an even number of data points?",
    "options": {
      "A": "It's the first value.",
      "B": "It's the last value.",
      "C": "It's the average of the two middle values.",
      "D": "It doesn't exist for an even number of points."
    },
    "answer": "C",
    "explanation": "When there's an even number of data points, the median is the average of the two central values after sorting.",
    "topic": "Measures of Central Tendency",
    "difficulty": "Easy"
  },
  {
    "id": 24,
    "question": "What is a data point that is significantly different from other observations?",
    "options": {
      "A": "Mean",
      "B": "Median",
      "C": "Outlier",
      "D": "Mode"
    },
    "answer": "C",
    "explanation": "An outlier is an observation point that is distant from other observations.",
    "topic": "Data Analysis",
    "difficulty": "Easy"
  },
  {
    "id": 25,
    "question": "What is a box plot primarily used for?",
    "options": {
      "A": "Showing trends over time",
      "B": "Displaying the distribution of numerical data and detecting outliers",
      "C": "Comparing discrete categories",
      "D": "Illustrating relationships between two variables"
    },
    "answer": "B",
    "explanation": "Box plots provide a visual summary of the five-number summary (minimum, Q1, median, Q3, maximum) and show potential outliers.",
    "topic": "Data Visualization",
    "difficulty": "Easy"
  },
  {
    "id": 26,
    "question": "What does IQR stand for?",
    "options": {
      "A": "Inter Quartile Range",
      "B": "Inner Quality Ratio",
      "C": "Interval Quantification Rate",
      "D": "Indexed Quantile Report"
    },
    "answer": "A",
    "explanation": "IQR stands for InterQuartile Range, which is a measure of statistical dispersion.",
    "topic": "Measures of Dispersion",
    "difficulty": "Easy"
  },
  {
    "id": 27,
    "question": "What is the first quartile (Q1) also known as?",
    "options": {
      "A": "The median",
      "B": "The 75th percentile",
      "C": "The 25th percentile",
      "D": "The maximum value"
    },
    "answer": "C",
    "explanation": "The first quartile (Q1) marks the point below which 25% of the data falls.",
    "topic": "Measures of Position",
    "difficulty": "Easy"
  },
  {
    "id": 28,
    "question": "What is the third quartile (Q3) also known as?",
    "options": {
      "A": "The median",
      "B": "The 75th percentile",
      "C": "The 25th percentile",
      "D": "The minimum value"
    },
    "answer": "B",
    "explanation": "The third quartile (Q3) marks the point below which 75% of the data falls.",
    "topic": "Measures of Position",
    "difficulty": "Easy"
  },
  {
    "id": 29,
    "question": "Which formula correctly defines the range of a dataset?",
    "options": {
      "A": "Mean - Mode",
      "B": "Median - Mean",
      "C": "Maximum Value - Minimum Value",
      "D": "Sum of all values"
    },
    "answer": "C",
    "explanation": "The range is the simplest measure of dispersion, found by subtracting the lowest value from the highest value.",
    "topic": "Measures of Dispersion",
    "difficulty": "Easy"
  },
  {
    "id": 30,
    "question": "What does variance conceptually measure?",
    "options": {
      "A": "The middle value of a dataset",
      "B": "The average of the squared differences from the mean",
      "C": "The most frequent value in a dataset",
      "D": "The difference between the highest and lowest values"
    },
    "answer": "B",
    "explanation": "Variance provides a measure of how much the data points deviate from the mean, in squared units.",
    "topic": "Measures of Dispersion",
    "difficulty": "Easy"
  },
  {
    "id": 31,
    "question": "What does standard deviation conceptually represent?",
    "options": {
      "A": "The average value of the data",
      "B": "The typical distance of data points from the mean",
      "C": "The number of times a value appears",
      "D": "The spread of data between quartiles"
    },
    "answer": "B",
    "explanation": "Standard deviation is the square root of the variance and indicates the typical spread of data points around the mean.",
    "topic": "Measures of Dispersion",
    "difficulty": "Easy"
  },
  {
    "id": 32,
    "question": "If the standard deviation of a dataset is 0, what does this imply about the data points?",
    "options": {
      "A": "All data points are different.",
      "B": "All data points are the same.",
      "C": "The mean is 0.",
      "D": "There are no data points."
    },
    "answer": "B",
    "explanation": "A standard deviation of 0 means there is no variation in the data; all values are identical.",
    "topic": "Measures of Dispersion",
    "difficulty": "Easy"
  },
  {
    "id": 33,
    "question": "What is the probability of rolling a 3 on a standard six-sided die?",
    "options": {
      "A": "1/2",
      "B": "1/3",
      "C": "1/6",
      "D": "1"
    },
    "answer": "C",
    "explanation": "There is one favorable outcome (rolling a 3) out of six possible outcomes (1, 2, 3, 4, 5, 6).",
    "topic": "Basic Probability",
    "difficulty": "Easy"
  },
  {
    "id": 34,
    "question": "What is the probability of rolling an even number on a standard six-sided die?",
    "options": {
      "A": "1/6",
      "B": "1/3",
      "C": "1/2",
      "D": "2/3"
    },
    "answer": "C",
    "explanation": "There are three even numbers (2, 4, 6) out of six possible outcomes, so 3/6 = 1/2.",
    "topic": "Basic Probability",
    "difficulty": "Easy"
  },
  {
    "id": 35,
    "question": "What type of events cannot occur at the same time?",
    "options": {
      "A": "Independent events",
      "B": "Dependent events",
      "C": "Mutually exclusive events",
      "D": "Complementary events"
    },
    "answer": "C",
    "explanation": "Mutually exclusive events are those that cannot happen simultaneously (e.g., rolling a 1 and a 2 on a single die roll).",
    "topic": "Types of Events",
    "difficulty": "Easy"
  },
  {
    "id": 36,
    "question": "What type of events have no effect on the probability of each other occurring?",
    "options": {
      "A": "Mutually exclusive events",
      "B": "Dependent events",
      "C": "Conditional events",
      "D": "Independent events"
    },
    "answer": "D",
    "explanation": "Independent events are those where the outcome of one event does not influence the outcome of another (e.g., flipping a coin twice).",
    "topic": "Types of Events",
    "difficulty": "Easy"
  },
  {
    "id": 37,
    "question": "If events A and B are independent, what is the formula for P(A and B)?",
    "options": {
      "A": "P(A) + P(B)",
      "B": "P(A) * P(B)",
      "C": "P(A) / P(B)",
      "D": "P(A) - P(B)"
    },
    "answer": "B",
    "explanation": "For independent events, the probability of both occurring is the product of their individual probabilities.",
    "topic": "Probability Rules",
    "difficulty": "Easy"
  },
  {
    "id": 38,
    "question": "What is the addition rule for mutually exclusive events A and B?",
    "options": {
      "A": "P(A and B) = P(A) * P(B)",
      "B": "P(A or B) = P(A) + P(B)",
      "C": "P(A or B) = P(A) + P(B) - P(A and B)",
      "D": "P(A|B) = P(A)"
    },
    "answer": "B",
    "explanation": "If events are mutually exclusive, the probability of A or B occurring is simply the sum of their individual probabilities.",
    "topic": "Probability Rules",
    "difficulty": "Easy"
  },
  {
    "id": 39,
    "question": "What is the term for all outcomes that are not in a given event?",
    "options": {
      "A": "Joint event",
      "B": "Intersection",
      "C": "Complement of an event",
      "D": "Union"
    },
    "answer": "C",
    "explanation": "The complement of an event A (denoted A') includes all outcomes in the sample space that are not in A.",
    "topic": "Probability Concepts",
    "difficulty": "Easy"
  },
  {
    "id": 40,
    "question": "If the probability of event A is 0.7, what is the probability of the complement of A?",
    "options": {
      "A": "0.3",
      "B": "0.7",
      "C": "0.5",
      "D": "1.0"
    },
    "answer": "A",
    "explanation": "P(A') = 1 - P(A) = 1 - 0.7 = 0.3.",
    "topic": "Probability Concepts",
    "difficulty": "Easy"
  },
  {
    "id": 41,
    "question": "What type of diagram is useful for visualizing the relationships between sets and their elements?",
    "options": {
      "A": "Bar chart",
      "B": "Line graph",
      "C": "Venn diagram",
      "D": "Scatter plot"
    },
    "answer": "C",
    "explanation": "Venn diagrams use overlapping circles to show relationships between sets and illustrate concepts like union and intersection.",
    "topic": "Data Visualization",
    "difficulty": "Easy"
  },
  {
    "id": 42,
    "question": "What is the sample space for rolling a single standard six-sided die?",
    "options": {
      "A": "{1, 2, 3}",
      "B": "{1, 2, 3, 4, 5, 6}",
      "C": "{Heads, Tails}",
      "D": "{Even, Odd}"
    },
    "answer": "B",
    "explanation": "The sample space includes all possible outcomes when rolling a die: 1, 2, 3, 4, 5, 6.",
    "topic": "Probability Concepts",
    "difficulty": "Easy"
  },
  {
    "id": 43,
    "question": "What is the sample space for flipping two fair coins?",
    "options": {
      "A": "{H, T}",
      "B": "{HH, TT}",
      "C": "{HH, HT, TH, TT}",
      "D": "{H, T, H, T}"
    },
    "answer": "C",
    "explanation": "The possible outcomes are Head-Head, Head-Tail, Tail-Head, Tail-Tail.",
    "topic": "Probability Concepts",
    "difficulty": "Easy"
  },
  {
    "id": 44,
    "question": "Which term refers to the number of ways to arrange items where order matters?",
    "options": {
      "A": "Combination",
      "B": "Permutation",
      "C": "Factorial",
      "D": "Probability"
    },
    "answer": "B",
    "explanation": "A permutation is an arrangement of items in a specific order.",
    "topic": "Combinatorics",
    "difficulty": "Easy"
  },
  {
    "id": 45,
    "question": "Which term refers to the number of ways to select items where order does not matter?",
    "options": {
      "A": "Permutation",
      "B": "Combination",
      "C": "Arrangement",
      "D": "Sequence"
    },
    "answer": "B",
    "explanation": "A combination is a selection of items where the order of selection is not important.",
    "topic": "Combinatorics",
    "difficulty": "Easy"
  },
  {
    "id": 46,
    "question": "In which scenario does the order of selection matter?",
    "options": {
      "A": "Choosing 3 toppings for a pizza",
      "B": "Selecting 2 students for a committee",
      "C": "Arranging 5 books on a shelf",
      "D": "Picking 4 lottery numbers"
    },
    "answer": "C",
    "explanation": "Arranging books on a shelf implies a specific order, making it a permutation problem.",
    "topic": "Combinatorics",
    "difficulty": "Easy"
  },
  {
    "id": 47,
    "question": "How many ways can you arrange 3 distinct books on a shelf?",
    "options": {
      "A": "1",
      "B": "3",
      "C": "6",
      "D": "9"
    },
    "answer": "C",
    "explanation": "This is 3! (3 factorial) = 3 * 2 * 1 = 6.",
    "topic": "Combinatorics",
    "difficulty": "Easy"
  },
  {
    "id": 48,
    "question": "How many different pairs can you choose from a group of 4 people?",
    "options": {
      "A": "2",
      "B": "4",
      "C": "6",
      "D": "12"
    },
    "answer": "C",
    "explanation": "This is a combination: C(4, 2) = 4! / (2! * (4-2)!) = 24 / (2 * 2) = 6.",
    "topic": "Combinatorics",
    "difficulty": "Easy"
  },
  {
    "id": 49,
    "question": "What is the mean of 1, 2, 3, 4, 5?",
    "options": {
      "A": "2",
      "B": "3",
      "C": "4",
      "D": "5"
    },
    "answer": "B",
    "explanation": "(1+2+3+4+5)/5 = 15/5 = 3.",
    "topic": "Measures of Central Tendency",
    "difficulty": "Easy"
  },
  {
    "id": 50,
    "question": "What is the median of 1, 2, 3, 4, 5?",
    "options": {
      "A": "1",
      "B": "2",
      "C": "3",
      "D": "4"
    },
    "answer": "C",
    "explanation": "The middle value in the sorted list is 3.",
    "topic": "Measures of Central Tendency",
    "difficulty": "Easy"
  },
  {
    "id": 51,
    "question": "What is the mode of 1, 1, 2, 3, 3, 3, 4, 5?",
    "options": {
      "A": "1",
      "B": "2",
      "C": "3",
      "D": "5"
    },
    "answer": "C",
    "explanation": "The number 3 appears most frequently (three times).",
    "topic": "Measures of Central Tendency",
    "difficulty": "Easy"
  },
  {
    "id": 52,
    "question": "If a bag contains 5 red balls and 5 blue balls, what is the probability of picking a red ball?",
    "options": {
      "A": "0.25",
      "B": "0.5",
      "C": "0.75",
      "D": "1"
    },
    "answer": "B",
    "explanation": "There are 5 red balls out of a total of 10 balls, so 5/10 = 0.5.",
    "topic": "Basic Probability",
    "difficulty": "Easy"
  },
  {
    "id": 53,
    "question": "What is the term for data collected directly by the researcher for a specific purpose?",
    "options": {
      "A": "Secondary data",
      "B": "Archival data",
      "C": "Primary data",
      "D": "Public data"
    },
    "answer": "C",
    "explanation": "Primary data is original data collected for the first time.",
    "topic": "Data Collection",
    "difficulty": "Easy"
  },
  {
    "id": 54,
    "question": "What is the term for data that has been collected by someone else for a different purpose?",
    "options": {
      "A": "Primary data",
      "B": "Secondary data",
      "C": "Experimental data",
      "D": "Observational data"
    },
    "answer": "B",
    "explanation": "Secondary data is data that has been collected by someone other than the user.",
    "topic": "Data Collection",
    "difficulty": "Easy"
  },
  {
    "id": 55,
    "question": "What type of variable can only take on a finite number of distinct values or an infinite number of countable values?",
    "options": {
      "A": "Continuous variable",
      "B": "Qualitative variable",
      "C": "Discrete variable",
      "D": "Nominal variable"
    },
    "answer": "C",
    "explanation": "Discrete variables are countable, such as the number of students in a class.",
    "topic": "Types of Variables",
    "difficulty": "Easy"
  },
  {
    "id": 56,
    "question": "What type of variable can take any value within a given range?",
    "options": {
      "A": "Discrete variable",
      "B": "Ordinal variable",
      "C": "Continuous variable",
      "D": "Categorical variable"
    },
    "answer": "C",
    "explanation": "Continuous variables can take on any value within a given interval, often involving measurements like height or weight.",
    "topic": "Types of Variables",
    "difficulty": "Easy"
  },
  {
    "id": 57,
    "question": "Is the number of children in a family discrete or continuous?",
    "options": {
      "A": "Discrete",
      "B": "Continuous"
    },
    "answer": "A",
    "explanation": "You can count the number of children; it's a whole number.",
    "topic": "Types of Variables",
    "difficulty": "Easy"
  },
  {
    "id": 58,
    "question": "Is the height of a person discrete or continuous?",
    "options": {
      "A": "Discrete",
      "B": "Continuous"
    },
    "answer": "B",
    "explanation": "Height can take on any value within a range (e.g., 170.5 cm, 170.55 cm), making it continuous.",
    "topic": "Types of Variables",
    "difficulty": "Easy"
  },
  {
    "id": 59,
    "question": "What is a sampling frame?",
    "options": {
      "A": "The method used to select a sample.",
      "B": "A list of all units in the population from which a sample is drawn.",
      "C": "The statistical analysis plan.",
      "D": "The report of the findings."
    },
    "answer": "B",
    "explanation": "A sampling frame is an actual list of individuals from which a sample can be drawn.",
    "topic": "Sampling",
    "difficulty": "Easy"
  },
  {
    "id": 60,
    "question": "What is the main advantage of using a sample instead of a population?",
    "options": {
      "A": "It's always more accurate.",
      "B": "It's cheaper and faster to collect data.",
      "C": "It eliminates all bias.",
      "D": "It guarantees perfect representation."
    },
    "answer": "B",
    "explanation": "Collecting data from an entire population is often impractical, costly, and time-consuming; samples offer efficiency.",
    "topic": "Sampling",
    "difficulty": "Easy"
  },
  {
    "id": 61,
    "question": "Which type of sampling involves dividing the population into homogenous subgroups and then taking a simple random sample from each subgroup?",
    "options": {
      "A": "Simple random sampling",
      "B": "Stratified sampling",
      "C": "Cluster sampling",
      "D": "Convenience sampling"
    },
    "answer": "B",
    "explanation": "Stratified sampling ensures representation from various subgroups within the population.",
    "topic": "Sampling Methods",
    "difficulty": "Easy"
  },
  {
    "id": 62,
    "question": "What is a parameter?",
    "options": {
      "A": "A characteristic of a sample",
      "B": "A characteristic of a population",
      "C": "A type of statistical test",
      "D": "A data point"
    },
    "answer": "B",
    "explanation": "A parameter is a descriptive measure of a population (e.g., population mean, population standard deviation).",
    "topic": "Notation",
    "difficulty": "Easy"
  },
  {
    "id": 63,
    "question": "What is a statistic?",
    "options": {
      "A": "A characteristic of a population",
      "B": "A characteristic of a sample",
      "C": "A type of data visualization",
      "D": "A conclusion drawn from data"
    },
    "answer": "B",
    "explanation": "A statistic is a descriptive measure calculated from sample data (e.g., sample mean, sample standard deviation).",
    "topic": "Notation",
    "difficulty": "Easy"
  },
  {
    "id": 64,
    "question": "What type of data scale has a true zero point and allows for meaningful ratios (e.g., height, weight)?",
    "options": {
      "A": "Nominal",
      "B": "Ordinal",
      "C": "Interval",
      "D": "Ratio"
    },
    "answer": "D",
    "explanation": "Ratio scale data has all the properties of interval data, plus a true zero point, allowing for ratio comparisons.",
    "topic": "Levels of Measurement",
    "difficulty": "Easy"
  },
  {
    "id": 65,
    "question": "What type of data scale has ordered categories but unequal intervals between them (e.g., low, medium, high)?",
    "options": {
      "A": "Nominal",
      "B": "Ordinal",
      "C": "Interval",
      "D": "Ratio"
    },
    "answer": "B",
    "explanation": "Ordinal data has categories with a meaningful order, but the differences between categories are not uniform.",
    "topic": "Levels of Measurement",
    "difficulty": "Easy"
  },
  {
    "id": 66,
    "question": "What type of data scale involves categories without any natural order (e.g., gender, marital status)?",
    "options": {
      "A": "Nominal",
      "B": "Ordinal",
      "C": "Interval",
      "D": "Ratio"
    },
    "answer": "A",
    "explanation": "Nominal data are categorical and cannot be ordered or ranked.",
    "topic": "Levels of Measurement",
    "difficulty": "Easy"
  },
  {
    "id": 67,
    "question": "What type of data scale has ordered categories with equal intervals, but no true zero point (e.g., temperature in Celsius)?",
    "options": {
      "A": "Nominal",
      "B": "Ordinal",
      "C": "Interval",
      "D": "Ratio"
    },
    "answer": "C",
    "explanation": "Interval data has ordered categories with equal differences, but zero doesn't mean the absence of the quantity.",
    "topic": "Levels of Measurement",
    "difficulty": "Easy"
  },
  {
    "id": 68,
    "question": "What is the purpose of cleaning data?",
    "options": {
      "A": "To make it look nicer in a graph.",
      "B": "To remove errors, inconsistencies, and prepare it for analysis.",
      "C": "To collect more data.",
      "D": "To analyze it immediately."
    },
    "answer": "B",
    "explanation": "Data cleaning is a crucial step to ensure data quality and accuracy before analysis.",
    "topic": "Data Preprocessing",
    "difficulty": "Easy"
  },
  {
    "id": 69,
    "question": "What is missing data?",
    "options": {
      "A": "Data that is irrelevant to the analysis.",
      "B": "Values that are not recorded or are unavailable for a variable.",
      "C": "Data that is duplicated.",
      "D": "Data that has been incorrectly entered."
    },
    "answer": "B",
    "explanation": "Missing data refers to observations where there is no data value stored for the variable in question.",
    "topic": "Data Preprocessing",
    "difficulty": "Easy"
  },
  {
    "id": 70,
    "question": "What is a common method for handling missing data?",
    "options": {
      "A": "Ignoring it completely",
      "B": "Deleting all rows with missing values",
      "C": "Imputation (filling in missing values)",
      "D": "Randomly guessing values"
    },
    "answer": "C",
    "explanation": "Imputation involves estimating and replacing missing values using various statistical techniques.",
    "topic": "Data Preprocessing",
    "difficulty": "Easy"
  },
  {
    "id": 71,
    "question": "What is the process of converting raw data into a usable and efficient format for analysis?",
    "options": {
      "A": "Data collection",
      "B": "Data visualization",
      "C": "Data transformation",
      "D": "Data interpretation"
    },
    "answer": "C",
    "explanation": "Data transformation involves converting data from one format or structure into another.",
    "topic": "Data Preprocessing",
    "difficulty": "Easy"
  },
  {
    "id": 72,
    "question": "What is typically represented on the x-axis of a histogram?",
    "options": {
      "A": "Frequency",
      "B": "Categories",
      "C": "Data values or intervals (bins)",
      "D": "Percentages"
    },
    "answer": "C",
    "explanation": "The x-axis of a histogram represents the range of data values, divided into bins or intervals.",
    "topic": "Data Visualization",
    "difficulty": "Easy"
  },
  {
    "id": 73,
    "question": "What is typically represented on the y-axis of a histogram?",
    "options": {
      "A": "Data values",
      "B": "Categories",
      "C": "Frequency or relative frequency",
      "D": "Time"
    },
    "answer": "C",
    "explanation": "The y-axis of a histogram indicates the frequency or count of observations within each bin.",
    "topic": "Data Visualization",
    "difficulty": "Easy"
  },
  {
    "id": 74,
    "question": "What is a scatter plot used to show?",
    "options": {
      "A": "Distribution of a single variable",
      "B": "Comparison of categories",
      "C": "Relationship between two quantitative variables",
      "D": "Parts of a whole"
    },
    "answer": "C",
    "explanation": "Scatter plots are used to visualize the relationship or correlation between two numerical variables.",
    "topic": "Data Visualization",
    "difficulty": "Easy"
  },
  {
    "id": 75,
    "question": "What does a positive correlation on a scatter plot indicate?",
    "options": {
      "A": "As one variable increases, the other decreases.",
      "B": "As one variable increases, the other also increases.",
      "C": "There is no relationship between the variables.",
      "D": "The variables are independent."
    },
    "answer": "B",
    "explanation": "A positive correlation means that as one variable goes up, the other tends to go up as well.",
    "topic": "Correlation",
    "difficulty": "Easy"
  },
  {
    "id": 76,
    "question": "What does a negative correlation on a scatter plot indicate?",
    "options": {
      "A": "As one variable increases, the other also increases.",
      "B": "As one variable increases, the other decreases.",
      "C": "There is no relationship between the variables.",
      "D": "The variables are unrelated."
    },
    "answer": "B",
    "explanation": "A negative correlation means that as one variable goes up, the other tends to go down.",
    "topic": "Correlation",
    "difficulty": "Easy"
  },
  {
    "id": 77,
    "question": "What does it mean if there is no correlation between two variables?",
    "options": {
      "A": "They always move in the same direction.",
      "B": "They always move in opposite directions.",
      "C": "There is no discernible linear relationship between them.",
      "D": "One causes the other."
    },
    "answer": "C",
    "explanation": "No correlation implies that changes in one variable are not consistently associated with changes in the other in a linear fashion.",
    "topic": "Correlation",
    "difficulty": "Easy"
  },
  {
    "id": 78,
    "question": "What is the primary function of a line graph?",
    "options": {
      "A": "To show parts of a whole",
      "B": "To compare discrete categories",
      "C": "To display trends or changes over time",
      "D": "To show the distribution of a single variable"
    },
    "answer": "C",
    "explanation": "Line graphs are excellent for visualizing how data changes continuously over a period.",
    "topic": "Data Visualization",
    "difficulty": "Easy"
  },
  {
    "id": 79,
    "question": "What is meant by 'skewness' in a data distribution?",
    "options": {
      "A": "The center of the data",
      "B": "The spread of the data",
      "C": "The asymmetry of the distribution",
      "D": "The number of peaks in the distribution"
    },
    "answer": "C",
    "explanation": "Skewness measures the asymmetry of the probability distribution of a real-valued random variable about its mean.",
    "topic": "Distribution Shapes",
    "difficulty": "Easy"
  },
  {
    "id": 80,
    "question": "What does a right-skewed (positively skewed) distribution look like?",
    "options": {
      "A": "The tail is longer on the left side.",
      "B": "The tail is longer on the right side.",
      "C": "It is perfectly symmetrical.",
      "D": "It has two peaks."
    },
    "answer": "B",
    "explanation": "In a right-skewed distribution, the majority of the data is on the left, and the tail extends to the right.",
    "topic": "Distribution Shapes",
    "difficulty": "Easy"
  },
  {
    "id": 81,
    "question": "What does a left-skewed (negatively skewed) distribution look like?",
    "options": {
      "A": "The tail is longer on the right side.",
      "B": "The tail is longer on the left side.",
      "C": "It is perfectly symmetrical.",
      "D": "It has no tails."
    },
    "answer": "B",
    "explanation": "In a left-skewed distribution, the majority of the data is on the right, and the tail extends to the left.",
    "topic": "Distribution Shapes",
    "difficulty": "Easy"
  },
  {
    "id": 82,
    "question": "What is a symmetrical distribution?",
    "options": {
      "A": "One where the mean, median, and mode are always different.",
      "B": "One where the left and right sides are mirror images.",
      "C": "One that only has positive values.",
      "D": "One that has multiple peaks."
    },
    "answer": "B",
    "explanation": "A symmetrical distribution has an equal shape on both sides of its center.",
    "topic": "Distribution Shapes",
    "difficulty": "Easy"
  },
  {
    "id": 83,
    "question": "What is the empirical rule (68-95-99.7 rule) typically associated with?",
    "options": {
      "A": "Uniform distribution",
      "B": "Exponential distribution",
      "C": "Normal distribution",
      "D": "Poisson distribution"
    },
    "answer": "C",
    "explanation": "The empirical rule applies to normal (bell-shaped) distributions, describing the percentage of data within 1, 2, and 3 standard deviations of the mean.",
    "topic": "Normal Distribution",
    "difficulty": "Easy"
  },
  {
    "id": 84,
    "question": "Approximately what percentage of data falls within one standard deviation of the mean in a normal distribution?",
    "options": {
      "A": "50%",
      "B": "68%",
      "C": "95%",
      "D": "99.7%"
    },
    "answer": "B",
    "explanation": "According to the empirical rule, about 68% of data falls within 1 standard deviation of the mean in a normal distribution.",
    "topic": "Normal Distribution",
    "difficulty": "Easy"
  },
  {
    "id": 85,
    "question": "Approximately what percentage of data falls within two standard deviations of the mean in a normal distribution?",
    "options": {
      "A": "68%",
      "B": "90%",
      "C": "95%",
      "D": "99.7%"
    },
    "answer": "C",
    "explanation": "About 95% of data falls within 2 standard deviations of the mean in a normal distribution.",
    "topic": "Normal Distribution",
    "difficulty": "Easy"
  },
  {
    "id": 86,
    "question": "Approximately what percentage of data falls within three standard deviations of the mean in a normal distribution?",
    "options": {
      "A": "95%",
      "B": "98%",
      "C": "99.7%",
      "D": "100%"
    },
    "answer": "C",
    "explanation": "About 99.7% of data falls within 3 standard deviations of the mean in a normal distribution.",
    "topic": "Normal Distribution",
    "difficulty": "Easy"
  },
  {
    "id": 87,
    "question": "What is a Z-score (standard score) used for?",
    "options": {
      "A": "To calculate the mean of a dataset.",
      "B": "To compare a score to a standard normal distribution.",
      "C": "To determine the mode of a dataset.",
      "D": "To find the range of a dataset."
    },
    "answer": "B",
    "explanation": "A Z-score tells you how many standard deviations an element is from the mean.",
    "topic": "Z-Scores",
    "difficulty": "Easy"
  },
  {
    "id": 88,
    "question": "What does a Z-score of 0 mean?",
    "options": {
      "A": "The data point is an outlier.",
      "B": "The data point is equal to the mean.",
      "C": "The data point is the minimum value.",
      "D": "The data point is the maximum value."
    },
    "answer": "B",
    "explanation": "A Z-score of 0 indicates that the data point is exactly at the mean of the distribution.",
    "topic": "Z-Scores",
    "difficulty": "Easy"
  },
  {
    "id": 89,
    "question": "What is the fundamental concept behind hypothesis testing?",
    "options": {
      "A": "To prove that a hypothesis is true.",
      "B": "To determine if there is enough evidence to reject a null hypothesis.",
      "C": "To find the mean of a population.",
      "D": "To visualize data."
    },
    "answer": "B",
    "explanation": "Hypothesis testing involves evaluating evidence from a sample to decide whether there is enough support to reject a null hypothesis.",
    "topic": "Hypothesis Testing Basics",
    "difficulty": "Easy"
  },
  {
    "id": 90,
    "question": "What is the null hypothesis (H0)?",
    "options": {
      "A": "The hypothesis that the researcher wants to prove.",
      "B": "The hypothesis that there is no effect or no difference.",
      "C": "The alternative hypothesis.",
      "D": "The conclusion of the study."
    },
    "answer": "B",
    "explanation": "The null hypothesis is a statement of no effect or no difference, which is assumed to be true until proven otherwise.",
    "topic": "Hypothesis Testing Basics",
    "difficulty": "Easy"
  },
  {
    "id": 91,
    "question": "What is the alternative hypothesis (H1 or Ha)?",
    "options": {
      "A": "The hypothesis of no effect or no difference.",
      "B": "The hypothesis that the researcher wants to prove, suggesting there is an effect or difference.",
      "C": "Always false.",
      "D": "Always true."
    },
    "answer": "B",
    "explanation": "The alternative hypothesis proposes that there is a significant difference or relationship.",
    "topic": "Hypothesis Testing Basics",
    "difficulty": "Easy"
  },
  {
    "id": 92,
    "question": "What is a p-value?",
    "options": {
      "A": "The probability of the alternative hypothesis being true.",
      "B": "The probability of observing the data (or more extreme data) if the null hypothesis were true.",
      "C": "The significance level.",
      "D": "The power of the test."
    },
    "answer": "B",
    "explanation": "The p-value helps determine the statistical significance of the results.",
    "topic": "Hypothesis Testing Basics",
    "difficulty": "Easy"
  },
  {
    "id": 93,
    "question": "If the p-value is less than the significance level (α), what do we typically do?",
    "options": {
      "A": "Accept the null hypothesis.",
      "B": "Fail to reject the null hypothesis.",
      "C": "Reject the null hypothesis.",
      "D": "Increase the sample size."
    },
    "answer": "C",
    "explanation": "A small p-value (less than α) indicates strong evidence against the null hypothesis, leading to its rejection.",
    "topic": "Hypothesis Testing Basics",
    "difficulty": "Easy"
  },
  {
    "id": 94,
    "question": "What is a confidence interval?",
    "options": {
      "A": "A single point estimate of a population parameter.",
      "B": "A range of values that is likely to contain the true population parameter.",
      "C": "The p-value of a statistical test.",
      "D": "The standard deviation of a sample."
    },
    "answer": "B",
    "explanation": "A confidence interval provides a range of plausible values for an unknown population parameter.",
    "topic": "Estimation",
    "difficulty": "Easy"
  },
  {
    "id": 95,
    "question": "What does a 95% confidence interval mean?",
    "options": {
      "A": "There is a 95% chance that the sample mean is within this interval.",
      "B": "We are 95% confident that the true population parameter lies within this interval.",
      "C": "95% of the data falls within this interval.",
      "D": "There is a 5% chance of making an error."
    },
    "answer": "B",
    "explanation": "A 95% confidence interval means that if we were to repeat the sampling many times, 95% of the intervals constructed would contain the true population parameter.",
    "topic": "Estimation",
    "difficulty": "Easy"
  },
  {
    "id": 96,
    "question": "What is the purpose of correlation analysis?",
    "options": {
      "A": "To determine causality between variables.",
      "B": "To describe the strength and direction of a linear relationship between two quantitative variables.",
      "C": "To predict future values.",
      "D": "To compare means of different groups."
    },
    "answer": "B",
    "explanation": "Correlation measures the extent to which two variables are linearly related.",
    "topic": "Correlation",
    "difficulty": "Easy"
  },
  {
    "id": 97,
    "question": "What does a correlation coefficient of +1 indicate?",
    "options": {
      "A": "Perfect negative linear relationship",
      "B": "No linear relationship",
      "C": "Perfect positive linear relationship",
      "D": "A curved relationship"
    },
    "answer": "C",
    "explanation": "A correlation coefficient of +1 signifies a perfect direct (positive) linear relationship.",
    "topic": "Correlation",
    "difficulty": "Easy"
  },
  {
    "id": 98,
    "question": "What does a correlation coefficient of -1 indicate?",
    "options": {
      "A": "Perfect positive linear relationship",
      "B": "No linear relationship",
      "C": "Perfect negative linear relationship",
      "D": "A weak relationship"
    },
    "answer": "C",
    "explanation": "A correlation coefficient of -1 signifies a perfect inverse (negative) linear relationship.",
    "topic": "Correlation",
    "difficulty": "Easy"
  },
  {
    "id": 99,
    "question": "What does a correlation coefficient of 0 indicate (for linear relationships)?",
    "options": {
      "A": "Perfect positive linear relationship",
      "B": "No linear relationship",
      "C": "Perfect negative linear relationship",
      "D": "A strong non-linear relationship"
    },
    "answer": "B",
    "explanation": "A correlation coefficient of 0 suggests no linear relationship between the variables.",
    "topic": "Correlation",
    "difficulty": "Easy"
  },
  {
    "id": 100,
    "question": "Which of the following is NOT a measure of central tendency?",
    "options": {
      "A": "Mean",
      "B": "Median",
      "C": "Mode",
      "D": "Range"
    },
    "answer": "D",
    "explanation": "Range is a measure of dispersion, not central tendency.",
    "topic": "Measures of Central Tendency",
    "difficulty": "Easy"
  },
  {
    "id": 101,
    "question": "A researcher wants to study the average height of adult males in a city. He measures the height of 100 randomly selected adult males. What is the population in this study?",
    "options": {
      "A": "The 100 randomly selected adult males",
      "B": "All adult males in the city",
      "C": "The average height of the 100 males",
      "D": "All adult males in the country"
    },
    "answer": "B",
    "explanation": "The population is the entire group the researcher is interested in studying.",
    "topic": "Sampling",
    "difficulty": "Medium"
  },
  {
    "id": 102,
    "question": "Calculate the variance of the following dataset: 2, 4, 6, 8, 10 (assume it's a sample).",
    "options": {
      "A": "8",
      "B": "10",
      "C": "12",
      "D": "12.5"
    },
    "answer": "B",
    "explanation": "Mean = 6. Deviations squared: (-4)^2=16, (-2)^2=4, (0)^2=0, (2)^2=4, (4)^2=16. Sum = 40. Variance = 40/(5-1) = 40/4 = 10.",
    "topic": "Measures of Dispersion",
    "difficulty": "Medium"
  },
  {
    "id": 103,
    "question": "What is the standard deviation of the following dataset: 2, 4, 6, 8, 10 (assume it's a sample)?",
    "options": {
      "A": "2.83",
      "B": "3.16",
      "C": "3.46",
      "D": "3.74"
    },
    "answer": "B",
    "explanation": "The standard deviation is the square root of the variance. Variance is 10, so sqrt(10) ≈ 3.16.",
    "topic": "Measures of Dispersion",
    "difficulty": "Medium"
  },
  {
    "id": 104,
    "question": "A card is drawn from a standard 52-card deck. What is the probability of drawing a red card or a King?",
    "options": {
      "A": "26/52",
      "B": "4/52",
      "C": "28/52",
      "D": "30/52"
    },
    "answer": "C",
    "explanation": "P(Red) = 26/52. P(King) = 4/52. P(Red King) = 2/52. P(Red or King) = P(Red) + P(King) - P(Red King) = 26/52 + 4/52 - 2/52 = 28/52.",
    "topic": "Probability Rules",
    "difficulty": "Medium"
  },
  {
    "id": 105,
    "question": "What is the probability of rolling a sum of 7 when rolling two fair six-sided dice?",
    "options": {
      "A": "1/36",
      "B": "2/36",
      "C": "6/36",
      "D": "7/36"
    },
    "answer": "C",
    "explanation": "Possible combinations for a sum of 7 are (1,6), (2,5), (3,4), (4,3), (5,2), (6,1). There are 6 favorable outcomes out of 36 total outcomes, so 6/36 = 1/6.",
    "topic": "Compound Probability",
    "difficulty": "Medium"
  },
  {
    "id": 106,
    "question": "A bag contains 3 red, 4 blue, and 5 green marbles. If you draw two marbles without replacement, what is the probability of drawing two red marbles?",
    "options": {
      "A": "3/12 * 2/11",
      "B": "3/12 * 3/12",
      "C": "3/12 + 2/11",
      "D": "2/12"
    },
    "answer": "A",
    "explanation": "P(1st Red) = 3/12. P(2nd Red | 1st Red) = 2/11. P(Both Red) = (3/12) * (2/11) = 6/132 = 1/22.",
    "topic": "Conditional Probability",
    "difficulty": "Medium"
  },
  {
    "id": 107,
    "question": "If the mean of a normally distributed dataset is 50 and the standard deviation is 5, what is the Z-score for a value of 60?",
    "options": {
      "A": "1",
      "B": "2",
      "C": "0.5",
      "D": "-2"
    },
    "answer": "B",
    "explanation": "Z = (X - μ) / σ = (60 - 50) / 5 = 10 / 5 = 2.",
    "topic": "Z-Scores",
    "difficulty": "Medium"
  },
  {
    "id": 108,
    "question": "A dataset has a mean of 75 and a standard deviation of 10. If a data point has a Z-score of -1.5, what is the value of that data point?",
    "options": {
      "A": "60",
      "B": "65",
      "C": "70",
      "D": "80"
    },
    "answer": "B",
    "explanation": "X = μ + Z*σ = 75 + (-1.5)*10 = 75 - 15 = 60.",
    "topic": "Z-Scores",
    "difficulty": "Medium"
  },
  {
    "id": 109,
    "question": "In a positively skewed distribution, what is the typical relationship between the mean, median, and mode?",
    "options": {
      "A": "Mean < Median < Mode",
      "B": "Mean > Median > Mode",
      "C": "Mean = Median = Mode",
      "D": "Mode > Mean > Median"
    },
    "answer": "B",
    "explanation": "In a positively (right) skewed distribution, the tail is on the right, pulling the mean to the right (higher) than the median, and the mode is typically the peak.",
    "topic": "Distribution Shapes",
    "difficulty": "Medium"
  },
  {
    "id": 110,
    "question": "In a negatively skewed distribution, what is the typical relationship between the mean, median, and mode?",
    "options": {
      "A": "Mean < Median < Mode",
      "B": "Mean > Median > Mode",
      "C": "Mean = Median = Mode",
      "D": "Mode < Mean < Median"
    },
    "answer": "A",
    "explanation": "In a negatively (left) skewed distribution, the tail is on the left, pulling the mean to the left (lower) than the median, and the mode is typically the peak.",
    "topic": "Distribution Shapes",
    "difficulty": "Medium"
  },
  {
    "id": 112,
    "question": "Which of the following describes a Type I error in hypothesis testing?",
    "options": {
      "A": "Failing to reject a false null hypothesis.",
      "B": "Rejecting a true null hypothesis.",
      "C": "Rejecting a false alternative hypothesis.",
      "D": "Failing to reject a true alternative hypothesis."
    },
    "answer": "B",
    "explanation": "A Type I error (alpha error) occurs when you incorrectly reject a true null hypothesis.",
    "topic": "Hypothesis Testing Errors",
    "difficulty": "Medium"
  },
  {
    "id": 113,
    "question": "Which of the following describes a Type II error in hypothesis testing?",
    "options": {
      "A": "Rejecting a true null hypothesis.",
      "B": "Failing to reject a false null hypothesis.",
      "C": "Accepting a false alternative hypothesis.",
      "D": "Rejecting a true alternative hypothesis."
    },
    "answer": "B",
    "explanation": "A Type II error (beta error) occurs when you incorrectly fail to reject a false null hypothesis.",
    "topic": "Hypothesis Testing Errors",
    "difficulty": "Medium"
  },
  {
    "id": 114,
    "question": "Increasing the sample size generally has what effect on the width of a confidence interval?",
    "options": {
      "A": "It increases the width.",
      "B": "It decreases the width.",
      "C": "It has no effect on the width.",
      "D": "It makes the interval asymmetrical."
    },
    "answer": "B",
    "explanation": "Larger sample sizes reduce the standard error, leading to narrower and more precise confidence intervals.",
    "topic": "Estimation",
    "difficulty": "Medium"
  },
  {
    "id": 115,
    "question": "If a 95% confidence interval for a population mean is (45, 55), what can we conclude about the population mean?",
    "options": {
      "A": "The population mean is exactly 50.",
      "B": "There is a 95% probability that the population mean is 50.",
      "C": "We are 95% confident that the population mean is between 45 and 55.",
      "D": "95% of the data falls between 45 and 55."
    },
    "answer": "C",
    "explanation": "A confidence interval provides a range within which the true population parameter is likely to fall with a certain level of confidence.",
    "topic": "Estimation",
    "difficulty": "Medium"
  },
  {
    "id": 116,
    "question": "When performing a hypothesis test, if the p-value is 0.03 and the significance level (α) is 0.05, what is the correct decision?",
    "options": {
      "A": "Fail to reject the null hypothesis.",
      "B": "Reject the null hypothesis.",
      "C": "Accept the null hypothesis.",
      "D": "Insufficient information to decide."
    },
    "answer": "B",
    "explanation": "Since 0.03 < 0.05, the p-value is less than alpha, leading to the rejection of the null hypothesis.",
    "topic": "Hypothesis Testing Decisions",
    "difficulty": "Medium"
  },
  {
    "id": 117,
    "question": "When performing a hypothesis test, if the p-value is 0.08 and the significance level (α) is 0.05, what is the correct decision?",
    "options": {
      "A": "Reject the null hypothesis.",
      "B": "Accept the alternative hypothesis.",
      "C": "Fail to reject the null hypothesis.",
      "D": "Decrease the significance level."
    },
    "answer": "C",
    "explanation": "Since 0.08 > 0.05, the p-value is greater than alpha, meaning there isn't enough evidence to reject the null hypothesis.",
    "topic": "Hypothesis Testing Decisions",
    "difficulty": "Medium"
  },
  {
    "id": 118,
    "question": "What is the primary assumption for using a t-test for comparing two means?",
    "options": {
      "A": "The data must be qualitative.",
      "B": "The population standard deviations are known.",
      "C": "The data is normally distributed (or sample size is large enough) and variances are approximately equal (for independent samples t-test).",
      "D": "The sample size must be exactly 30."
    },
    "answer": "C",
    "explanation": "T-tests assume normality of the underlying population or a sufficiently large sample size for the Central Limit Theorem to apply, and often homogeneity of variances.",
    "topic": "Hypothesis Testing (t-tests)",
    "difficulty": "Medium"
  },
  {
    "id": 119,
    "question": "When is it appropriate to use a Z-test instead of a t-test for comparing means?",
    "options": {
      "A": "When the sample size is small.",
      "B": "When the population standard deviation is known.",
      "C": "When the data is not normally distributed.",
      "D": "When comparing more than two means."
    },
    "answer": "B",
    "explanation": "A Z-test is used when the population standard deviation is known. When it's unknown and estimated from the sample, a t-test is more appropriate.",
    "topic": "Hypothesis Testing (Z-tests)",
    "difficulty": "Medium"
  },
  {
    "id": 120,
    "question": "What does a coefficient of determination (R-squared) of 0.81 mean in a regression analysis?",
    "options": {
      "A": "81% of the data points are on the regression line.",
      "B": "81% of the variance in the dependent variable is explained by the independent variable(s).",
      "C": "The correlation coefficient is 0.81.",
      "D": "The model is 81% accurate in its predictions."
    },
    "answer": "B",
    "explanation": "R-squared represents the proportion of the variance in the dependent variable that can be predicted from the independent variable(s).",
    "topic": "Regression Analysis",
    "difficulty": "Medium"
  },
  {
    "id": 121,
    "question": "What is multicollinearity in a multiple regression model?",
    "options": {
      "A": "When the dependent variable is related to multiple independent variables.",
      "B": "When two or more independent variables are highly correlated with each other.",
      "C": "When the residuals are not normally distributed.",
      "D": "When the regression line is curved."
    },
    "answer": "B",
    "explanation": "Multicollinearity occurs when independent variables in a multiple regression model are correlated, which can cause issues with model interpretation and stability.",
    "topic": "Regression Analysis",
    "difficulty": "Medium"
  },
  {
    "id": 122,
    "question": "In regression analysis, what does the 'residual' represent?",
    "options": {
      "A": "The predicted value of the dependent variable.",
      "B": "The difference between the observed value and the predicted value of the dependent variable.",
      "C": "The independent variable.",
      "D": "The slope of the regression line."
    },
    "answer": "B",
    "explanation": "A residual is the error in predicting the dependent variable, or the vertical distance from a data point to the regression line.",
    "topic": "Regression Analysis",
    "difficulty": "Medium"
  },
  {
    "id": 123,
    "question": "What is the purpose of ANOVA (Analysis of Variance)?",
    "options": {
      "A": "To compare two population means.",
      "B": "To compare variances of two populations.",
      "C": "To compare means of three or more groups.",
      "D": "To test the correlation between two variables."
    },
    "answer": "C",
    "explanation": "ANOVA is used to test for differences between three or more population means by analyzing the variance within and between groups.",
    "topic": "ANOVA",
    "difficulty": "Medium"
  },
  {
    "id": 124,
    "question": "When performing an ANOVA test, what is the null hypothesis?",
    "options": {
      "A": "At least one group mean is different.",
      "B": "All group means are equal.",
      "C": "All group means are different.",
      "D": "There is a significant correlation between variables."
    },
    "answer": "B",
    "explanation": "The null hypothesis for ANOVA is that all population means are equal.",
    "topic": "ANOVA",
    "difficulty": "Medium"
  },
  {
    "id": 125,
    "question": "What is the chi-square test used for?",
    "options": {
      "A": "Comparing means of continuous data.",
      "B": "Testing the independence between two categorical variables.",
      "C": "Predicting values based on a linear model.",
      "D": "Measuring the spread of data."
    },
    "answer": "B",
    "explanation": "The chi-square test of independence assesses whether there is a statistically significant association between two categorical variables.",
    "topic": "Chi-Square Test",
    "difficulty": "Medium"
  },
  {
    "id": 126,
    "question": "What is the expected value of a discrete random variable X with probability mass function P(x)?",
    "options": {
      "A": "Σ P(x)",
      "B": "Σ x * P(x)",
      "C": "Σ x^2 * P(x)",
      "D": "Max(x)"
    },
    "answer": "B",
    "explanation": "The expected value is the sum of each possible value multiplied by its probability.",
    "topic": "Probability Distributions",
    "difficulty": "Medium"
  },
  {
    "id": 127,
    "question": "A fair coin is flipped 10 times. What type of distribution describes the number of heads obtained?",
    "options": {
      "A": "Poisson distribution",
      "B": "Normal distribution",
      "C": "Binomial distribution",
      "D": "Uniform distribution"
    },
    "answer": "C",
    "explanation": "The binomial distribution models the number of successes in a fixed number of independent Bernoulli trials (e.g., coin flips) with two outcomes.",
    "topic": "Probability Distributions",
    "difficulty": "Medium"
  },
  {
    "id": 128,
    "question": "In a normal distribution, what percentage of values lie between -1 and +1 standard deviations from the mean?",
    "options": {
      "A": "50%",
      "B": "68.27%",
      "C": "95.45%",
      "D": "99.73%"
    },
    "answer": "B",
    "explanation": "This is a key part of the empirical rule for normal distributions.",
    "topic": "Normal Distribution",
    "difficulty": "Medium"
  },
  {
    "id": 129,
    "question": "What is the Central Limit Theorem?",
    "options": {
      "A": "States that the mean of a sample is always equal to the population mean.",
      "B": "States that the distribution of sample means approaches a normal distribution as the sample size increases, regardless of the population distribution.",
      "C": "States that all data is normally distributed.",
      "D": "States that the standard deviation is always positive."
    },
    "answer": "B",
    "explanation": "The CLT is a fundamental theorem in statistics, allowing for the use of normal distribution properties for sample means even with non-normal populations, given a sufficiently large sample size.",
    "topic": "Sampling Distributions",
    "difficulty": "Medium"
  },
  {
    "id": 130,
    "question": "What is the standard error of the mean?",
    "options": {
      "A": "The standard deviation of the population.",
      "B": "The standard deviation of the sample.",
      "C": "The standard deviation of the sampling distribution of the sample mean.",
      "D": "The mean of the sample means."
    },
    "answer": "C",
    "explanation": "The standard error of the mean measures how much the sample mean is expected to vary from the population mean due to random sampling.",
    "topic": "Sampling Distributions",
    "difficulty": "Medium"
  },
  {
    "id": 131,
    "question": "Which of the following is most resistant to the effects of outliers?",
    "options": {
      "A": "Mean",
      "B": "Standard deviation",
      "C": "Median",
      "D": "Range"
    },
    "answer": "C",
    "explanation": "The median is the middle value and is not affected by extremely large or small values, making it robust to outliers.",
    "topic": "Measures of Central Tendency",
    "difficulty": "Medium"
  },
  {
    "id": 132,
    "question": "A survey asks respondents to rate their satisfaction on a scale from 1 (Very Dissatisfied) to 5 (Very Satisfied). What type of data is this?",
    "options": {
      "A": "Nominal",
      "B": "Ordinal",
      "C": "Interval",
      "D": "Ratio"
    },
    "answer": "B",
    "explanation": "The categories have a meaningful order (satisfaction level), but the difference between 'Very Dissatisfied' and 'Dissatisfied' may not be the same as between 'Satisfied' and 'Very Satisfied'.",
    "topic": "Levels of Measurement",
    "difficulty": "Medium"
  },
  {
    "id": 133,
    "question": "What is the primary goal of stratified random sampling?",
    "options": {
      "A": "To select participants based on convenience.",
      "B": "To ensure every individual has an equal chance of selection.",
      "C": "To ensure representation from all important subgroups in the population.",
      "D": "To select entire clusters of individuals."
    },
    "answer": "C",
    "explanation": "Stratified sampling aims to reduce sampling error by ensuring proportional representation of key strata (subgroups).",
    "topic": "Sampling Methods",
    "difficulty": "Medium"
  },
  {
    "id": 134,
    "question": "What is the term for a systematic error in sampling that results in an unrepresentative sample?",
    "options": {
      "A": "Random error",
      "B": "Sampling bias",
      "C": "Measurement error",
      "D": "Typographical error"
    },
    "answer": "B",
    "explanation": "Sampling bias occurs when the method of sampling causes certain members of the population to be more or less likely to be included than others.",
    "topic": "Sampling Errors",
    "difficulty": "Medium"
  },
  {
    "id": 135,
    "question": "If two events A and B are independent, and P(A) = 0.4, P(B) = 0.5, what is P(A or B)?",
    "options": {
      "A": "0.2",
      "B": "0.7",
      "C": "0.9",
      "D": "0.1"
    },
    "answer": "B",
    "explanation": "P(A or B) = P(A) + P(B) - P(A and B). Since A and B are independent, P(A and B) = P(A) * P(B) = 0.4 * 0.5 = 0.2. So, P(A or B) = 0.4 + 0.5 - 0.2 = 0.7.",
    "topic": "Probability Rules",
    "difficulty": "Medium"
  },
  {
    "id": 136,
    "question": "A discrete random variable X can take values 0, 1, 2, with probabilities P(X=0)=0.3, P(X=1)=0.5, P(X=2)=0.2. What is E[X]?",
    "options": {
      "A": "0.5",
      "B": "0.9",
      "C": "1.0",
      "D": "1.2"
    },
    "answer": "B",
    "explanation": "E[X] = (0 * 0.3) + (1 * 0.5) + (2 * 0.2) = 0 + 0.5 + 0.4 = 0.9.",
    "topic": "Expected Value",
    "difficulty": "Medium"
  },
  {
    "id": 137,
    "question": "What is the degrees of freedom for a one-sample t-test with a sample size of 'n'?",
    "options": {
      "A": "n",
      "B": "n-1",
      "C": "n-2",
      "D": "n+1"
    },
    "answer": "B",
    "explanation": "For a one-sample t-test, the degrees of freedom are calculated as the sample size minus 1.",
    "topic": "Degrees of Freedom",
    "difficulty": "Medium"
  },
  {
    "id": 138,
    "question": "What is the primary assumption required for simple linear regression?",
    "options": {
      "A": "The relationship between variables is exponential.",
      "B": "The relationship between the independent and dependent variables is linear.",
      "C": "The independent variable is normally distributed.",
      "D": "The dependent variable is categorical."
    },
    "answer": "B",
    "explanation": "Simple linear regression models a linear relationship between one independent and one dependent variable.",
    "topic": "Regression Analysis",
    "difficulty": "Medium"
  },
  {
    "id": 139,
    "question": "When interpreting a regression coefficient (slope) of 0.75, what does it mean?",
    "options": {
      "A": "For every one-unit increase in the dependent variable, the independent variable increases by 0.75 units.",
      "B": "For every one-unit increase in the independent variable, the dependent variable increases by 0.75 units.",
      "C": "There is no relationship between the variables.",
      "D": "The dependent variable is 75% explained by the independent variable."
    },
    "answer": "B",
    "explanation": "The slope coefficient indicates the expected change in the dependent variable for a one-unit change in the independent variable.",
    "topic": "Regression Analysis",
    "difficulty": "Medium"
  },
  {
    "id": 140,
    "question": "What is the mode of the following data if it's a grouped frequency distribution with class intervals and frequencies: Class: 10-20 (Freq: 5), 20-30 (Freq: 12), 30-40 (Freq: 8), 40-50 (Freq: 3)?",
    "options": {
      "A": "20-30",
      "B": "25",
      "C": "30-40",
      "D": "12"
    },
    "answer": "A",
    "explanation": "The modal class is the class interval with the highest frequency, which is 20-30 with a frequency of 12.",
    "topic": "Measures of Central Tendency (Grouped Data)",
    "difficulty": "Medium"
  },
  {
    "id": 141,
    "question": "What type of graph would you use to display the relationship between temperature and ice cream sales?",
    "options": {
      "A": "Bar graph",
      "B": "Pie chart",
      "C": "Scatter plot",
      "D": "Histogram"
    },
    "answer": "C",
    "explanation": "A scatter plot is ideal for visualizing the relationship between two quantitative variables.",
    "topic": "Data Visualization",
    "difficulty": "Medium"
  },
  {
    "id": 142,
    "question": "You want to determine if there's a significant difference in the average test scores of students taught by three different methods (A, B, C). Which statistical test is most appropriate?",
    "options": {
      "A": "Paired t-test",
      "B": "Independent samples t-test",
      "C": "ANOVA",
      "D": "Chi-square test"
    },
    "answer": "C",
    "explanation": "ANOVA is used to compare the means of three or more independent groups.",
    "topic": "Hypothesis Testing (ANOVA)",
    "difficulty": "Medium"
  },
  {
    "id": 143,
    "question": "What is the critical value in hypothesis testing?",
    "options": {
      "A": "The calculated test statistic.",
      "B": "The p-value.",
      "C": "A threshold value used to determine whether to reject the null hypothesis.",
      "D": "The significance level."
    },
    "answer": "C",
    "explanation": "The critical value defines the boundary of the rejection region for the null hypothesis.",
    "topic": "Hypothesis Testing Concepts",
    "difficulty": "Medium"
  },
  {
    "id": 144,
    "question": "If a confidence interval for a proportion includes 0.5, what does this suggest about a two-sided hypothesis test for the proportion being equal to 0.5?",
    "options": {
      "A": "The null hypothesis should be rejected.",
      "B": "The null hypothesis should not be rejected.",
      "C": "The alternative hypothesis is true.",
      "D": "The sample size is too small."
    },
    "answer": "B",
    "explanation": "If the null hypothesis value (e.g., 0.5) falls within the confidence interval, we fail to reject the null hypothesis.",
    "topic": "Confidence Intervals and Hypothesis Testing",
    "difficulty": "Medium"
  },
  {
    "id": 145,
    "question": "What is a pooled variance in the context of an independent samples t-test?",
    "options": {
      "A": "The average of the two sample means.",
      "B": "A weighted average of the two sample variances, used when assuming equal population variances.",
      "C": "The variance of the differences between paired observations.",
      "D": "The largest of the two sample variances."
    },
    "answer": "B",
    "explanation": "Pooled variance combines the variance information from both samples to get a better estimate of the common population variance, assuming they are equal.",
    "topic": "T-Tests",
    "difficulty": "Medium"
  },
  {
    "id": 146,
    "question": "When should you use a paired samples t-test instead of an independent samples t-test?",
    "options": {
      "A": "When comparing more than two groups.",
      "B": "When the two samples are related or dependent (e.g., before-after measurements on the same subjects).",
      "C": "When the population standard deviations are known.",
      "D": "When the data is categorical."
    },
    "answer": "B",
    "explanation": "Paired t-tests are for dependent samples where each observation in one sample has a natural pairing with an observation in the other sample.",
    "topic": "T-Tests",
    "difficulty": "Medium"
  },
  {
    "id": 147,
    "question": "What is the purpose of conducting a post-hoc test after a significant ANOVA result?",
    "options": {
      "A": "To confirm the overall significance.",
      "B": "To determine which specific group means differ significantly from each other.",
      "C": "To calculate the grand mean of all groups.",
      "D": "To adjust the p-value for multiple comparisons before the ANOVA."
    },
    "answer": "B",
    "explanation": "ANOVA tells you if there's an overall difference, but post-hoc tests pinpoint exactly where those differences lie between specific pairs of groups.",
    "topic": "ANOVA",
    "difficulty": "Medium"
  },
  {
    "id": 148,
    "question": "What does a correlation coefficient of -0.9 imply?",
    "options": {
      "A": "A weak positive linear relationship.",
      "B": "A strong positive linear relationship.",
      "C": "A strong negative linear relationship.",
      "D": "No linear relationship."
    },
    "answer": "C",
    "explanation": "A value close to -1 indicates a strong inverse linear relationship.",
    "topic": "Correlation",
    "difficulty": "Medium"
  },
  {
    "id": 149,
    "question": "What is the purpose of a Bonferroni correction in hypothesis testing?",
    "options": {
      "A": "To increase the power of the test.",
      "B": "To reduce the chance of a Type I error when performing multiple comparisons.",
      "C": "To decrease the sample size required for significance.",
      "D": "To make the distribution normal."
    },
    "answer": "B",
    "explanation": "Bonferroni correction adjusts the significance level (alpha) for each individual test to control the overall family-wise error rate when multiple comparisons are made.",
    "topic": "Multiple Comparisons",
    "difficulty": "Medium"
  },
  {
    "id": 150,
    "question": "In a clinical trial, patients are randomly assigned to either receive a new drug or a placebo. Which type of study design is this?",
    "options": {
      "A": "Observational study",
      "B": "Survey",
      "C": "Experimental study",
      "D": "Case study"
    },
    "answer": "C",
    "explanation": "Random assignment and intervention (drug vs. placebo) are characteristics of an experimental study.",
    "topic": "Study Design",
    "difficulty": "Medium"
  },
  {
    "id": 151,
    "question": "What is the purpose of randomization in an experimental study?",
    "options": {
      "A": "To make the study easier to conduct.",
      "B": "To ensure participants are satisfied.",
      "C": "To minimize bias and ensure comparability between groups.",
      "D": "To reduce the sample size needed."
    },
    "answer": "C",
    "explanation": "Randomization helps to balance out confounding variables across treatment groups, making the groups comparable.",
    "topic": "Study Design",
    "difficulty": "Medium"
  },
  {
    "id": 152,
    "question": "What is a confounding variable?",
    "options": {
      "A": "A variable that has no effect on the outcome.",
      "B": "A variable that is directly manipulated by the researcher.",
      "C": "A variable that influences both the dependent variable and the independent variable, potentially distorting the true relationship.",
      "D": "A variable that is measured after the experiment."
    },
    "answer": "C",
    "explanation": "Confounding variables create a spurious association, making it seem like there's a relationship where none exists or masking a true one.",
    "topic": "Research Design",
    "difficulty": "Medium"
  },
  {
    "id": 153,
    "question": "Which of the following is a key characteristic of an observational study?",
    "options": {
      "A": "The researcher actively intervenes and assigns treatments.",
      "B": "Participants are randomly assigned to groups.",
      "C": "The researcher observes and collects data without manipulating variables.",
      "D": "It always establishes cause-and-effect relationships."
    },
    "answer": "C",
    "explanation": "In observational studies, researchers simply observe and measure characteristics of a population without any manipulation.",
    "topic": "Study Design",
    "difficulty": "Medium"
  },
  {
    "id": 154,
    "question": "What is statistical power in hypothesis testing?",
    "options": {
      "A": "The probability of making a Type I error.",
      "B": "The probability of making a Type II error.",
      "C": "The probability of correctly rejecting a false null hypothesis.",
      "D": "The significance level of the test."
    },
    "answer": "C",
    "explanation": "Power (1 - β) is the probability that a test will correctly detect an effect if there is an effect to be detected.",
    "topic": "Hypothesis Testing Concepts",
    "difficulty": "Medium"
  },
  {
    "id": 155,
    "question": "How can you increase the power of a statistical test?",
    "options": {
      "A": "Decrease the sample size.",
      "B": "Increase the significance level (α).",
      "C": "Decrease the effect size.",
      "D": "Use a one-tailed test when a two-tailed test is appropriate."
    },
    "answer": "B",
    "explanation": "Increasing alpha (e.g., from 0.05 to 0.10) makes it easier to reject the null hypothesis, thus increasing power, but also increases the risk of Type I error.",
    "topic": "Hypothesis Testing Concepts",
    "difficulty": "Medium"
  },
  {
    "id": 156,
    "question": "What is an F-statistic used for?",
    "options": {
      "A": "To test the correlation between two variables.",
      "B": "To compare the means of two groups.",
      "C": "To test the equality of variances among two or more groups (in ANOVA, it's ratio of between-group variance to within-group variance).",
      "D": "To calculate the median of a dataset."
    },
    "answer": "C",
    "explanation": "The F-statistic is used in ANOVA to determine if the variability between group means is significantly larger than the variability within groups.",
    "topic": "ANOVA",
    "difficulty": "Medium"
  },
  {
    "id": 157,
    "question": "What does a negatively correlated scatter plot typically show?",
    "options": {
      "A": "Points clustered around an upward sloping line.",
      "B": "Points scattered randomly with no clear pattern.",
      "C": "Points clustered around a downward sloping line.",
      "D": "Points forming a U-shape."
    },
    "answer": "C",
    "explanation": "A downward sloping pattern indicates that as one variable increases, the other tends to decrease.",
    "topic": "Correlation",
    "difficulty": "Medium"
  },
  {
    "id": 158,
    "question": "What is the purpose of a residual plot in regression analysis?",
    "options": {
      "A": "To visualize the relationship between the independent and dependent variables.",
      "B": "To check the assumptions of linearity, homoscedasticity, and independence of errors.",
      "C": "To determine the R-squared value.",
      "D": "To identify outliers in the independent variable."
    },
    "answer": "B",
    "explanation": "Residual plots help diagnose whether the assumptions of linear regression are met.",
    "topic": "Regression Analysis",
    "difficulty": "Medium"
  },
  {
    "id": 159,
    "question": "What does homoscedasticity mean in the context of linear regression?",
    "options": {
      "A": "The errors are normally distributed.",
      "B": "The variance of the errors is constant across all levels of the independent variable.",
      "C": "There is no correlation between the independent and dependent variables.",
      "D": "The relationship between variables is perfectly linear."
    },
    "answer": "B",
    "explanation": "Homoscedasticity is the assumption that the variance of the residuals (errors) is constant at every level of the independent variable.",
    "topic": "Regression Assumptions",
    "difficulty": "Medium"
  },
  {
    "id": 160,
    "question": "What is the bias-variance trade-off in statistical modeling?",
    "options": {
      "A": "A trade-off between the mean and the standard deviation of data.",
      "B": "A trade-off between simplifying a model (increasing bias) and making it more complex (increasing variance).",
      "C": "A trade-off between sample size and significance level.",
      "D": "A trade-off between Type I and Type II errors."
    },
    "answer": "B",
    "explanation": "It's a central concept in machine learning and statistics, where reducing bias often increases variance, and vice versa.",
    "topic": "Model Selection",
    "difficulty": "Medium"
  },
  {
    "id": 161,
    "question": "What is bootstrapping in statistics?",
    "options": {
      "A": "A method for collecting primary data.",
      "B": "A resampling technique used to estimate population parameters by repeatedly sampling with replacement from the observed data.",
      "C": "A type of hypothesis test.",
      "D": "A method for visualizing high-dimensional data."
    },
    "answer": "B",
    "explanation": "Bootstrapping allows for estimation of the sampling distribution of a statistic without strong assumptions about the population distribution.",
    "topic": "Resampling Methods",
    "difficulty": "Medium"
  },
  {
    "id": 162,
    "question": "What is cross-validation used for in statistical modeling?",
    "options": {
      "A": "To increase the sample size.",
      "B": "To estimate how well a model will generalize to new, unseen data.",
      "C": "To find the mean of a dataset.",
      "D": "To correct for multicollinearity."
    },
    "answer": "B",
    "explanation": "Cross-validation helps evaluate a model's performance on unseen data, preventing overfitting.",
    "topic": "Model Evaluation",
    "difficulty": "Medium"
  },
  {
    "id": 163,
    "question": "Which of the following is an example of an unethical practice in statistics?",
    "options": {
      "A": "Collecting data from a representative sample.",
      "B": "Reporting all findings, even those that don't support the hypothesis.",
      "C": "Manipulating data or selectively reporting results to support a desired conclusion.",
      "D": "Using appropriate statistical tests."
    },
    "answer": "C",
    "explanation": "Data manipulation and selective reporting are serious ethical violations in statistical practice.",
    "topic": "Ethics in Statistics",
    "difficulty": "Medium"
  },
  {
    "id": 164,
    "question": "What does 'statistical significance' mean?",
    "options": {
      "A": "The observed effect is large and important.",
      "B": "The observed effect is unlikely to have occurred by chance, assuming the null hypothesis is true.",
      "C": "The results are perfectly accurate.",
      "D": "The sample size was very large."
    },
    "answer": "B",
    "explanation": "Statistical significance means that the observed result is not likely due to random chance.",
    "topic": "Hypothesis Testing Concepts",
    "difficulty": "Medium"
  },
  {
    "id": 165,
    "question": "What is the difference between descriptive and inferential statistics?",
    "options": {
      "A": "Descriptive statistics predict future events; inferential statistics summarize past data.",
      "B": "Descriptive statistics generalize from samples; inferential statistics describe populations.",
      "C": "Descriptive statistics describe and summarize data; inferential statistics draw conclusions about a population based on a sample.",
      "D": "They are two names for the same thing."
    },
    "answer": "C",
    "explanation": "Descriptive statistics deal with observed data, while inferential statistics use sample data to make broader conclusions.",
    "topic": "Types of Statistics",
    "difficulty": "Medium"
  },
  {
    "id": 166,
    "question": "A researcher is studying the relationship between hours spent studying and exam scores. What type of variable is 'exam scores'?",
    "options": {
      "A": "Independent variable",
      "B": "Dependent variable",
      "C": "Categorical variable",
      "D": "Confounding variable"
    },
    "answer": "B",
    "explanation": "Exam scores are the outcome variable that is assumed to be affected by the hours spent studying.",
    "topic": "Variables",
    "difficulty": "Medium"
  },
  {
    "id": 167,
    "question": "What is the goal of a matched-pairs design in an experiment?",
    "options": {
      "A": "To create unequal groups for comparison.",
      "B": "To ensure that participants are randomly selected from the population.",
      "C": "To control for confounding variables by pairing similar subjects or using the same subjects for both conditions.",
      "D": "To simplify data analysis."
    },
    "answer": "C",
    "explanation": "Matched-pairs designs reduce variability and control for individual differences by matching participants or using repeated measures.",
    "topic": "Study Design",
    "difficulty": "Medium"
  },
  {
    "id": 168,
    "question": "You calculate a sample mean of 70 from a sample of 36 observations. The population standard deviation is known to be 12. What is the standard error of the mean?",
    "options": {
      "A": "0.33",
      "B": "2",
      "C": "12",
      "D": "36"
    },
    "answer": "B",
    "explanation": "Standard Error = σ / sqrt(n) = 12 / sqrt(36) = 12 / 6 = 2.",
    "topic": "Sampling Distributions",
    "difficulty": "Medium"
  },
  {
    "id": 169,
    "question": "What is a parameter in the context of a statistical model?",
    "options": {
      "A": "A value calculated from a sample.",
      "B": "A characteristic of the population being estimated or tested.",
      "C": "A type of statistical error.",
      "D": "The number of observations in a dataset."
    },
    "answer": "B",
    "explanation": "Parameters are unknown characteristics of a population that we often try to estimate using sample statistics.",
    "topic": "General Statistical Concepts",
    "difficulty": "Medium"
  },
  {
    "id": 170,
    "question": "What is the purpose of data visualization?",
    "options": {
      "A": "To hide complex data patterns.",
      "B": "To make data analysis more difficult.",
      "C": "To communicate patterns, trends, and insights from data effectively.",
      "D": "To collect raw data."
    },
    "answer": "C",
    "explanation": "Data visualization transforms data into graphical representations for easier understanding and exploration.",
    "topic": "Data Visualization",
    "difficulty": "Medium"
  },
  {
    "id": 171,
    "question": "Which measure of central tendency is appropriate for ordinal data?",
    "options": {
      "A": "Mean",
      "B": "Median",
      "C": "Mode",
      "D": "Both Median and Mode"
    },
    "answer": "D",
    "explanation": "For ordinal data, categories have an order, so both mode (most frequent category) and median (middle ordered category) are meaningful. Mean is not appropriate as intervals are not equal.",
    "topic": "Measures of Central Tendency",
    "difficulty": "Medium"
  },
  {
    "id": 172,
    "question": "What does a confidence level of 99% imply about a confidence interval?",
    "options": {
      "A": "The interval contains 99% of the population data.",
      "B": "There is a 1% chance the interval contains the population parameter.",
      "C": "If we repeat the sampling many times, 99% of the constructed intervals would contain the true population parameter.",
      "D": "The sample mean will be within 99% of the population mean."
    },
    "answer": "C",
    "explanation": "Confidence level refers to the long-run proportion of intervals that will capture the true parameter.",
    "topic": "Estimation",
    "difficulty": "Medium"
  },
  {
    "id": 173,
    "question": "What is the primary difference between a population distribution and a sampling distribution?",
    "options": {
      "A": "A population distribution is always normal, while a sampling distribution is never normal.",
      "B": "A population distribution describes individual data points; a sampling distribution describes sample statistics (e.g., sample means) from repeated samples.",
      "C": "A population distribution is based on a sample; a sampling distribution is based on the entire population.",
      "D": "They are the same thing."
    },
    "answer": "B",
    "explanation": "The population distribution describes the values of individuals, while a sampling distribution describes the values of statistics computed from many samples.",
    "topic": "Sampling Distributions",
    "difficulty": "Medium"
  },
  {
    "id": 174,
    "question": "When is a permutation used instead of a combination?",
    "options": {
      "A": "When the number of items is small.",
      "B": "When the order of selection does not matter.",
      "C": "When the order of selection matters.",
      "D": "When sampling with replacement."
    },
    "answer": "C",
    "explanation": "Permutations count arrangements where the sequence of items is important.",
    "topic": "Combinatorics",
    "difficulty": "Medium"
  },
  {
    "id": 175,
    "question": "In the context of probability, what does 'conditional probability' mean?",
    "options": {
      "A": "The probability of an event occurring, given that another event has already occurred.",
      "B": "The probability of two events occurring simultaneously.",
      "C": "The sum of probabilities of two events.",
      "D": "The probability of an event occurring regardless of any other event."
    },
    "answer": "A",
    "explanation": "Conditional probability, denoted P(A|B), is the probability of event A given that event B has occurred.",
    "topic": "Probability Concepts",
    "difficulty": "Medium"
  },
  {
    "id": 176,
    "question": "What is the formula for the conditional probability of A given B, P(A|B)?",
    "options": {
      "A": "P(A) + P(B)",
      "B": "P(A) * P(B)",
      "C": "P(A and B) / P(B)",
      "D": "P(B) / P(A and B)"
    },
    "answer": "C",
    "explanation": "This is the definition of conditional probability, assuming P(B) > 0.",
    "topic": "Probability Rules",
    "difficulty": "Medium"
  },
  {
    "id": 177,
    "question": "What is the term for a bell-shaped, symmetrical probability distribution that is widely used in statistics?",
    "options": {
      "A": "Uniform distribution",
      "B": "Exponential distribution",
      "C": "Normal distribution",
      "D": "Binomial distribution"
    },
    "answer": "C",
    "explanation": "The normal distribution (Gaussian distribution) is characterized by its bell shape and symmetry around the mean.",
    "topic": "Probability Distributions",
    "difficulty": "Medium"
  },
  {
    "id": 178,
    "question": "If a dataset has a mean of 100 and a standard deviation of 15, what value corresponds to a Z-score of 1.0?",
    "options": {
      "A": "85",
      "B": "100",
      "C": "115",
      "D": "130"
    },
    "answer": "C",
    "explanation": "X = μ + Z*σ = 100 + 1*15 = 115.",
    "topic": "Z-Scores",
    "difficulty": "Medium"
  },
  {
    "id": 179,
    "question": "Which of the following is an assumption of the Pearson correlation coefficient?",
    "options": {
      "A": "The variables are categorical.",
      "B": "The relationship between variables is nonlinear.",
      "C": "The variables are normally distributed.",
      "D": "The relationship between the two variables is linear."
    },
    "answer": "D",
    "explanation": "Pearson's r measures the strength and direction of a linear relationship. Non-linear relationships might exist but won't be captured well by Pearson's r.",
    "topic": "Correlation",
    "difficulty": "Medium"
  },
  {
    "id": 180,
    "question": "What is the effect of increasing the confidence level (e.g., from 95% to 99%) on the width of a confidence interval, assuming the sample size and standard deviation remain constant?",
    "options": {
      "A": "The width decreases.",
      "B": "The width increases.",
      "C": "The width remains the same.",
      "D": "The interval shifts to the left."
    },
    "answer": "B",
    "explanation": "To be more confident that the interval contains the true parameter, the interval must be wider to encompass more potential values.",
    "topic": "Estimation",
    "difficulty": "Medium"
  },
  {
    "id": 181,
    "question": "What is the purpose of a stem-and-leaf plot?",
    "options": {
      "A": "To show the trend of data over time.",
      "B": "To display the distribution of a quantitative variable while retaining individual data values.",
      "C": "To compare proportions of different categories.",
      "D": "To visualize the relationship between two variables."
    },
    "answer": "B",
    "explanation": "Stem-and-leaf plots are a way to organize numerical data in a graphical display that shows both the shape of the distribution and the actual values.",
    "topic": "Data Visualization",
    "difficulty": "Medium"
  },
  {
    "id": 182,
    "question": "What is the coefficient of variation (CV)?",
    "options": {
      "A": "A measure of central tendency.",
      "B": "A measure of relative variability, expressed as a percentage.",
      "C": "A measure of skewness.",
      "D": "A type of correlation coefficient."
    },
    "answer": "B",
    "explanation": "CV = (Standard Deviation / Mean) * 100%. It allows for comparison of variability between datasets with different units or scales.",
    "topic": "Measures of Dispersion",
    "difficulty": "Medium"
  },
  {
    "id": 183,
    "question": "When would you use a nonparametric test instead of a parametric test?",
    "options": {
      "A": "When the data is normally distributed.",
      "B": "When the sample size is very large.",
      "C": "When the assumptions of parametric tests (e.g., normality, homogeneity of variance) are violated or when dealing with ordinal/nominal data.",
      "D": "When you know the population parameters."
    },
    "answer": "C",
    "explanation": "Nonparametric tests do not rely on specific distributional assumptions and are suitable for data that doesn't meet parametric test requirements.",
    "topic": "Nonparametric Tests",
    "difficulty": "Medium"
  },
  {
    "id": 184,
    "question": "What is a common nonparametric alternative to the independent samples t-test?",
    "options": {
      "A": "ANOVA",
      "B": "Mann-Whitney U test",
      "C": "Paired t-test",
      "D": "Chi-square test"
    },
    "answer": "B",
    "explanation": "The Mann-Whitney U test (also known as the Wilcoxon rank-sum test) is used to compare two independent groups when the data is not normally distributed or is ordinal.",
    "topic": "Nonparametric Tests",
    "difficulty": "Medium"
  },
  {
    "id": 185,
    "question": "What is the purpose of a goodness-of-fit test?",
    "options": {
      "A": "To compare two means.",
      "B": "To assess how well observed data fit an expected distribution.",
      "C": "To determine the correlation between two variables.",
      "D": "To predict future values."
    },
    "answer": "B",
    "explanation": "Goodness-of-fit tests, like the chi-square goodness-of-fit test, evaluate if observed frequencies differ significantly from expected frequencies under a hypothesized distribution.",
    "topic": "Chi-Square Test",
    "difficulty": "Medium"
  },
  {
    "id": 186,
    "question": "What does a small p-value (e.g., < 0.01) indicate in hypothesis testing?",
    "options": {
      "A": "The null hypothesis is definitely true.",
      "B": "There is strong evidence against the null hypothesis.",
      "C": "The alternative hypothesis is definitely false.",
      "D": "The results are not practically significant."
    },
    "answer": "B",
    "explanation": "A small p-value suggests that the observed data is unlikely to have occurred by chance if the null hypothesis were true, leading to rejection of the null.",
    "topic": "Hypothesis Testing Concepts",
    "difficulty": "Medium"
  },
  {
    "id": 187,
    "question": "Which of the following describes the level of measurement for a variable like 'number of correct answers on a test'?",
    "options": {
      "A": "Nominal",
      "B": "Ordinal",
      "C": "Interval",
      "D": "Ratio"
    },
    "answer": "D",
    "explanation": "Number of correct answers has a true zero (0 correct answers) and meaningful ratios (8 correct is twice as many as 4 correct).",
    "topic": "Levels of Measurement",
    "difficulty": "Medium"
  },
  {
    "id": 188,
    "question": "What is the difference between a one-tailed and a two-tailed hypothesis test?",
    "options": {
      "A": "A one-tailed test has two critical regions, a two-tailed test has one.",
      "B": "A one-tailed test tests for an effect in a specific direction; a two-tailed test tests for an effect in either direction.",
      "C": "A one-tailed test is more powerful than a two-tailed test.",
      "D": "One-tailed tests are used for categorical data, two-tailed for continuous."
    },
    "answer": "B",
    "explanation": "A one-tailed test is directional (e.g., 'greater than'), while a two-tailed test is non-directional (e.g., 'different from').",
    "topic": "Hypothesis Testing",
    "difficulty": "Medium"
  },
  {
    "id": 189,
    "question": "What is the primary assumption for linear regression regarding the residuals?",
    "options": {
      "A": "They must be increasing.",
      "B": "They must be correlated.",
      "C": "They must be normally distributed with a mean of zero and constant variance.",
      "D": "They must be positive."
    },
    "answer": "C",
    "explanation": "Assumptions about residuals (normality, zero mean, homoscedasticity, independence) are crucial for valid inferences from linear regression.",
    "topic": "Regression Assumptions",
    "difficulty": "Medium"
  },
  {
    "id": 190,
    "question": "When interpreting a 95% confidence interval, if it contains the null value (e.g., 0 for a difference in means), what does this suggest about the corresponding two-sided hypothesis test at α = 0.05?",
    "options": {
      "A": "Reject the null hypothesis.",
      "B": "Fail to reject the null hypothesis.",
      "C": "The alternative hypothesis is true.",
      "D": "The confidence interval is too wide."
    },
    "answer": "B",
    "explanation": "If the null value falls within the confidence interval, it implies that the null hypothesis is a plausible value for the population parameter, and thus, we do not have enough evidence to reject it.",
    "topic": "Confidence Intervals and Hypothesis Testing",
    "difficulty": "Medium"
  },
  {
    "id": 191,
    "question": "What is the primary difference between a sample variance and a population variance?",
    "options": {
      "A": "Sample variance is always larger than population variance.",
      "B": "Population variance is always calculated, sample variance is estimated.",
      "C": "Sample variance uses 'n-1' in the denominator for unbiased estimation, while population variance uses 'N'.",
      "D": "There is no difference, they are identical."
    },
    "answer": "C",
    "explanation": "Dividing by 'n-1' (degrees of freedom) for sample variance corrects for the tendency of sample variance to underestimate population variance.",
    "topic": "Measures of Dispersion",
    "difficulty": "Medium"
  },
  {
    "id": 192,
    "question": "Which of the following is a limitation of correlation analysis?",
    "options": {
      "A": "It can establish cause-and-effect relationships.",
      "B": "It only works for categorical data.",
      "C": "Correlation does not imply causation.",
      "D": "It can only be used with very large datasets."
    },
    "answer": "C",
    "explanation": "A strong correlation indicates an association, but it does not mean that one variable causes the other. There could be confounding variables or reverse causality.",
    "topic": "Correlation",
    "difficulty": "Medium"
  },
  {
    "id": 193,
    "question": "What is a parameter in the context of probability distributions (e.g., mean and standard deviation for a normal distribution)?",
    "options": {
      "A": "A value derived from a sample.",
      "B": "A constant that defines a specific probability distribution.",
      "C": "The outcome of an experiment.",
      "D": "The range of possible values."
    },
    "answer": "B",
    "explanation": "Parameters (like mean μ and standard deviation σ for a normal distribution) define the shape and characteristics of a theoretical probability distribution.",
    "topic": "Probability Distributions",
    "difficulty": "Medium"
  },
  {
    "id": 194,
    "question": "What is the purpose of data normalization (or standardization)?",
    "options": {
      "A": "To make the data perfectly normal.",
      "B": "To ensure all features have the same mean and standard deviation, often to prepare data for machine learning algorithms.",
      "C": "To remove outliers from the data.",
      "D": "To convert continuous data into discrete data."
    },
    "answer": "B",
    "explanation": "Normalization scales features to a comparable range, preventing features with larger values from dominating algorithms.",
    "topic": "Data Preprocessing",
    "difficulty": "Medium"
  },
  {
    "id": 195,
    "question": "What is a simple random sample without replacement?",
    "options": {
      "A": "Each selected item is put back into the population before the next selection.",
      "B": "Each individual has an equal chance of being selected, and once selected, cannot be selected again.",
      "C": "Individuals are selected based on their availability.",
      "D": "The population is divided into groups, and entire groups are selected."
    },
    "answer": "B",
    "explanation": "Sampling without replacement means that once an item is selected, it's not put back into the pool for subsequent selections.",
    "topic": "Sampling Methods",
    "difficulty": "Medium"
  },
  {
    "id": 196,
    "question": "What is the primary characteristic of a skewed distribution?",
    "options": {
      "A": "It is perfectly symmetrical.",
      "B": "It has multiple peaks.",
      "C": "Its tails are unequal in length, indicating asymmetry.",
      "D": "It has no outliers."
    },
    "answer": "C",
    "explanation": "Skewness describes the asymmetry of a distribution, with one tail being longer than the other.",
    "topic": "Distribution Shapes",
    "difficulty": "Medium"
  },
  {
    "id": 197,
    "question": "In the context of the Central Limit Theorem, what is the minimum sample size generally considered 'large enough' for the sampling distribution of the mean to be approximately normal, even if the population distribution is not normal?",
    "options": {
      "A": "5",
      "B": "10",
      "C": "30",
      "D": "100"
    },
    "answer": "C",
    "explanation": "A common rule of thumb for the Central Limit Theorem is that a sample size of n ≥ 30 is generally sufficient.",
    "topic": "Central Limit Theorem",
    "difficulty": "Medium"
  },
  {
    "id": 198,
    "question": "What does the term 'degrees of freedom' represent in statistical tests?",
    "options": {
      "A": "The number of observations in a dataset.",
      "B": "The number of independent pieces of information used to estimate a parameter or calculate a statistic.",
      "C": "The number of variables in a model.",
      "D": "The range of the data."
    },
    "answer": "B",
    "explanation": "Degrees of freedom refer to the number of values in a final calculation of a statistic that are free to vary.",
    "topic": "General Statistical Concepts",
    "difficulty": "Medium"
  },
  {
    "id": 199,
    "question": "What is a key difference between a histogram and a bar graph?",
    "options": {
      "A": "Histograms are for qualitative data, bar graphs are for quantitative data.",
      "B": "Histograms have gaps between bars, bar graphs do not.",
      "C": "Histograms show the distribution of continuous data in bins; bar graphs compare discrete categories.",
      "D": "Bar graphs show trends over time, histograms do not."
    },
    "answer": "C",
    "explanation": "This is the primary distinction in their application: histograms for continuous data distribution, bar graphs for discrete categories.",
    "topic": "Data Visualization",
    "difficulty": "Medium"
  },
  {
    "id": 200,
    "question": "What is the primary purpose of a statistical hypothesis?",
    "options": {
      "A": "To prove a theory to be absolutely true.",
      "B": "To make a statement about a population parameter that can be tested using sample data.",
      "C": "To summarize the characteristics of a sample.",
      "D": "To create new data points."
    },
    "answer": "B",
    "explanation": "Hypotheses are formulated as testable statements about population parameters.",
    "topic": "Hypothesis Testing Basics",
    "difficulty": "Medium"
  },
  {
    "id": 201,
    "question": "A researcher conducts an experiment to compare the effectiveness of three different teaching methods. After collecting data, they perform an ANOVA test and obtain a p-value of 0.001. They then proceed to conduct multiple pairwise t-tests without any correction. What type of error are they increasing the risk of, and why?",
    "options": {
      "A": "Type I error; due to increased family-wise error rate from multiple comparisons.",
      "B": "Type II error; because the ANOVA was significant.",
      "C": "Type I error; because the sample size was too small.",
      "D": "Type II error; by not using a post-hoc test."
    },
    "answer": "A",
    "explanation": "Performing multiple comparisons without adjustment (like Bonferroni or Tukey HSD) inflates the probability of making at least one Type I error across the set of tests.",
    "topic": "Multiple Comparisons / Hypothesis Testing Errors",
    "difficulty": "Hard"
  },
  {
    "id": 202,
    "question": "Explain the concept of 'power' in hypothesis testing and how it relates to Type II error.",
    "options": {
      "A": "Power is the probability of rejecting a false null hypothesis. It is 1 - P(Type II Error).",
      "B": "Power is the probability of failing to reject a true null hypothesis. It is equal to P(Type II Error).",
      "C": "Power is the probability of rejecting a true null hypothesis. It is equal to P(Type I Error).",
      "D": "Power is the probability of failing to reject a false null hypothesis. It is 1 - P(Type I Error)."
    },
    "answer": "A",
    "explanation": "Power is the ability of a test to correctly detect an effect (reject H0 when it is false). A Type II error (β) is failing to reject a false H0, so Power = 1 - β.",
    "topic": "Hypothesis Testing Concepts",
    "difficulty": "Hard"
  },
  {
    "id": 203,
    "question": "Describe the assumptions of linear regression (BLUE assumptions, excluding normality) and explain why each is important.",
    "options": {
      "A": "Normality of residuals, homoscedasticity, independence of errors. Important for valid inference and efficient estimators.",
      "B": "Linearity, independence of observations, homoscedasticity, normality of residuals. Important for unbiased, efficient, and consistent estimators, and valid inference.",
      "C": "Random sampling, large sample size. Important for generalizability.",
      "D": "High R-squared, low p-value. Important for model fit."
    },
    "answer": "B",
    "explanation": "BLUE stands for Best Linear Unbiased Estimators, which are achieved under these assumptions: Linearity (correct functional form), Independence of observations, Homoscedasticity (constant variance of errors), and Normality of residuals. These ensure that regression coefficients are unbiased, efficient, and that hypothesis tests and confidence intervals are valid.",
    "topic": "Regression Assumptions",
    "difficulty": "Hard"
  },
  {
    "id": 204,
    "question": "You are comparing the means of two independent groups using a t-test. You find that the variances of the two groups are significantly different. Which version of the t-test should you use, and why?",
    "options": {
      "A": "Pooled t-test; because it's more powerful.",
      "B": "Welch's t-test (unequal variances t-test); because it accounts for the difference in variances, providing a more robust result.",
      "C": "Paired t-test; because the samples are independent.",
      "D": "ANOVA; because it's always better for comparing means."
    },
    "answer": "B",
    "explanation": "When the assumption of equal variances is violated, Welch's t-test is appropriate as it adjusts the degrees of freedom to account for the unequal variances, leading to a more accurate p-value.",
    "topic": "T-Tests",
    "difficulty": "Hard"
  },
  {
    "id": 205,
    "question": "What is the difference between a binomial distribution and a Poisson distribution? Provide an example where each would be appropriate.",
    "options": {
      "A": "Binomial for continuous events, Poisson for discrete. E.g., height (binomial), number of cars passing a point (Poisson).",
      "B": "Binomial for number of successes in fixed trials, Poisson for number of events in a fixed interval of time/space. E.g., coin flips (binomial), calls to a call center per hour (Poisson).",
      "C": "Binomial for events with unknown rates, Poisson for events with known rates. E.g., customer arrivals (binomial), disease outbreaks (Poisson).",
      "D": "They are interchangeable distributions for any discrete data."
    },
    "answer": "B",
    "explanation": "Binomial describes the number of successes in a fixed number of independent Bernoulli trials (n trials, p probability of success). Poisson describes the number of events occurring in a fixed interval of time or space, where events occur with a known average rate and independently of the time since the last event.",
    "topic": "Probability Distributions",
    "difficulty": "Hard"
  },
  {
    "id": 206,
    "question": "Explain the concept of 'degrees of freedom' in the context of a chi-square test of independence.",
    "options": {
      "A": "It's the total number of observations in the contingency table.",
      "B": "It's the number of cells in the table minus one.",
      "C": "It's related to the number of independent pieces of information used to calculate the test statistic, typically (rows - 1) * (columns - 1).",
      "D": "It determines the sample size needed for the test."
    },
    "answer": "C",
    "explanation": "For a chi-square test of independence, degrees of freedom are calculated as (number of rows - 1) * (number of columns - 1). This represents the number of cell frequencies that are free to vary once the marginal totals and overall total are fixed.",
    "topic": "Chi-Square Test / Degrees of Freedom",
    "difficulty": "Hard"
  },
  {
    "id": 207,
    "question": "You are building a predictive model and notice that it performs exceptionally well on the training data but poorly on new, unseen data. What statistical phenomenon is likely occurring, and how can you mitigate it?",
    "options": {
      "A": "Underfitting; add more features.",
      "B": "Overfitting; use cross-validation, regularization, or simplify the model.",
      "C": "Sampling bias; increase sample size.",
      "D": "Type I error; lower the significance level."
    },
    "answer": "B",
    "explanation": "Overfitting occurs when a model learns the training data too well, including its noise, and thus fails to generalize. Mitigation strategies include cross-validation, regularization (e.g., L1, L2), reducing model complexity, or increasing training data.",
    "topic": "Model Evaluation / Overfitting",
    "difficulty": "Hard"
  },
  {
    "id": 208,
    "question": "What is the difference between a confidence interval and a prediction interval in regression?",
    "options": {
      "A": "A confidence interval predicts the future value of a single observation, while a prediction interval estimates the mean response.",
      "B": "A confidence interval estimates the mean response for a given X, while a prediction interval predicts the individual response for a given X.",
      "C": "A confidence interval is always wider than a prediction interval.",
      "D": "They are the same, just used in different contexts."
    },
    "answer": "B",
    "explanation": "A confidence interval for the mean response estimates the mean of the dependent variable for all subjects with a given independent variable value. A prediction interval for an individual observation predicts the value of a single future observation for a given independent variable value. Prediction intervals are always wider because they account for both the uncertainty in the mean response and the individual variability.",
    "topic": "Regression Analysis / Intervals",
    "difficulty": "Hard"
  },
  {
    "id": 209,
    "question": "Explain the concept of 'multicollinearity' in multiple linear regression. What are its consequences, and how can it be detected and addressed?",
    "options": {
      "A": "It's when independent variables are unrelated. Consequences: strong model fit. Detection: VIF. Address: add more variables.",
      "B": "It's when the dependent variable is correlated with itself. Consequences: biased coefficients. Detection: scatter plots. Address: remove outliers.",
      "C": "It's when two or more independent variables are highly correlated. Consequences: unstable/inflated coefficient standard errors, difficulty interpreting individual coefficients. Detection: VIF, correlation matrix. Address: remove/combine variables, principal component analysis.",
      "D": "It's when the residuals are correlated. Consequences: invalid p-values. Detection: Durbin-Watson. Address: time series models."
    },
    "answer": "C",
    "explanation": "Multicollinearity inflates the variance of coefficient estimates, making them unstable and difficult to interpret. It's detected using Variance Inflation Factor (VIF) or by examining the correlation matrix. Addressing it involves removing highly correlated variables, combining them, or using techniques like PCA.",
    "topic": "Regression Assumptions / Multicollinearity",
    "difficulty": "Hard"
  },
  {
    "id": 210,
    "question": "What is the likelihood function in statistical inference, and how is it used in Maximum Likelihood Estimation (MLE)?",
    "options": {
      "A": "It's the probability of observing the parameters given the data. MLE finds parameters that minimize this.",
      "B": "It's the probability of observing the data given the parameters. MLE finds the parameters that maximize this probability.",
      "C": "It's a measure of model fit. MLE finds parameters that make the model fit best.",
      "D": "It's a hypothesis test statistic. MLE tests if parameters are significant."
    },
    "answer": "B",
    "explanation": "The likelihood function (L(θ|data)) is the probability of observing the given data as a function of the parameters (θ) of a statistical model. MLE finds the parameter values that make the observed data most probable, i.e., maximize the likelihood function.",
    "topic": "Statistical Inference / MLE",
    "difficulty": "Hard"
  },
  {
    "id": 211,
    "question": "Explain the concept of 'Bayesian inference' and how it differs from 'frequentist inference'.",
    "options": {
      "A": "Bayesian inference uses only current data; frequentist uses prior beliefs. They are identical in practice.",
      "B": "Bayesian inference treats parameters as random variables with probability distributions (prior and posterior); frequentist inference treats parameters as fixed, unknown constants. Bayesian updates beliefs with data; frequentist focuses on probability of data given fixed parameters.",
      "C": "Bayesian inference is for small samples; frequentist for large samples. Bayesian doesn't use p-values.",
      "D": "Frequentist inference calculates posterior probabilities; Bayesian calculates p-values."
    },
    "answer": "B",
    "explanation": "Frequentist inference views parameters as fixed and unknown, relying on the probability of observed data given those parameters. Bayesian inference treats parameters as random variables, incorporating prior beliefs about parameters and updating them with observed data to obtain posterior distributions.",
    "topic": "Statistical Inference / Bayesian vs. Frequentist",
    "difficulty": "Hard"
  },
  {
    "id": 212,
    "question": "What is the difference between a Type I error and a Type II error, and how are their probabilities controlled in hypothesis testing?",
    "options": {
      "A": "Type I: Rejecting H0 when true (α). Type II: Failing to reject H0 when false (β). α is set by researcher; β is inversely related to power, increased by sample size/effect size.",
      "B": "Type I: Failing to reject H0 when true. Type II: Rejecting H0 when false. Both controlled by p-value.",
      "C": "Type I: Occurs when sample size is too small. Type II: Occurs when significance level is too high.",
      "D": "They are both controlled by adjusting the confidence interval."
    },
    "answer": "A",
    "explanation": "Type I error (false positive) is controlled by setting the significance level (α). Type II error (false negative) is inversely related to statistical power, which can be increased by increasing sample size, effect size, or alpha.",
    "topic": "Hypothesis Testing Errors",
    "difficulty": "Hard"
  },
  {
    "id": 213,
    "question": "Explain the concept of 'p-hacking' and why it is an unethical practice in statistical research.",
    "options": {
      "A": "P-hacking is a valid method for finding significant results more quickly. It's ethical if disclosed.",
      "B": "P-hacking is the practice of selectively choosing data or analysis methods to achieve a statistically significant p-value. It's unethical because it leads to false positives and non-reproducible research.",
      "C": "P-hacking is a term for using highly complex statistical models. It's ethical as long as the model is documented.",
      "D": "P-hacking refers to publishing only significant results. It's ethical if the study was well-designed."
    },
    "answer": "B",
    "explanation": "P-hacking involves practices like running many analyses and only reporting those with significant p-values, stopping data collection when a p-value becomes significant, or including/excluding covariates until significance is achieved. This inflates Type I error rates and produces misleading findings.",
    "topic": "Ethics in Statistics",
    "difficulty": "Hard"
  },
  {
    "id": 214,
    "question": "What is the difference between statistical significance and practical significance?",
    "options": {
      "A": "They are the same; if something is statistically significant, it is always practically important.",
      "B": "Statistical significance means an effect is unlikely by chance; practical significance means the effect is large or meaningful in a real-world context.",
      "C": "Statistical significance is about the sample; practical significance is about the population.",
      "D": "Practical significance depends on the p-value; statistical significance depends on the effect size."
    },
    "answer": "B",
    "explanation": "A statistically significant result simply means that the observed difference or relationship is unlikely to be due to chance. However, this difference may be very small and not have any real-world importance or practical utility, which is what practical significance refers to.",
    "topic": "Hypothesis Testing Concepts",
    "difficulty": "Hard"
  },
  {
    "id": 215,
    "question": "Explain the 'bootstrapping' method for constructing confidence intervals and its advantages over traditional methods when assumptions are violated.",
    "options": {
      "A": "Bootstrapping creates intervals by assuming normality. Advantage: faster computation.",
      "B": "Bootstrapping involves drawing multiple resamples with replacement from the original sample to create an empirical sampling distribution. Advantage: robust to violations of normality and other distributional assumptions.",
      "C": "Bootstrapping is a technique for correcting for missing data. Advantage: no need for large sample sizes.",
      "D": "Bootstrapping is a method for generating synthetic data. Advantage: perfect intervals every time."
    },
    "answer": "B",
    "explanation": "Bootstrapping is a non-parametric resampling method. By repeatedly drawing samples with replacement from the observed data, it creates an empirical distribution of a statistic, from which confidence intervals can be derived without strong distributional assumptions, making it robust for complex or non-normal data.",
    "topic": "Resampling Methods / Estimation",
    "difficulty": "Hard"
  },
  {
    "id": 216,
    "question": "What is 'heteroscedasticity' in linear regression, what are its implications, and how can it be addressed?",
    "options": {
      "A": "It means constant variance of errors. Implications: no issues. Address: none needed.",
      "B": "It means the variance of the errors is not constant. Implications: OLS estimators are still unbiased but inefficient, standard errors are biased, invalid p-values/confidence intervals. Address: weighted least squares, robust standard errors, data transformation.",
      "C": "It means errors are correlated. Implications: multicollinearity. Address: remove variables.",
      "D": "It means data is not normally distributed. Implications: non-linear relationships. Address: use non-linear models."
    },
    "answer": "B",
    "explanation": "Heteroscedasticity violates the assumption of constant error variance. While OLS coefficients remain unbiased, their standard errors are inaccurate, leading to incorrect inferences. It can be addressed by transformations (e.g., log), using robust standard errors, or weighted least squares.",
    "topic": "Regression Assumptions / Heteroscedasticity",
    "difficulty": "Hard"
  },
  {
    "id": 217,
    "question": "Explain the concept of 'bias' and 'variance' in the context of the bias-variance trade-off in statistical modeling.",
    "options": {
      "A": "Bias is model complexity, variance is model simplicity.",
      "B": "Bias is the error from approximating a real-world problem with a simplified model; variance is the amount by which the model's prediction would change if trained on a different dataset.",
      "C": "Bias is due to outliers, variance is due to missing data.",
      "D": "Bias is random error, variance is systematic error."
    },
    "answer": "B",
    "explanation": "Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simpler model. Variance refers to the amount by which the model's prediction function would change if we refit it with a different training dataset. High bias implies underfitting; high variance implies overfitting.",
    "topic": "Model Selection / Bias-Variance Trade-off",
    "difficulty": "Hard"
  },
  {
    "id": 218,
    "question": "You are designing a study to detect a small but clinically meaningful difference between two treatments. What statistical considerations are crucial to ensure your study has a high chance of detecting this difference?",
    "options": {
      "A": "Focus only on statistical significance.",
      "B": "Prioritize a very small sample size.",
      "C": "Conduct a power analysis to determine the adequate sample size, considering the effect size, significance level, and desired power.",
      "D": "Ignore the variability of the data."
    },
    "answer": "C",
    "explanation": "A power analysis is essential to determine the minimum sample size required to detect a specified effect size with a desired level of statistical power (and chosen significance level), thus ensuring the study is adequately powered.",
    "topic": "Sample Size / Power Analysis",
    "difficulty": "Hard"
  },
  {
    "id": 219,
    "question": "What is the philosophical basis of the Central Limit Theorem, and why is it so important in inferential statistics?",
    "options": {
      "A": "It states that all populations are normal. Importance: simplifies data collection.",
      "B": "It states that the sample mean always equals the population mean. Importance: eliminates need for large samples.",
      "C": "It states that the distribution of sample means approaches a normal distribution as sample size increases, regardless of population distribution. Importance: enables use of normal distribution (and Z/t tests) for inference about population means, even when population is non-normal.",
      "D": "It's a theorem for discrete data. Importance: used for categorical tests only."
    },
    "answer": "C",
    "explanation": "The Central Limit Theorem (CLT) is foundational because it allows statisticians to make inferences about population means using normal distribution theory, even if the original population data is not normally distributed, as long as the sample size is sufficiently large. This legitimizes many parametric tests.",
    "topic": "Central Limit Theorem",
    "difficulty": "Hard"
  },
  {
    "id": 220,
    "question": "Discuss the difference between 'sampling error' and 'non-sampling error' in survey research, providing examples of each.",
    "options": {
      "A": "Sampling error is always avoidable; non-sampling error is not. Example: measurement errors (sampling), non-response (non-sampling).",
      "B": "Sampling error arises from taking a sample instead of a whole population (e.g., random variation); non-sampling error arises from sources other than sampling (e.g., measurement bias, non-response bias, faulty questionnaire design).",
      "C": "Sampling error is a type of bias; non-sampling error is random. Example: systematic errors (sampling), random errors (non-sampling).",
      "D": "They are both types of random errors that affect data accuracy."
    },
    "answer": "B",
    "explanation": "Sampling error is inherent in any sampling process, representing the difference between a sample statistic and its corresponding population parameter due to random chance in selection. Non-sampling error arises from various sources during data collection and analysis, independent of the sampling process (e.g., poorly worded questions, data entry mistakes, interviewer bias, non-response).",
    "topic": "Sampling Errors / Survey Methodology",
    "difficulty": "Hard"
  },
  {
    "id": 221,
    "question": "When is it appropriate to use a non-parametric test instead of its parametric counterpart, and what are the general advantages and disadvantages of non-parametric tests?",
    "options": {
      "A": "Use when data is normal. Advantages: more powerful. Disadvantages: complex.",
      "B": "Use when parametric assumptions (e.g., normality, homogeneity of variance) are violated, or with ordinal/nominal data. Advantages: fewer assumptions, robustness to outliers. Disadvantages: less powerful than parametric tests when assumptions met, generally provide less information.",
      "C": "Use for very large sample sizes. Advantages: always more accurate. Disadvantages: difficult to interpret.",
      "D": "Use when population parameters are known. Advantages: no disadvantages."
    },
    "answer": "B",
    "explanation": "Non-parametric tests are used when data do not meet the assumptions of parametric tests, especially normality or interval/ratio scale data. Advantages include applicability to various data types and robustness to outliers. Disadvantages include potentially lower statistical power (if parametric assumptions hold) and less detailed information (e.g., about means).",
    "topic": "Nonparametric Tests",
    "difficulty": "Hard"
  },
  {
    "id": 222,
    "question": "Discuss the ethical considerations in data collection and analysis, particularly concerning privacy, informed consent, and data integrity.",
    "options": {
      "A": "Ethics only apply to medical studies; data integrity is not a major concern.",
      "B": "Privacy: protecting personal information. Informed consent: ensuring participants understand and agree to the study. Data integrity: ensuring accuracy and honesty in collection, storage, and analysis. All are crucial for trustworthy and responsible research.",
      "C": "Informed consent is only required if data is publicly shared. Privacy is not a major issue with anonymized data.",
      "D": "Ethical guidelines hinder scientific progress; they should be minimized."
    },
    "answer": "B",
    "explanation": "Ethical considerations are paramount in all research. Privacy involves safeguarding personal data. Informed consent ensures participants freely and knowingly agree to participate. Data integrity demands honesty and accuracy at all stages to prevent fraud and maintain public trust in findings.",
    "topic": "Ethics in Statistics",
    "difficulty": "Hard"
  },
  {
    "id": 223,
    "question": "How does the concept of 'robustness' apply to statistical methods? Provide an example of a robust statistic and explain why it is considered robust.",
    "options": {
      "A": "Robustness means a method is difficult to calculate. Example: mean.",
      "B": "Robustness means a method is insensitive to small deviations from assumptions or outliers. Example: Median is robust because it's less affected by extreme values than the mean.",
      "C": "Robustness means a method produces exact p-values. Example: t-test.",
      "D": "Robustness means a method requires a very large sample size. Example: z-test."
    },
    "answer": "B",
    "explanation": "A robust statistical method is one that performs well even when the underlying assumptions of the model are not perfectly met, or in the presence of outliers. The median is a robust measure of central tendency because it is not significantly influenced by extreme values, unlike the mean.",
    "topic": "General Statistical Concepts",
    "difficulty": "Hard"
  },
  {
    "id": 224,
    "question": "Explain the concept of 'confounding' in observational studies and how researchers attempt to address it in their analyses.",
    "options": {
      "A": "Confounding is a type of random error. Addressed by increasing sample size.",
      "B": "Confounding occurs when an extraneous variable influences both the independent and dependent variables, creating a spurious association. Addressed by: randomization (if possible), stratification, regression adjustment, propensity score matching.",
      "C": "Confounding is when the dependent variable causes the independent variable. Addressed by changing the study design.",
      "D": "Confounding means the data is not normally distributed. Addressed by using non-parametric tests."
    },
    "answer": "B",
    "explanation": "Confounding variables make it difficult to determine the true relationship between an exposure and an outcome. In observational studies where randomization is not possible, researchers use statistical methods (like regression, stratification, or matching) to control for known confounders.",
    "topic": "Research Design / Confounding",
    "difficulty": "Hard"
  },
  {
    "id": 225,
    "question": "What is the 'likelihood ratio test' and when is it typically used?",
    "options": {
      "A": "A test for normality of residuals. Used after regression.",
      "B": "A test for comparing two means. Used instead of a t-test.",
      "C": "A statistical test used to compare the fit of two nested statistical models (one model is a special case of the other). It's based on the ratio of their maximized likelihoods.",
      "D": "A test for independence of categorical variables. Used instead of chi-square."
    },
    "answer": "C",
    "explanation": "The likelihood ratio test is a powerful hypothesis test used to compare the goodness of fit of two statistical models, where one model (the null model) is a restricted version of the other (the alternative model). It's frequently used in generalized linear models and survival analysis.",
    "topic": "Hypothesis Testing / Model Comparison",
    "difficulty": "Hard"
  },
  {
    "id": 226,
    "question": "Describe the intuition behind 'regularization' techniques (e.g., Ridge, Lasso) in regression, and explain their role in preventing overfitting.",
    "options": {
      "A": "Regularization adds more features to the model to improve fit.",
      "B": "Regularization simplifies the model by removing irrelevant features. Prevents overfitting by reducing variance.",
      "C": "Regularization adds a penalty term to the loss function to shrink regression coefficients, reducing model complexity and preventing overfitting by limiting variance.",
      "D": "Regularization transforms the dependent variable to ensure linearity."
    },
    "answer": "C",
    "explanation": "Regularization techniques (like L1/Lasso and L2/Ridge) add a penalty term to the ordinary least squares (OLS) loss function. This penalty discourages large coefficients, effectively shrinking them towards zero, which simplifies the model and reduces its variance, thereby preventing overfitting to the training data.",
    "topic": "Regression Analysis / Regularization",
    "difficulty": "Hard"
  },
  {
    "id": 227,
    "question": "When might the mean of a dataset be a misleading measure of central tendency? Provide an alternative and explain why it's better in that scenario.",
    "options": {
      "A": "Always misleading; median is always better.",
      "B": "When the data is perfectly symmetrical; median is also good.",
      "C": "When the data contains outliers or is highly skewed. The median is a better alternative because it is not affected by extreme values.",
      "D": "When the sample size is small; mode is better."
    },
    "answer": "C",
    "explanation": "The mean is sensitive to outliers and skewness, as extreme values can pull it significantly. The median, as the middle value, is resistant to such influence and provides a more representative center in skewed distributions or those with outliers.",
    "topic": "Measures of Central Tendency",
    "difficulty": "Hard"
  },
  {
    "id": 228,
    "question": "What is the 'Law of Large Numbers' in probability theory, and what is its practical implication in statistics?",
    "options": {
      "A": "States that small samples are representative. Implication: no need for large samples.",
      "B": "States that as the number of trials increases, the observed frequency of an event approaches its theoretical probability. Implication: validates using large samples to estimate population parameters accurately.",
      "C": "States that large numbers are more likely to occur. Implication: focus on large values.",
      "D": "States that the mean of a sample is always equal to the population mean."
    },
    "answer": "B",
    "explanation": "The Law of Large Numbers states that as the number of independent, identically distributed random variables increases, their sample mean converges to the true expected value of the population. Practically, this means that with a large enough sample, sample statistics provide reliable estimates of population parameters.",
    "topic": "Probability Theory",
    "difficulty": "Hard"
  },
  {
    "id": 229,
    "question": "Explain the concept of 'Simpson's Paradox' and provide a hypothetical example.",
    "options": {
      "A": "A paradox where small samples yield more accurate results than large ones. Example: a small drug trial is better than a large one.",
      "B": "A phenomenon where a trend appears in several groups of data but disappears or reverses when the groups are combined. Hypothetical example: Drug A seems better than Drug B in both male and female groups, but Drug B seems better overall when combined.",
      "C": "A paradox where correlation implies causation. Example: Ice cream sales and drowning deaths.",
      "D": "A statistical error where a p-value is misinterpreted. Example: assuming a large p-value means H0 is true."
    },
    "answer": "B",
    "explanation": "Simpson's Paradox occurs when an association between two variables reverses direction or disappears when a third, confounding variable is considered or when data from different groups are inappropriately aggregated. Example: A drug may appear more effective in both men and women individually, but less effective overall when data are combined, due to differing baseline success rates and group sizes.",
    "topic": "Data Analysis Pitfalls / Confounding",
    "difficulty": "Hard"
  },
  {
    "id": 230,
    "question": "What is the purpose of 'principal component analysis (PCA)' in statistics?",
    "options": {
      "A": "To predict a dependent variable based on independent variables.",
      "B": "To reduce the dimensionality of a dataset while retaining as much variance as possible, by transforming correlated variables into a set of uncorrelated principal components.",
      "C": "To identify outliers in a dataset.",
      "D": "To perform hypothesis tests on multiple variables simultaneously."
    },
    "answer": "B",
    "explanation": "PCA is a dimensionality reduction technique used to simplify complex datasets. It identifies new, uncorrelated variables (principal components) that capture most of the variance in the original data, which can be useful for visualization, noise reduction, and addressing multicollinearity.",
    "topic": "Dimensionality Reduction",
    "difficulty": "Hard"
  },
  {
    "id": 231,
    "question": "Describe the difference between 'sampling with replacement' and 'sampling without replacement' and their implications for probability calculations.",
    "options": {
      "A": "With replacement: items are not returned; Without replacement: items are returned. Implications: none for probability.",
      "B": "With replacement: items are returned, events are independent, probability remains constant. Without replacement: items are not returned, events are dependent, probability changes for subsequent draws.",
      "C": "With replacement: used for small populations; Without replacement: used for large populations.",
      "D": "They are only relevant for combinatorics, not probability."
    },
    "answer": "B",
    "explanation": "Sampling with replacement means an item can be selected multiple times, maintaining constant probabilities for each draw (independent events). Sampling without replacement means an item cannot be selected again, changing the probabilities for subsequent draws (dependent events).",
    "topic": "Sampling Methods / Probability",
    "difficulty": "Hard"
  },
  {
    "id": 232,
    "question": "What is 'survival analysis' and in what types of scenarios is it applied?",
    "options": {
      "A": "A type of regression for predicting continuous outcomes. Applied to economic forecasting.",
      "B": "A statistical method for analyzing the time until an event occurs. Applied in medical research (patient survival), engineering (device failure), social sciences (unemployment duration).",
      "C": "A method for measuring the likelihood of an event not happening. Applied to quality control.",
      "D": "A technique for analyzing data without any missing values. Applied to clean datasets."
    },
    "answer": "B",
    "explanation": "Survival analysis (also known as 'time-to-event analysis') is a branch of statistics for analyzing the expected duration of time until one or more events happen, such as death in biological organisms or failure in mechanical systems. It accounts for 'censoring', where the event has not yet occurred or is unobserved.",
    "topic": "Advanced Statistical Methods",
    "difficulty": "Hard"
  },
  {
    "id": 233,
    "question": "Explain the concept of 'causation' versus 'correlation' using an example. What are the key criteria for establishing causation?",
    "options": {
      "A": "They are synonymous. If two things are correlated, one causes the other. Example: Ice cream sales cause drownings.",
      "B": "Correlation indicates an association; causation means one variable directly influences another. Example: High ice cream sales correlate with more drownings, but a hidden factor (hot weather) causes both. Criteria for causation: temporal precedence, consistent association, elimination of alternative explanations.",
      "C": "Causation implies correlation, but correlation never implies causation. Example: exercise causing weight loss.",
      "D": "Causation is only established in observational studies; correlation in experiments."
    },
    "answer": "B",
    "explanation": "Correlation (association) does not imply causation. To establish causation, one needs: 1) Temporal precedence (cause precedes effect), 2) Consistent association (correlation), and 3) Elimination of alternative explanations (controlling for confounders), often best achieved through randomized controlled trials.",
    "topic": "Causation vs. Correlation",
    "difficulty": "Hard"
  },
  {
    "id": 234,
    "question": "What is the difference between a 'one-sample t-test' and a 'paired-samples t-test'? Provide an example where each would be appropriate.",
    "options": {
      "A": "One-sample tests a single group against another single group. Paired-samples tests more than two groups.",
      "B": "One-sample compares a sample mean to a known population mean (or hypothesized value). Paired-samples compares means from the same subjects under two different conditions or from matched pairs. Example: Is average height of students 170cm? (one-sample) vs. Is there a difference in blood pressure before and after medication? (paired-samples).",
      "C": "One-sample for categorical data. Paired-samples for continuous data.",
      "D": "They are interchangeable depending on data size."
    },
    "answer": "B",
    "explanation": "A one-sample t-test compares the mean of a single sample to a known or hypothesized population mean. A paired-samples t-test (also called a dependent samples t-test) compares the means of two related groups (e.g., before-and-after measurements on the same individuals, or matched pairs).",
    "topic": "T-Tests",
    "difficulty": "Hard"
  },
  {
    "id": 235,
    "question": "Explain the underlying principle of the 'F-statistic' in ANOVA. What does a large F-statistic typically indicate?",
    "options": {
      "A": "Measures the overall mean. Large F means overall mean is high.",
      "B": "Compares within-group variance to between-group variance. A large F-statistic indicates that the variation between group means is significantly larger than the variation within groups, suggesting that at least one group mean is different.",
      "C": "Compares sample size to population size. Large F means small sample size.",
      "D": "Measures the difference between two specific group means. Large F means a large difference."
    },
    "answer": "B",
    "explanation": "The F-statistic is the ratio of 'mean square between groups' to 'mean square within groups'. A large F-statistic suggests that the variability between the group means is substantial compared to the variability within each group, providing evidence against the null hypothesis that all population means are equal.",
    "topic": "ANOVA",
    "difficulty": "Hard"
  },
  {
    "id": 236,
    "question": "What is the 'Maximum Likelihood Estimation (MLE)' principle? What are its properties and why is it widely used?",
    "options": {
      "A": "Finds parameters that minimize the probability of the data. Properties: biased. Used for simplicity.",
      "B": "Finds parameter values that maximize the likelihood of observing the given data. Properties: asymptotically unbiased, consistent, efficient, and asymptotically normal. Widely used due to these desirable asymptotic properties.",
      "C": "Finds parameters using a least squares approach. Properties: sensitive to outliers. Used for interpretability.",
      "D": "Finds parameters by averaging all possible values. Properties: non-parametric. Used for robustness."
    },
    "answer": "B",
    "explanation": "MLE is a method of estimating parameters of a probability distribution by maximizing a likelihood function, such that under the assumed statistical model, the observed data is most probable. Its desirable asymptotic properties (consistency, asymptotic normality, efficiency) make it a cornerstone of statistical inference.",
    "topic": "Statistical Inference / MLE",
    "difficulty": "Hard"
  },
  {
    "id": 237,
    "question": "Discuss the potential limitations and misinterpretations of p-values in hypothesis testing.",
    "options": {
      "A": "P-values tell you the probability that the null hypothesis is true. Limitation: always inaccurate.",
      "B": "P-values tell you about the magnitude of an effect. Limitation: ignores sample size.",
      "C": "Limitations: A p-value is NOT the probability that H0 is true; it doesn't indicate effect size or practical significance; it's sensitive to sample size. Misinterpretations lead to overemphasis on arbitrary cutoffs and ignoring context.",
      "D": "P-values are only useful for Type I errors. Misinterpretation: cannot be used for Type II errors."
    },
    "answer": "C",
    "explanation": "P-values are often misinterpreted. They do not represent the probability of the null hypothesis being true or the probability of the alternative hypothesis being false. A small p-value merely indicates how likely the observed data (or more extreme) would be if the null hypothesis were true. It doesn't convey effect size or practical importance, and it's heavily influenced by sample size.",
    "topic": "Hypothesis Testing Concepts",
    "difficulty": "Hard"
  },
  {
    "id": 238,
    "question": "What is a 'Generalized Linear Model (GLM)'? Provide an example of when it would be more appropriate than a standard linear regression model.",
    "options": {
      "A": "A GLM is a linear regression with multiple independent variables. Example: always use it.",
      "B": "A GLM extends linear regression to models where the dependent variable is not normally distributed or where the relationship between the mean of the dependent variable and the independent variables is not linear. Example: Logistic regression for binary outcomes (e.g., success/failure), Poisson regression for count data (e.g., number of events).",
      "C": "A GLM is a non-parametric model. Example: when data is highly skewed.",
      "D": "A GLM is a model with interactions. Example: always use it if interaction terms are present."
    },
    "answer": "B",
    "explanation": "GLMs generalize linear regression by allowing for response variables that have error distribution models other than a normal distribution (e.g., binomial, Poisson) and by allowing a link function to relate the mean of the response to the linear predictor. This makes them suitable for modeling various types of non-normal data where standard linear regression assumptions are violated.",
    "topic": "Regression Analysis / GLM",
    "difficulty": "Hard"
  },
  {
    "id": 239,
    "question": "Explain the concept of 'confounding by indication' in observational studies and how researchers try to mitigate its impact.",
    "options": {
      "A": "Confounding by indication occurs when a treatment is indicated for certain conditions, leading to bias. Mitigation: randomization (if experimental), propensity score matching, instrumental variables, careful covariate adjustment in observational studies.",
      "B": "It's when researchers accidentally reveal the study's purpose. Mitigation: blinding.",
      "C": "It's a type of measurement error. Mitigation: better instruments.",
      "D": "It means the sample size is too small for a clear indication. Mitigation: increase sample size."
    },
    "answer": "A",
    "explanation": "Confounding by indication is a specific type of confounding bias prevalent in observational studies where the reason a patient receives a certain treatment (the 'indication') is also related to the outcome. Since you cannot randomize treatment in observational studies, researchers try to mitigate it using statistical methods like propensity score matching or instrumental variables, or by carefully adjusting for all known confounders in multivariable regression.",
    "topic": "Research Design / Bias",
    "difficulty": "Hard"
  },
  {
    "id": 240,
    "question": "What is the 'Bonferroni correction' for multiple comparisons, and what is its main drawback?",
    "options": {
      "A": "A method to increase Type I error. Drawback: too powerful.",
      "B": "A method to decrease Type II error. Drawback: too conservative.",
      "C": "A method to adjust p-values for multiple comparisons by dividing the original significance level (α) by the number of tests. Drawback: Can be overly conservative, increasing the risk of Type II errors.",
      "D": "A method for controlling Type II error by increasing sample size. Drawback: requires small samples."
    },
    "answer": "C",
    "explanation": "The Bonferroni correction is a simple method to control the family-wise error rate. However, its main drawback is that it can be too conservative, meaning it makes it harder to detect true effects and increases the chance of Type II errors, especially when many tests are performed.",
    "topic": "Multiple Comparisons",
    "difficulty": "Hard"
  },
  {
    "id": 241,
    "question": "What is 'Akaike Information Criterion (AIC)' and 'Bayesian Information Criterion (BIC)' used for in statistical modeling?",
    "options": {
      "A": "Measures of data normality. Used to check assumptions.",
      "B": "Measures of model complexity. Used to avoid overfitting.",
      "C": "Information criteria used for model selection, balancing model fit with complexity. Lower AIC/BIC generally indicates a better model.",
      "D": "Hypothesis testing statistics. Used to determine significance."
    },
    "answer": "C",
    "explanation": "AIC and BIC are used for model selection. They estimate the quality of statistical models relative to each other for a given set of data, by penalizing models with more parameters (complexity) to avoid overfitting. Models with lower AIC/BIC values are generally preferred.",
    "topic": "Model Selection",
    "difficulty": "Hard"
  },
  {
    "id": 242,
    "question": "Explain the concept of 'non-response bias' in survey sampling and how it can affect the validity of results. How can researchers attempt to minimize it?",
    "options": {
      "A": "Non-response bias is when people respond too quickly. Mitigation: slower surveys.",
      "B": "Non-response bias occurs when those who don't respond to a survey differ systematically from those who do, leading to an unrepresentative sample. Mitigation: follow-ups, incentives, weighting, careful questionnaire design.",
      "C": "Non-response bias is when respondents give incorrect answers. Mitigation: lie detectors.",
      "D": "It's a random error; mitigation is impossible."
    },
    "answer": "B",
    "explanation": "Non-response bias is a major threat to external validity. It arises when there are systematic differences between respondents and non-respondents, making the collected sample non-representative. Mitigation strategies include repeated contact, offering incentives, making surveys easy to complete, and using statistical adjustments like weighting based on known population demographics.",
    "topic": "Sampling Errors / Survey Methodology",
    "difficulty": "Hard"
  },
  {
    "id": 243,
    "question": "What is the 'bootstrap percentile method' for constructing a confidence interval, and what are its main advantages?",
    "options": {
      "A": "It assumes a normal distribution for the statistic. Advantage: simple calculations.",
      "B": "It involves repeatedly resampling with replacement, calculating the statistic of interest for each resample, and then using the percentiles of this empirical distribution as the confidence interval bounds. Advantages: non-parametric, robust to non-normal distributions, can be used for complex statistics.",
      "C": "It creates a narrow interval. Advantage: very precise.",
      "D": "It requires knowing the population standard deviation. Advantage: computationally fast."
    },
    "answer": "B",
    "explanation": "The bootstrap percentile method directly uses the quantiles (percentiles) of the bootstrap distribution of a statistic to form a confidence interval. Its primary advantage is its non-parametric nature, meaning it doesn't rely on assumptions about the underlying population distribution, making it broadly applicable.",
    "topic": "Resampling Methods / Estimation",
    "difficulty": "Hard"
  },
  {
    "id": 244,
    "question": "In the context of hypothesis testing, what is the 'sampling distribution of the test statistic,' and why is it crucial?",
    "options": {
      "A": "It's the distribution of the population. Crucial for descriptive statistics.",
      "B": "It's the distribution of sample data. Crucial for data visualization.",
      "C": "It's the probability distribution of a test statistic (e.g., t, F, Chi-square) if the null hypothesis were true. It's crucial because it allows us to calculate the p-value and determine whether to reject the null hypothesis.",
      "D": "It's the distribution of all possible p-values. Crucial for interpretation."
    },
    "answer": "C",
    "explanation": "The sampling distribution of the test statistic under the null hypothesis allows us to determine how likely our observed test statistic is if the null hypothesis is true. This probability is the p-value, which is central to making a decision about rejecting or failing to reject the null hypothesis.",
    "topic": "Hypothesis Testing Concepts",
    "difficulty": "Hard"
  },
  {
    "id": 245,
    "question": "What is 'instrumental variables' analysis, and when is it employed in causal inference?",
    "options": {
      "A": "A method to select the best instruments for data collection. Used in experimental design.",
      "B": "A technique used to estimate causal effects in observational studies when there is unmeasured confounding, by using a variable (instrument) that affects the exposure but only influences the outcome through the exposure.",
      "C": "A method to analyze data with measurement errors. Used when data quality is poor.",
      "D": "A type of machine learning algorithm for classification. Used in predictive modeling."
    },
    "answer": "B",
    "explanation": "Instrumental variables (IV) analysis is a sophisticated econometric technique used to infer causal relationships when direct randomization is not feasible and unmeasured confounders might be present. An instrumental variable must affect the treatment/exposure but only influence the outcome through its effect on the treatment.",
    "topic": "Causal Inference / Advanced Methods",
    "difficulty": "Hard"
  },
  {
    "id": 246,
    "question": "Explain 'Bayes' Theorem' and its application in updating probabilities based on new evidence.",
    "options": {
      "A": "A theorem for calculating descriptive statistics. Application: finding the mean.",
      "B": "P(A|B) = [P(B|A) * P(A)] / P(B). It updates the probability of a hypothesis (P(A)) given new evidence (B) to obtain a posterior probability (P(A|B)). Application: medical diagnosis, spam filtering.",
      "C": "A theorem for proving null hypotheses. Application: always reject the null.",
      "D": "A theorem for finding the correlation between two variables. Application: linear regression."
    },
    "answer": "B",
    "explanation": "Bayes' Theorem provides a way to update the probability of a hypothesis given new evidence. It combines a prior probability of the hypothesis, the likelihood of observing the evidence given the hypothesis, and the marginal probability of the evidence, to yield a posterior probability. It's fundamental to Bayesian inference.",
    "topic": "Probability Theory / Bayesian Inference",
    "difficulty": "Hard"
  },
  {
    "id": 247,
    "question": "What is the 'Granger causality test,' and what does it actually test?",
    "options": {
      "A": "It tests if X causes Y in a direct physical sense.",
      "B": "It tests if X can predict Y, not necessarily implying a direct causal link. It tests if past values of one time series variable (X) help predict future values of another time series variable (Y), beyond what can be predicted by past values of Y alone.",
      "C": "It tests for correlation between two variables. Always implies causation.",
      "D": "It's a test for normality of time series data."
    },
    "answer": "B",
    "explanation": "Granger causality is a statistical hypothesis test for determining whether one time series is useful in forecasting another. If past values of X significantly improve the prediction of Y beyond what past values of Y alone can do, then X is said to Granger-cause Y. It's about predictability, not necessarily true causation.",
    "topic": "Time Series Analysis / Causality",
    "difficulty": "Hard"
  },
  {
    "id": 248,
    "question": "Discuss the 'curse of dimensionality' in statistical modeling and machine learning. What are its implications, and how can it be addressed?",
    "options": {
      "A": "It's when you have too few variables. Implications: underfitting. Address: add more features.",
      "B": "It refers to the problems encountered when analyzing and organizing data in high-dimensional spaces (many features). Implications: data sparsity, increased computational cost, overfitting, difficulty in visualization. Address: dimensionality reduction techniques (PCA, feature selection), regularization, larger datasets.",
      "C": "It's when data has only two dimensions. Implications: simplified models. Address: none needed.",
      "D": "It implies that high-dimensional data is always better. Implications: improved model performance."
    },
    "answer": "B",
    "explanation": "The curse of dimensionality describes how the properties of data change in high-dimensional spaces, often making them counter-intuitive and problematic for analysis and modeling. As the number of features increases, the amount of data needed to ensure a good representation of feature space grows exponentially, leading to sparsity and increased risk of overfitting. Solutions involve feature selection, dimensionality reduction techniques like PCA, or regularization.",
    "topic": "Dimensionality Reduction / Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 249,
    "question": "What is 'survival bias' (or 'survivorship bias') in data collection, and how can it lead to misleading conclusions?",
    "options": {
      "A": "It's a bias towards living organisms. Leads to overestimation of survival rates.",
      "B": "It's a form of selection bias where only 'surviving' (successful or present) observations are considered, leading to incomplete data and potentially inaccurate conclusions about the original population. Example: only studying successful companies to learn success factors, ignoring failed ones.",
      "C": "It's a bias where researchers only publish positive results. Leads to publication bias.",
      "D": "It refers to the bias introduced when participants drop out of a study. Leads to attrition bias."
    },
    "answer": "B",
    "explanation": "Survivorship bias occurs when analysis is focused only on surviving entities, ignoring those that failed or were filtered out. This can lead to overly optimistic conclusions or misidentification of contributing factors because the 'failures' (which might hold crucial information) are excluded from the dataset.",
    "topic": "Bias / Data Collection",
    "difficulty": "Hard"
  },
  {
    "id": 250,
    "question": "Explain the concept of 'ecological fallacy' in statistical interpretation.",
    "options": {
      "A": "Assuming individuals within a group will behave like the group average. Example: concluding a person is wealthy because they live in a wealthy neighborhood.",
      "B": "Assuming a correlation at the individual level applies to the group level.",
      "C": "Mistaking correlation for causation. Example: A causes B.",
      "D": "Ignoring the sample size when interpreting results."
    },
    "answer": "A",
    "explanation": "The ecological fallacy is a logical error that occurs when one makes an inference about an individual based on data that applies to a group. For example, if a region has a high crime rate, it doesn't mean every individual in that region is a criminal.",
    "topic": "Data Interpretation Pitfalls",
    "difficulty": "Hard"
  },
  {
    "id": 251,
    "question": "What is the 'Wald test' and how does it relate to Maximum Likelihood Estimation?",
    "options": {
      "A": "A test for normality of residuals. Used after regression.",
      "B": "A test for comparing two means, similar to a t-test.",
      "C": "A statistical test used to test hypotheses about parameters of a model estimated by maximum likelihood. It assesses if parameter estimates are significantly different from hypothesized values by comparing the estimated parameter to its standard error.",
      "D": "A non-parametric test for categorical data."
    },
    "answer": "C",
    "explanation": "The Wald test is a commonly used statistical test for testing hypotheses about the parameters of a statistical model, particularly those estimated using Maximum Likelihood Estimation. It works by assessing whether the estimated parameter is sufficiently far from its hypothesized value, relative to its standard error.",
    "topic": "Hypothesis Testing / MLE",
    "difficulty": "Hard"
  },
  {
    "id": 252,
    "question": "Describe the 'Jeffreys prior' in Bayesian statistics and its role in handling 'uninformative priors'.",
    "options": {
      "A": "A prior distribution based on subjective beliefs. Used to incorporate strong prior knowledge.",
      "B": "A prior distribution that is always uniform. Used for simplicity.",
      "C": "A non-informative prior distribution that is proportional to the square root of the Fisher information. It's designed to be 'uninformative' in a way that respects the geometry of the parameter space, often leading to more objective Bayesian inference, especially for reparameterizations.",
      "D": "A prior distribution that always leads to a normal posterior."
    },
    "answer": "C",
    "explanation": "The Jeffreys prior is a specific type of non-informative prior in Bayesian statistics. It's 'uninformative' in the sense that it aims to have minimal impact on the posterior distribution and is invariant to reparameterization, meaning that if you transform your parameters, the prior for the transformed parameters correctly maps from the prior for the original parameters.",
    "topic": "Bayesian Statistics",
    "difficulty": "Hard"
  },
  {
    "id": 253,
    "question": "What are 'robust standard errors' in regression analysis, and when are they particularly useful?",
    "options": {
      "A": "Standard errors that are always larger. Useful for increasing p-values.",
      "B": "Standard errors that are resistant to outliers. Useful when data has many extreme values.",
      "C": "Standard errors that are adjusted to provide valid inferences even when the assumption of homoscedasticity is violated. Useful when heteroscedasticity is present.",
      "D": "Standard errors calculated using a bootstrap method. Useful for small samples."
    },
    "answer": "C",
    "explanation": "Robust standard errors (also known as Huber-White standard errors or sandwich estimators) are used in regression to provide valid inference (correct p-values and confidence intervals) even when the assumption of homoscedasticity is violated (i.e., when heteroscedasticity is present). They correct for the misestimation of standard errors by Ordinary Least Squares (OLS) under heteroscedasticity.",
    "topic": "Regression Analysis / Robustness",
    "difficulty": "Hard"
  },
  {
    "id": 254,
    "question": "Explain the concept of 'censoring' in survival analysis and why it's a critical consideration.",
    "options": {
      "A": "Censoring is when data is intentionally hidden. Critical for ethics.",
      "B": "Censoring is when the exact time of an event is not observed for some subjects (e.g., they drop out or the study ends before the event occurs). Critical because ignoring censored data leads to biased estimates of survival probabilities.",
      "C": "Censoring is when the data is not normally distributed. Critical for choosing tests.",
      "D": "Censoring is a form of data transformation. Critical for data cleaning."
    },
    "answer": "B",
    "explanation": "Censoring is a common feature in survival analysis where the time to an event is not fully observed for all individuals. For instance, a patient might leave the study, or the study might end before they experience the event. Properly handling censored data is critical to avoid underestimating the true survival times or event rates.",
    "topic": "Survival Analysis",
    "difficulty": "Hard"
  },
  {
    "id": 255,
    "question": "What is the difference between a 'fixed effect' and a 'random effect' in statistical modeling (e.g., in mixed models)? Provide an example for each.",
    "options": {
      "A": "Fixed effects are for categorical variables, random effects for continuous.",
      "B": "Fixed effects are parameters whose values are assumed to be fixed and apply to all observations; random effects are assumed to be drawn from a larger population and represent variability across different groups or individuals. Example: Treatment A vs B (fixed) vs. variability across individual patients (random).",
      "C": "Fixed effects are for small samples, random effects for large samples.",
      "D": "Fixed effects are always more important than random effects."
    },
    "answer": "B",
    "explanation": "Fixed effects are typically assumed to have a constant effect on the mean outcome for all observations and are directly estimated (e.g., the effect of a specific drug dose). Random effects, on the other hand, account for unobserved heterogeneity or grouping in the data, where the effects vary randomly across different units (e.g., patient-specific effects in a multi-center trial).",
    "topic": "Mixed Models / Advanced Regression",
    "difficulty": "Hard"
  },
  {
    "id": 256,
    "question": "Explain the concept of 'bootstrapping' for hypothesis testing, specifically for comparing two means without assuming normality.",
    "options": {
      "A": "Bootstrap hypothesis testing involves assuming normality and then using the t-distribution.",
      "B": "It involves repeatedly resampling from the combined (pooled) data under the null hypothesis (e.g., assuming no difference in means), calculating the test statistic for each resample, and then determining the p-value based on the proportion of resamples that yield a test statistic as extreme as or more extreme than the observed one. It doesn't require normality assumptions.",
      "C": "It calculates the p-value directly from the original data.",
      "D": "It's only for categorical data, not means."
    },
    "answer": "B",
    "explanation": "Bootstrap hypothesis testing provides a non-parametric approach to hypothesis testing. For comparing two means, it typically involves combining data from both groups, repeatedly resampling from this combined pool to create 'null' samples, calculating the test statistic (e.g., difference in means) for each, and then seeing where the original observed test statistic falls within this distribution.",
    "topic": "Resampling Methods / Hypothesis Testing",
    "difficulty": "Hard"
  },
  {
    "id": 257,
    "question": "What is 'instrumental bias' in observational studies, and how does instrumental variables (IV) analysis aim to address it?",
    "options": {
      "A": "Bias caused by poorly calibrated instruments. IV addresses it by using better instruments.",
      "B": "A bias that arises when an instrument itself has a direct effect on the outcome that is not mediated by the exposure. IV analysis aims to address confounding by using a valid instrument that only affects the outcome through the exposure, but it's very difficult to find such a valid instrument.",
      "C": "Bias from using too many instruments. IV addresses it by reducing the number of instruments.",
      "D": "Bias from not using instruments at all. IV addresses it by introducing instruments."
    },
    "answer": "B",
    "explanation": "Instrumental bias occurs when the chosen instrumental variable does not meet the strict assumptions of an instrument (e.g., it directly affects the outcome, or it's correlated with unmeasured confounders). IV analysis aims to remove confounding when direct randomization isn't possible, but its validity hinges critically on the strength and exogeneity of the instrument, which are often hard to verify in practice.",
    "topic": "Causal Inference / Bias",
    "difficulty": "Hard"
  },
  {
    "id": 258,
    "question": "Explain the concept of 'shrinkage' in the context of statistical modeling (e.g., in regularization or mixed models).",
    "options": {
      "A": "Shrinkage refers to making a model smaller by reducing sample size.",
      "B": "Shrinkage refers to pulling parameter estimates (e.g., regression coefficients or random effects) towards a central value (like zero or a group mean), typically to reduce variance and improve out-of-sample prediction, especially in models with many parameters or limited data.",
      "C": "Shrinkage means transforming data to a smaller scale. Used for visualization.",
      "D": "Shrinkage is a method for removing outliers from a dataset."
    },
    "answer": "B",
    "explanation": "Shrinkage involves 'pulling back' parameter estimates. In regularization, it's done by adding a penalty to the loss function, forcing coefficients towards zero (e.g., Lasso) or reducing their magnitude (e.g., Ridge). In mixed models, random effects estimates are 'shrunk' towards the overall mean, especially for groups with few observations, to prevent overfitting to small group data.",
    "topic": "Model Selection / Advanced Regression",
    "difficulty": "Hard"
  },
  {
    "id": 259,
    "question": "What is 'Propensity Score Matching (PSM)' and what problem in causal inference is it designed to address?",
    "options": {
      "A": "A method to match similar outcomes. Addresses outcome variability.",
      "B": "A method used in observational studies to balance the distribution of observed confounding variables between treatment and control groups, by creating a single 'propensity score' for each subject representing the probability of receiving treatment. Addresses confounding bias in non-randomized studies.",
      "C": "A method to predict the likelihood of a person responding to a survey. Addresses non-response bias.",
      "D": "A statistical test for categorical data. Addresses issues of independence."
    },
    "answer": "B",
    "explanation": "Propensity Score Matching (PSM) is a quasi-experimental method used to reduce confounding in observational studies. It attempts to mimic randomization by creating comparable groups (treatment and control) based on their propensity score (the probability of receiving treatment given observed covariates), thereby balancing the distribution of these covariates between groups and allowing for more valid causal inferences.",
    "topic": "Causal Inference / Observational Studies",
    "difficulty": "Hard"
  },
  {
    "id": 260,
    "question": "Discuss the challenges and potential pitfalls of interpreting 'interaction effects' in statistical models.",
    "options": {
      "A": "Interaction effects are always easy to interpret. Pitfall: too simple.",
      "B": "Challenges: Interpretation depends on the specific type of interaction (e.g., additive, multiplicative), can be difficult to visualize in higher dimensions, simple interpretation of main effects becomes misleading. Pitfalls: misinterpreting main effects in the presence of strong interactions, overcomplicating models with non-significant interactions.",
      "C": "Interaction effects only occur in non-linear models. Pitfall: assuming linearity.",
      "D": "Interaction effects are only relevant for categorical variables. Pitfall: applying to continuous data."
    },
    "answer": "B",
    "explanation": "Interaction effects mean the effect of one independent variable on the dependent variable depends on the level of another independent variable. Interpreting them can be complex, especially with multiple interacting terms, as the main effects no longer tell the whole story. Visualization and careful consideration of the specific model type are crucial to avoid misinterpretation.",
    "topic": "Regression Analysis / Model Interpretation",
    "difficulty": "Hard"
  },
  {
    "id": 261,
    "question": "You are comparing two different diagnostic tests for a disease. One test has high sensitivity and low specificity, while the other has low sensitivity and high specificity. Explain what these terms mean and the trade-offs involved.",
    "options": {
      "A": "Sensitivity: proportion of true negatives correctly identified. Specificity: proportion of true positives correctly identified. Trade-off: high sensitivity means more false positives.",
      "B": "Sensitivity: proportion of true positives correctly identified (correctly identifies disease). Specificity: proportion of true negatives correctly identified (correctly identifies absence of disease). Trade-off: improving one often comes at the expense of the other, depending on the test's cutoff.",
      "C": "Sensitivity: test's reliability. Specificity: test's validity. Trade-off: none.",
      "D": "Sensitivity: number of positive results. Specificity: number of negative results. Trade-off: sample size."
    },
    "answer": "B",
    "explanation": "Sensitivity (True Positive Rate) is the ability of a test to correctly identify those with the disease. Specificity (True Negative Rate) is the ability of a test to correctly identify those without the disease. There's often a trade-off: a highly sensitive test might catch all cases but also have more false alarms (low specificity), while a highly specific test might miss some cases (low sensitivity) but have fewer false alarms.",
    "topic": "Diagnostic Statistics / Classification Metrics",
    "difficulty": "Hard"
  },
  {
    "id": 262,
    "question": "What is the 'Maximum a Posteriori (MAP)' estimation in Bayesian statistics, and how does it relate to Maximum Likelihood Estimation (MLE)?",
    "options": {
      "A": "MAP estimates the parameters that maximize the likelihood function, identical to MLE.",
      "B": "MAP estimates parameters that maximize the posterior distribution, combining the likelihood of the data with a prior distribution for the parameters. It is related to MLE in that if a uniform (uninformative) prior is used, MAP estimates become identical to MLE estimates.",
      "C": "MAP is a method for minimizing estimation errors, unrelated to MLE.",
      "D": "MAP estimates the probability of the data given the prior, while MLE estimates the probability of the prior given the data."
    },
    "answer": "B",
    "explanation": "MAP estimation is a method of estimating parameters that maximize the posterior probability distribution, which is proportional to the product of the likelihood function and the prior probability distribution. If the prior distribution is uniform (i.e., every parameter value is equally likely), then MAP estimation yields the same results as MLE.",
    "topic": "Bayesian Statistics / Estimation",
    "difficulty": "Hard"
  },
  {
    "id": 263,
    "question": "Explain the concept of 'confounding by common cause' and how directed acyclic graphs (DAGs) can be used to visualize and identify potential confounders.",
    "options": {
      "A": "Confounding by common cause means X causes Y and Z. DAGs show simple relationships.",
      "B": "Confounding by common cause occurs when a third variable (confounder) causes both the exposure and the outcome. DAGs are visual tools that represent causal relationships between variables; they help identify variables that need to be controlled for to estimate a non-confounded causal effect.",
      "C": "It's a type of measurement error. DAGs show measurement error flow.",
      "D": "It's when multiple independent variables cause the same dependent variable. DAGs show multicollinearity."
    },
    "answer": "B",
    "explanation": "Confounding by common cause is a key issue in causal inference, where an unmeasured or unaddressed variable is a common cause of both the 'treatment' and the 'outcome,' making it difficult to isolate the true effect of the treatment. DAGs graphically represent causal assumptions and can be used to identify adjustment sets (variables to control for) to block confounding paths and estimate causal effects.",
    "topic": "Causal Inference / DAGs",
    "difficulty": "Hard"
  },
  {
    "id": 264,
    "question": "What is the 'Delta method' in statistics, and when is it particularly useful?",
    "options": {
      "A": "A method for calculating medians. Useful for skewed data.",
      "B": "A method for approximating the sampling distribution of a function of one or more random variables, especially for estimating the standard error of a statistic that is a non-linear function of other estimated parameters. Useful when direct analytical solutions for standard errors are complex.",
      "C": "A method for performing non-linear regression. Useful for complex models.",
      "D": "A method for testing hypothesis about proportions. Useful for binary data."
    },
    "answer": "B",
    "explanation": "The Delta method is a mathematical tool used to approximate the variance and standard error of functions of random variables. It's particularly useful when you have estimated parameters (e.g., regression coefficients) and you want to find the standard error of a quantity that is a non-linear combination of these parameters (e.g., odds ratios, relative risks in generalized linear models).",
    "topic": "Statistical Inference / Advanced Methods",
    "difficulty": "Hard"
  },
  {
    "id": 265,
    "question": "Explain the philosophical difference between 'frequentist' and 'Bayesian' interpretations of probability.",
    "options": {
      "A": "Frequentist: probability as long-run frequency. Bayesian: probability as degree of belief. They are essentially the same.",
      "B": "Frequentist defines probability as the long-run relative frequency of an event over many trials. Bayesian defines probability as a degree of belief or subjective certainty about an event, which can be updated with new evidence.",
      "C": "Frequentist probability is for continuous events, Bayesian for discrete.",
      "D": "Frequentist involves prior distributions; Bayesian does not."
    },
    "answer": "B",
    "explanation": "Frequentist probability interprets probability as the limiting relative frequency of an event in a large number of trials. Bayesian probability, on the other hand, views probability as a subjective degree of belief or confidence in a proposition, which can be updated as new evidence becomes available.",
    "topic": "Probability Theory / Philosophies",
    "difficulty": "Hard"
  },
  {
    "id": 266,
    "question": "What is the 'Hawthorne effect' in research, and how can it introduce bias?",
    "options": {
      "A": "The effect of a new treatment. Bias: researcher bias.",
      "B": "A type of observer effect where subjects improve an aspect of their behavior in response to their awareness of being observed. Bias: results may not generalize to real-world settings where observation is absent.",
      "C": "The effect of historical events on study participants. Bias: historical bias.",
      "D": "The effect of environmental factors on the outcome. Bias: environmental confounding."
    },
    "answer": "B",
    "explanation": "The Hawthorne effect is a form of reactivity whereby subjects improve or modify an aspect of their behavior being experimentally measured in response to the fact that they are being observed. This can lead to biased results that overstate the effectiveness of an intervention or change, as the effect is due to observation rather than the intervention itself.",
    "topic": "Research Bias",
    "difficulty": "Hard"
  },
  {
    "id": 267,
    "question": "Discuss the challenges of handling 'missing at random (MAR)' data and 'missing not at random (MNAR)' data. Which is more problematic, and why?",
    "options": {
      "A": "MAR is more problematic because it's random. MNAR is easy.",
      "B": "MAR: missingness depends on observed data but not on the missing data itself. MNAR: missingness depends on the value of the missing data itself. MNAR is more problematic because imputation methods that don't account for the dependency will be biased, making accurate inference difficult.",
      "C": "MAR and MNAR are equally problematic; both require deletion.",
      "D": "MAR is only for categorical data, MNAR for continuous."
    },
    "answer": "B",
    "explanation": "Missing at Random (MAR) means the probability of missingness depends on observed variables but not on the missing variable's unobserved value. Missing Not At Random (MNAR) means the missingness depends on the unobserved value of the missing data itself. MNAR is much more problematic because standard imputation methods often assume MAR, and if MNAR is present, these methods will yield biased results unless the missingness mechanism can be explicitly modeled.",
    "topic": "Data Preprocessing / Missing Data",
    "difficulty": "Hard"
  },
  {
    "id": 268,
    "question": "What is 'kernel density estimation (KDE)' and how is it used in data visualization and non-parametric density estimation?",
    "options": {
      "A": "A method for finding the mean of a dataset. Used for central tendency.",
      "B": "A non-parametric method for estimating the probability density function of a random variable. It smooths out the data points to create a continuous curve, often used to visualize the shape of a distribution when a histogram might be too jagged.",
      "C": "A method for fitting a linear regression model. Used for prediction.",
      "D": "A method for performing clustering analysis. Used for grouping data points."
    },
    "answer": "B",
    "explanation": "Kernel Density Estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. It involves placing a 'kernel' (a small probability distribution, often a Gaussian) at each data point and summing these kernels to create a smooth estimate of the underlying density, providing a more visually appealing representation of data distribution than a histogram.",
    "topic": "Nonparametric Methods / Data Visualization",
    "difficulty": "Hard"
  },
  {
    "id": 269,
    "question": "Explain the concept of 'statistical interaction' in multi-variable models. Provide an example where ignoring an interaction might lead to incorrect conclusions.",
    "options": {
      "A": "Interaction means variables are independent. Ignoring it has no impact.",
      "B": "Interaction occurs when the effect of one independent variable on the dependent variable changes depending on the level of another independent variable. Example: A drug's effectiveness might be different for men vs. women. Ignoring this might lead to concluding the drug is ineffective overall when it's highly effective for one gender and ineffective for the other.",
      "C": "Interaction refers to the correlation between variables. Ignoring it leads to multicollinearity.",
      "D": "Interaction means a curvilinear relationship. Ignoring it leads to non-linearity issues."
    },
    "answer": "B",
    "explanation": "A statistical interaction means that the combined effect of two or more independent variables on a dependent variable is different from the sum of their individual effects. Ignoring a significant interaction can lead to misleading conclusions because the main effects alone do not accurately represent the relationships. For example, a treatment might be beneficial for one subgroup but harmful for another; ignoring the interaction would lead to a false conclusion about the overall treatment effect.",
    "topic": "Regression Analysis / Model Interpretation",
    "difficulty": "Hard"
  },
  {
    "id": 270,
    "question": "What is 'endogeneity' in econometrics and regression analysis, and why is it a serious problem for causal inference?",
    "options": {
      "A": "Endogeneity means variables are perfectly correlated. Problem: multicollinearity.",
      "B": "Endogeneity occurs when an independent variable is correlated with the error term of a regression model. Problem: leads to biased and inconsistent coefficient estimates, making causal inference impossible without addressing it. Causes: omitted variable bias, measurement error, simultaneity/reverse causality.",
      "C": "Endogeneity means the dependent variable is categorical. Problem: cannot use linear regression.",
      "D": "Endogeneity means the sample size is too small. Problem: low power."
    },
    "answer": "B",
    "explanation": "Endogeneity is a fundamental problem in regression analysis when trying to establish causality. If an independent variable is correlated with the error term, the ordinary least squares (OLS) estimator will be biased and inconsistent, meaning the estimated coefficients do not accurately reflect the true causal effects. Common sources include omitted variable bias, measurement error in covariates, and simultaneity/reverse causality.",
    "topic": "Causal Inference / Econometrics",
    "difficulty": "Hard"
  },
  {
    "id": 271,
    "question": "Explain the concept of 'regularization' in the context of generalized linear models (GLMs) like logistic regression. Why is it used?",
    "options": {
      "A": "Regularization helps normalize the data for GLMs.",
      "B": "Regularization (e.g., L1/Lasso, L2/Ridge) adds a penalty to the loss function of GLMs to shrink coefficient estimates, preventing overfitting and improving model generalization, especially when dealing with high-dimensional data or correlated predictors.",
      "C": "Regularization makes GLMs non-linear. Used for complex relationships.",
      "D": "Regularization only applies to linear regression, not GLMs."
    },
    "answer": "B",
    "explanation": "Just as in linear regression, regularization is used in GLMs (like logistic or Poisson regression) to prevent overfitting. By adding a penalty to the objective function, it discourages excessively large coefficients, leading to simpler models that generalize better to new data, and can also help with feature selection (Lasso).",
    "topic": "Regression Analysis / GLM / Regularization",
    "difficulty": "Hard"
  },
  {
    "id": 272,
    "question": "What are 'survival functions' and 'hazard functions' in survival analysis, and how do they differ?",
    "options": {
      "A": "Survival functions: probability of surviving. Hazard functions: probability of dying. No difference.",
      "B": "Survival function (S(t)): The probability that an individual survives beyond time t. Hazard function (h(t)): The instantaneous rate of an event occurring at time t, given that it has not occurred before time t. Survival function is cumulative probability, hazard function is instantaneous risk.",
      "C": "Survival function: for continuous data. Hazard function: for discrete data.",
      "D": "They are unrelated concepts in survival analysis."
    },
    "answer": "B",
    "explanation": "The survival function S(t) gives the probability that an individual will survive beyond a certain time t. The hazard function h(t), on the other hand, describes the instantaneous risk of experiencing the event at time t, given that the individual has survived up to time t. The hazard function is a rate, while the survival function is a cumulative probability.",
    "topic": "Survival Analysis",
    "difficulty": "Hard"
  },
  {
    "id": 273,
    "question": "Explain the concept of 'instrumental variables (IV)' estimation in a broader sense and its core assumptions for valid causal inference.",
    "options": {
      "A": "IV is a descriptive statistic. Assumptions: data is normally distributed.",
      "B": "IV is a method to estimate causal effects in observational studies. Core assumptions: 1) Relevance (instrument affects treatment), 2) Exclusion restriction (instrument affects outcome only through treatment), 3) Independence (instrument is independent of unmeasured confounders).",
      "C": "IV is a type of randomization. Assumptions: large sample size.",
      "D": "IV estimates conditional probabilities. Assumptions: only for binary outcomes."
    },
    "answer": "B",
    "explanation": "Instrumental variables (IV) estimation is a powerful technique for addressing endogeneity and estimating causal effects in non-experimental settings. Its validity critically depends on three strong (and often untestable) assumptions: the instrument must be relevant to the exposure, it must affect the outcome only through the exposure (exclusion restriction), and it must be independent of any unmeasured confounders that affect both the exposure and outcome.",
    "topic": "Causal Inference / Instrumental Variables",
    "difficulty": "Hard"
  },
  {
    "id": 274,
    "question": "What is 'Markov Chain Monte Carlo (MCMC)' and why is it a crucial computational tool in Bayesian inference?",
    "options": {
      "A": "MCMC is a method for generating random numbers. Crucial for descriptive statistics.",
      "B": "MCMC is a class of algorithms for sampling from complex probability distributions (especially posterior distributions in Bayesian models) by constructing a Markov chain whose stationary distribution is the target distribution. Crucial because analytical solutions for posterior distributions are often intractable, allowing for numerical approximation.",
      "C": "MCMC is a hypothesis testing framework. Crucial for frequentist statistics.",
      "D": "MCMC is a data cleaning technique. Crucial for data preprocessing."
    },
    "answer": "B",
    "explanation": "MCMC algorithms are essential in Bayesian inference because for many complex models, the posterior distribution cannot be computed analytically. MCMC provides a way to draw samples from this posterior distribution, allowing for estimation of posterior means, credible intervals, and other quantities of interest, thereby making complex Bayesian models computationally feasible.",
    "topic": "Bayesian Statistics / Computational Methods",
    "difficulty": "Hard"
  },
  {
    "id": 275,
    "question": "Discuss the problem of 'multiple comparisons' in statistical testing. How does it arise, and what are the general strategies to control for it beyond Bonferroni?",
    "options": {
      "A": "Arises from too few tests. Strategies: do more tests.",
      "B": "Arises when performing multiple hypothesis tests, increasing the probability of a Type I error (false positive) across all tests. Strategies: Family-Wise Error Rate (FWER) control (Bonferroni, Holm), False Discovery Rate (FDR) control (Benjamini-Hochberg).",
      "C": "Arises from too many variables. Strategies: reduce variables.",
      "D": "Arises from small sample sizes. Strategies: increase sample size."
    },
    "answer": "B",
    "explanation": "The multiple comparisons problem occurs because each test has a chance of a Type I error (α). Performing many tests inflates the overall probability of at least one Type I error. Strategies include controlling the Family-Wise Error Rate (FWER) which is the probability of at least one Type I error (e.g., Holm, Sidak), or controlling the False Discovery Rate (FDR) which is the expected proportion of false discoveries among all rejected hypotheses (e.g., Benjamini-Hochberg).",
    "topic": "Multiple Comparisons",
    "difficulty": "Hard"
  },
  {
    "id": 276,
    "question": "What is a 'mixed-effects model' (or 'multilevel model') and when is it appropriate to use?",
    "options": {
      "A": "A model for mixed types of data (e.g., categorical and continuous).",
      "B": "A statistical model that contains both fixed effects and random effects. Appropriate when data has a hierarchical or clustered structure (e.g., students within classes, patients within hospitals) or repeated measurements on the same individuals, allowing for estimation of both population-level effects and individual/group-level variability.",
      "C": "A model used when independent variables are highly correlated. Appropriate for multicollinearity.",
      "D": "A model used for survival analysis. Appropriate for time-to-event data."
    },
    "answer": "B",
    "explanation": "Mixed-effects models are used when data points are not independent but are grouped or clustered (e.g., students nested within schools, longitudinal data from the same individuals). They allow for simultaneously modeling fixed effects (effects that are constant across all groups) and random effects (effects that vary across groups), correctly accounting for the dependency in the data and providing more accurate inferences.",
    "topic": "Advanced Regression / Mixed Models",
    "difficulty": "Hard"
  },
  {
    "id": 277,
    "question": "Explain the concept of 'identifiability' in statistical models, and why it's important for parameter estimation.",
    "options": {
      "A": "Identifiability means the model is easy to understand. Important for interpretation.",
      "B": "Identifiability means that unique parameter values can be uniquely determined from the observed data. If a model is not identifiable, different sets of parameter values can produce the same observed data, making it impossible to estimate the true parameters, leading to unstable or non-unique solutions.",
      "C": "Identifiability means the model is simple. Important for avoiding overfitting.",
      "D": "Identifiability means the model has a normal distribution. Important for hypothesis testing."
    },
    "answer": "B",
    "explanation": "Identifiability is a crucial property for statistical models. A model is identifiable if distinct parameter values always lead to distinct probability distributions of the observed data. If a model is not identifiable, there are multiple combinations of parameters that could have generated the observed data, leading to ambiguity and making it impossible to obtain unique and meaningful estimates for the parameters.",
    "topic": "Statistical Modeling / Estimation",
    "difficulty": "Hard"
  },
  {
    "id": 278,
    "question": "What is 'Grubbs' test' and when would you use it in data analysis?",
    "options": {
      "A": "A test for normality. Used before regression.",
      "B": "A test for comparing means of two small samples.",
      "C": "A statistical test used to detect outliers in a univariate data set that is assumed to come from a normally distributed population. You would use it when you suspect a single outlier or a few outliers are present and want to statistically test for their significance.",
      "D": "A test for multicollinearity. Used in multiple regression."
    },
    "answer": "C",
    "explanation": "Grubbs' test (also known as the maximum normed residual test) is specifically used to detect whether the highest or lowest value in a dataset is a significant outlier, assuming the rest of the data follows a normal distribution.",
    "topic": "Outlier Detection",
    "difficulty": "Hard"
  },
  {
    "id": 279,
    "question": "What is 'information theory' and how does it relate to statistics (e.g., in model selection)?",
    "options": {
      "A": "Information theory is about how to store data. Relates to data storage efficiency.",
      "B": "Information theory quantifies information and uncertainty. It relates to statistics by providing concepts like entropy and Kullback-Leibler divergence, which are used in model selection criteria (e.g., AIC, BIC) to balance model fit with complexity by penalizing models that waste 'information' or are overly complex.",
      "C": "Information theory is a branch of computer science, unrelated to statistics.",
      "D": "Information theory helps prove the Central Limit Theorem."
    },
    "answer": "B",
    "explanation": "Information theory, through concepts like entropy and information gain, provides a theoretical basis for evaluating and comparing statistical models. Criteria like AIC and BIC are derived from information theory principles, aiming to select models that minimize the information lost when approximating the true data-generating process, balancing model fit and complexity.",
    "topic": "Model Selection / Information Theory",
    "difficulty": "Hard"
  },
  {
    "id": 280,
    "question": "Discuss the 'likelihood principle' in statistical inference and its implications for frequentist vs. Bayesian approaches.",
    "options": {
      "A": "The likelihood principle states that inference should be based solely on the observed data's likelihood function. Implications: Bayesian inference (which updates priors via likelihood) adheres to it, while frequentist methods (which often depend on sampling distributions of unobserved data) typically violate it.",
      "B": "It states that all data must be likely. Implications: only likely data can be used.",
      "C": "It states that the prior distribution is always most important. Implications: favors Bayesian methods always.",
      "D": "It is a principle for choosing the best model. Implications: favors simple models."
    },
    "answer": "A",
    "explanation": "The likelihood principle states that all the information relevant for inference about a parameter, from an experiment, is contained in the likelihood function for the observed data. This principle is a cornerstone of Bayesian inference. Frequentist methods, however, often rely on the sampling distribution of test statistics (which considers all possible data that could have been observed, not just the actual observed data), and thus generally do not adhere to the likelihood principle.",
    "topic": "Statistical Inference / Philosophies",
    "difficulty": "Hard"
  },
  {
    "id": 281,
    "question": "What is 'propensity score stratification' and how does it compare to propensity score matching?",
    "options": {
      "A": "Stratification is for categorical variables, matching for continuous.",
      "B": "Stratification divides subjects into strata based on their propensity scores, then compares outcomes within each stratum. Matching pairs subjects with similar propensity scores. Both aim to reduce confounding, but stratification keeps all data and is often more robust to model misspecification.",
      "C": "Stratification is always better than matching. Matching is only for small samples.",
      "D": "Stratification is a form of randomization; matching is not."
    },
    "answer": "B",
    "explanation": "Both propensity score matching and stratification are methods used to balance covariates between treatment and control groups in observational studies. Stratification involves dividing the data into subgroups (strata) based on propensity scores and comparing outcomes within these strata. Matching involves finding individual control subjects with similar propensity scores to treated subjects. Stratification retains all data, which can be an advantage, and is often less sensitive to the specific functional form of the propensity score model.",
    "topic": "Causal Inference / Propensity Scores",
    "difficulty": "Hard"
  },
  {
    "id": 282,
    "question": "Explain the concept of 'nuisance parameters' in statistical modeling and how they are typically handled.",
    "options": {
      "A": "Nuisance parameters are unimportant parameters that are ignored. Handled by removal.",
      "B": "Nuisance parameters are parameters that are not of primary interest but must be estimated to make inferences about the parameters of interest. Handled by marginalization (Bayesian) or profiling (Frequentist), where they are integrated or maximized out.",
      "C": "Nuisance parameters are only found in non-linear models. Handled by linearization.",
      "D": "Nuisance parameters are those causing multicollinearity. Handled by regularization."
    },
    "answer": "B",
    "explanation": "A nuisance parameter is a parameter in a statistical model that is not directly of interest but needs to be accounted for when making inferences about the parameters that are of interest. In frequentist statistics, they are often 'profiled out' by maximizing the likelihood function with respect to them. In Bayesian statistics, they are 'marginalized out' by integrating over their posterior distribution.",
    "topic": "Statistical Modeling / Parameter Estimation",
    "difficulty": "Hard"
  },
  {
    "id": 283,
    "question": "What is the 'zero-inflated Poisson (ZIP) model' and when would it be an appropriate choice for analyzing count data?",
    "options": {
      "A": "A model for continuous data with many zeros. Appropriate for highly skewed data.",
      "B": "A model for count data that specifically accounts for an excess number of zero observations beyond what a standard Poisson distribution would predict. Appropriate when there are two distinct processes generating zeros: one where the event truly cannot occur, and another where it simply hasn't occurred (like in standard Poisson). Example: number of times a person visits a doctor, where some never visit, and others visit some count.",
      "C": "A model for binary data with zero failures. Appropriate for perfectly predicted outcomes.",
      "D": "A model for time-to-event data with zero events. Appropriate for survival analysis."
    },
    "answer": "B",
    "explanation": "The Zero-Inflated Poisson (ZIP) model is used for count data when there are a higher number of zero counts than would be expected from a standard Poisson distribution. It assumes that the excess zeros come from a separate process (e.g., some individuals never experience the event, while others do with a certain Poisson rate), making it suitable for situations where two mechanisms could produce a zero count.",
    "topic": "Generalized Linear Models / Count Data",
    "difficulty": "Hard"
  },
  {
    "id": 284,
    "question": "Explain the concept of 'shrinkage estimators' beyond regularization, using an example like the 'Stein's paradox' or 'James-Stein estimator'.",
    "options": {
      "A": "Shrinkage estimators always reduce bias. Example: mean.",
      "B": "Shrinkage estimators systematically move estimates towards a central value or a prior belief to reduce variance and overall mean squared error, even if it introduces some bias. James-Stein estimator is a classic example demonstrating that for three or more dimensions, shrinking individual means towards a common mean can yield better overall estimates than separate individual estimates, even if the common mean is arbitrary.",
      "C": "Shrinkage estimators are only for small samples. Example: t-test.",
      "D": "Shrinkage estimators remove outliers. Example: median."
    },
    "answer": "B",
    "explanation": "Shrinkage estimators, like the James-Stein estimator, pull parameter estimates towards a common value (often the grand mean or zero). This seemingly counter-intuitive approach, particularly in higher dimensions, can paradoxically reduce the overall mean squared error of the estimates, even at the cost of introducing a small amount of bias, by significantly reducing the variance.",
    "topic": "Estimation / Advanced Concepts",
    "difficulty": "Hard"
  },
  {
    "id": 285,
    "question": "What is the 'Fisher Information' in statistical inference, and what role does it play in determining the 'Cramér-Rao Lower Bound'?",
    "options": {
      "A": "Fisher Information is a measure of model complexity. Lower bound: minimum complexity.",
      "B": "Fisher Information measures the amount of information that an observable random variable carries about an unknown parameter. It is the negative expected value of the second derivative of the log-likelihood function. The Cramér-Rao Lower Bound states that the variance of any unbiased estimator of a parameter is at least as large as the reciprocal of the Fisher Information, providing a theoretical minimum variance for estimators.",
      "C": "Fisher Information is a measure of data quality. Lower bound: minimum data quality.",
      "D": "Fisher Information is related to the p-value. Lower bound: minimum p-value."
    },
    "answer": "B",
    "explanation": "Fisher Information quantifies the amount of information that a random sample contains about an unknown parameter. A higher Fisher Information means the parameter can be estimated more precisely. The Cramér-Rao Lower Bound is a fundamental result in estimation theory, stating that there's a theoretical lower limit on the variance of any unbiased estimator, and this limit is inversely proportional to the Fisher Information.",
    "topic": "Statistical Inference / Estimation Theory",
    "difficulty": "Hard"
  },
  {
    "id": 286,
    "question": "Explain 'exact tests' (e.g., Fisher's Exact Test) and when they are preferred over asymptotic tests (e.g., Chi-square test).",
    "options": {
      "A": "Exact tests are for continuous data. Preferred for large samples.",
      "B": "Exact tests calculate probabilities directly from the permutations of the data, rather than relying on asymptotic approximations. Preferred when sample sizes are small or when expected cell counts in contingency tables are low, as asymptotic tests may be inaccurate under these conditions.",
      "C": "Exact tests always produce p-values of exactly 0 or 1. Preferred for clear results.",
      "D": "Exact tests are for qualitative variables, asymptotic for quantitative."
    },
    "answer": "B",
    "explanation": "Exact tests, like Fisher's Exact Test for contingency tables, compute the exact probability of observing the data (or more extreme data) under the null hypothesis, by considering all possible arrangements of the data that maintain the marginal totals. They are preferred over asymptotic tests (like the Chi-square test) when sample sizes are small or expected cell counts are low, where the large-sample assumptions of asymptotic tests are violated, leading to inaccurate p-values.",
    "topic": "Nonparametric Tests / Hypothesis Testing",
    "difficulty": "Hard"
  },
  {
    "id": 287,
    "question": "What is the 'Generalized Method of Moments (GMM)' in econometrics, and what problem does it address?",
    "options": {
      "A": "GMM is a method for visualizing data. Addresses data visualization issues.",
      "B": "GMM is a flexible estimation technique that uses moment conditions (population orthogonality conditions) to estimate parameters. It addresses issues like endogeneity and heteroscedasticity, especially when the exact distribution of the data is unknown, by requiring fewer assumptions about the error distribution.",
      "C": "GMM is a simple linear regression technique. Addresses multicollinearity.",
      "D": "GMM is a non-parametric test for categorical data. Addresses small sample sizes."
    },
    "answer": "B",
    "explanation": "The Generalized Method of Moments (GMM) is a powerful and flexible estimation technique commonly used in econometrics. It addresses situations where traditional methods like OLS would yield biased estimates due to problems like endogeneity or heteroscedasticity, by using moment conditions (derived from theoretical relationships) to identify and estimate parameters without requiring full specification of the data's probability distribution.",
    "topic": "Econometrics / Estimation",
    "difficulty": "Hard"
  },
  {
    "id": 288,
    "question": "Explain the concept of 'missing completely at random (MCAR)' in the context of missing data. Why is it the 'best-case' scenario for handling missing data?",
    "options": {
      "A": "MCAR means data is missing for a reason related to other variables. Best-case because it's easy to impute.",
      "B": "MCAR means the probability of data being missing is independent of both observed and unobserved data. It's the best-case because missingness does not introduce bias, and methods like complete-case analysis (deleting rows with missing values) or mean imputation can yield unbiased estimates (though possibly inefficient).",
      "C": "MCAR means data is missing systematically. Best-case because it implies no underlying problem.",
      "D": "MCAR means only a few data points are missing. Best-case for avoiding imputation."
    },
    "answer": "B",
    "explanation": "MCAR (Missing Completely At Random) is the ideal scenario for missing data because the missingness mechanism is unrelated to any data, whether observed or unobserved. This means that the missing data is a random subset of the complete data, and simply analyzing the complete cases (after deleting rows with missing values) will yield unbiased parameter estimates, although power might be reduced.",
    "topic": "Data Preprocessing / Missing Data",
    "difficulty": "Hard"
  },
  {
    "id": 289,
    "question": "What is 'instrument strength' in instrumental variables (IV) analysis, and what happens if the instruments are 'weak'?",
    "options": {
      "A": "Instrument strength refers to the number of instruments used. Weak instruments mean fewer instruments.",
      "B": "Instrument strength refers to the correlation between the instrumental variable and the endogenous independent variable. If instruments are 'weak' (low correlation), IV estimates can be severely biased towards OLS estimates, and their standard errors can be large, leading to unreliable inferences.",
      "C": "Instrument strength is the correlation between the instrument and the outcome. Weak instruments mean low correlation with outcome.",
      "D": "Instrument strength indicates the quality of data collection. Weak instruments mean poor data."
    },
    "answer": "B",
    "explanation": "Instrument strength is crucial for the validity of IV estimates. Weak instruments (those that are only weakly correlated with the endogenous independent variable) can lead to highly biased and imprecise IV estimates. This is a significant practical challenge in IV analysis, as it can make results unreliable even if the other IV assumptions are met.",
    "topic": "Causal Inference / Instrumental Variables",
    "difficulty": "Hard"
  },
  {
    "id": 290,
    "question": "Explain the concept of 'propensity score weighting' (e.g., Inverse Probability of Treatment Weighting - IPTW) and its advantage over propensity score matching.",
    "options": {
      "A": "Weighting assigns random weights. Advantage: simpler to implement.",
      "B": "Propensity score weighting uses propensity scores to create weights for each observation, essentially balancing covariates across treatment groups by giving more weight to underrepresented subjects. Advantage: utilizes all available data, unlike matching which might discard observations, leading to less loss of precision and potentially better generalizability.",
      "C": "Weighting is for categorical outcomes only. Advantage: only for binary outcomes.",
      "D": "Weighting removes outliers. Advantage: data cleaning."
    },
    "answer": "B",
    "explanation": "Propensity score weighting (like IPTW) assigns a weight to each individual based on their propensity score, making the treated and control groups comparable on observed covariates. A key advantage over matching is that it retains all observations in the analysis, potentially leading to more efficient estimates and less loss of statistical power, and can generalize better to the full study population.",
    "topic": "Causal Inference / Propensity Scores",
    "difficulty": "Hard"
  },
  {
    "id": 291,
    "question": "What is the 'robustness check' in statistical modeling, and why is it important?",
    "options": {
      "A": "Checking if the model is robust to missing data. Important for data cleaning.",
      "B": "Running the analysis multiple times with different specifications or assumptions to see if the main conclusions remain consistent. Important because it increases confidence in the findings by showing they are not overly sensitive to specific choices made in the analysis.",
      "C": "Testing if the model can predict future data points. Important for predictive accuracy.",
      "D": "Ensuring the model is computationally efficient. Important for large datasets."
    },
    "answer": "B",
    "explanation": "A robustness check involves re-running your analysis with slightly different assumptions, specifications, or datasets to see if the core findings hold. This is crucial for strengthening the credibility of research, as it demonstrates that the results,",
    "topic": "Causal Inference / Propensity Scores",
    "difficulty": "Hard"
    }
  ]