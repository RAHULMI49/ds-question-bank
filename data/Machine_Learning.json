[
  {
    "id": 1,
    "question": "Which of the following is a supervised machine learning algorithm?",
    "options": {
      "A": "K-Means",
      "B": "Principal Component Analysis (PCA)",
      "C": "Support Vector Machine (SVM)",
      "D": "DBSCAN"
    },
    "answer": "C",
    "explanation": "SVM is a supervised learning algorithm used for classification and regression, requiring labeled data.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 2,
    "question": "What is the primary goal of a classification task in machine learning?",
    "options": {
      "A": "To predict a continuous numerical value",
      "B": "To group similar data points together",
      "C": "To assign data points to predefined categories or classes",
      "D": "To reduce the number of features in a dataset"
    },
    "answer": "C",
    "explanation": "Classification aims to predict a categorical label for new input data.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 3,
    "question": "Which of these is a common metric to evaluate a regression model?",
    "options": {
      "A": "Accuracy",
      "B": "Precision",
      "C": "Mean Squared Error (MSE)",
      "D": "F1-score"
    },
    "answer": "C",
    "explanation": "Mean Squared Error (MSE) measures the average of the squares of the errors, suitable for continuous predictions.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 4,
    "question": "What is 'training data' in machine learning?",
    "options": {
      "A": "Data used to evaluate the model's performance",
      "B": "Data used to teach the model and discover patterns",
      "C": "Data generated by the model itself",
      "D": "Data with missing values that need to be imputed"
    },
    "answer": "B",
    "explanation": "Training data is the dataset used to fit the parameters of a machine learning model.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 5,
    "question": "Which type of learning involves training a model with unlabeled data to find hidden patterns or structures?",
    "options": {
      "A": "Supervised Learning",
      "B": "Unsupervised Learning",
      "C": "Reinforcement Learning",
      "D": "Semi-supervised Learning"
    },
    "answer": "B",
    "explanation": "Unsupervised learning works with unlabeled data to discover underlying patterns, like clustering.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 6,
    "question": "What is 'feature engineering' in the context of machine learning?",
    "options": {
      "A": "The process of training a machine learning model",
      "B": "The process of creating new input features from existing ones to improve model performance",
      "C": "The process of evaluating model accuracy",
      "D": "The process of deploying a model into production"
    },
    "answer": "B",
    "explanation": "Feature engineering involves transforming raw data into features that better represent the underlying problem to the predictive models.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 7,
    "question": "Which of these is an example of an unsupervised learning task?",
    "options": {
      "A": "Predicting house prices",
      "B": "Classifying emails as spam or not spam",
      "C": "Customer segmentation based on purchasing behavior",
      "D": "Recognizing faces in images"
    },
    "answer": "C",
    "explanation": "Customer segmentation, typically done using clustering, does not require predefined labels for each customer.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 8,
    "question": "What does 'overfitting' mean in machine learning?",
    "options": {
      "A": "The model performs well on both training and test data",
      "B": "The model performs poorly on both training and test data",
      "C": "The model learns the training data too well, performing poorly on unseen data",
      "D": "The model is too simple to capture the underlying patterns"
    },
    "answer": "C",
    "explanation": "Overfitting occurs when a model learns noise and specific details from the training data, failing to generalize to new data.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 9,
    "question": "Which of the following is used to split a dataset into training and testing sets?",
    "options": {
      "A": "Feature Scaling",
      "B": "One-Hot Encoding",
      "C": "Train-Test Split",
      "D": "Cross-Validation"
    },
    "answer": "C",
    "explanation": "Train-test split is a common method to divide a dataset for model training and evaluation.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 10,
    "question": "What is the role of a 'loss function' in machine learning?",
    "options": {
      "A": "To visualize the dataset",
      "B": "To measure how well the model's predictions match the actual values",
      "C": "To preprocess the data before training",
      "D": "To optimize the model's architecture"
    },
    "answer": "B",
    "explanation": "A loss function quantifies the error between predicted and actual values, guiding the model's learning process.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 11,
    "question": "Which of these is a common method for handling missing values in a dataset?",
    "options": {
      "A": "Outlier detection",
      "B": "Imputation",
      "C": "Dimensionality reduction",
      "D": "Model stacking"
    },
    "answer": "B",
    "explanation": "Imputation involves filling in missing values using various strategies (e.g., mean, median, mode).",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 12,
    "question": "What does 'bias' refer to in the context of bias-variance trade-off?",
    "options": {
      "A": "The model's ability to fit training data well",
      "B": "The model's sensitivity to small fluctuations in the training set",
      "C": "The error introduced by approximating a real-world problem with a simplified model",
      "D": "The variance of the model's predictions"
    },
    "answer": "C",
    "explanation": "Bias refers to the simplifying assumptions made by a model to learn the target function, leading to errors from these assumptions.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 13,
    "question": "Which algorithm is a simple, linear model used for binary classification?",
    "options": {
      "A": "K-Nearest Neighbors (KNN)",
      "B": "Decision Tree",
      "C": "Logistic Regression",
      "D": "Random Forest"
    },
    "answer": "C",
    "explanation": "Logistic Regression is a linear model used for predicting the probability of a binary outcome.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 14,
    "question": "What is the main purpose of data 'normalization' or 'standardization' in machine learning?",
    "options": {
      "A": "To convert categorical data to numerical data",
      "B": "To remove duplicate records from the dataset",
      "C": "To scale numerical features to a standard range or distribution",
      "D": "To impute missing values in the dataset"
    },
    "answer": "C",
    "explanation": "Normalization/standardization scales features to prevent features with larger values from dominating those with smaller values.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 15,
    "question": "Which of these is NOT a common phase in a typical machine learning project workflow?",
    "options": {
      "A": "Data Collection and Preprocessing",
      "B": "Model Training",
      "C": "Quantum Entanglement",
      "D": "Model Evaluation"
    },
    "answer": "C",
    "explanation": "Quantum Entanglement is a concept from quantum physics and is not a standard phase in machine learning workflows.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 16,
    "question": "In K-Nearest Neighbors (KNN), what does 'K' represent?",
    "options": {
      "A": "The number of features in the dataset",
      "B": "The number of clusters to form",
      "C": "The number of nearest data points to consider for classification/regression",
      "D": "The learning rate of the algorithm"
    },
    "answer": "C",
    "explanation": "K in KNN determines how many of the nearest neighbors are used to classify a new data point.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 17,
    "question": "What is the main advantage of using a Decision Tree for classification?",
    "options": {
      "A": "They are always highly accurate",
      "B": "They are easily interpretable and visualize decision rules",
      "C": "They are immune to overfitting",
      "D": "They require very little data to train"
    },
    "answer": "B",
    "explanation": "Decision Trees are often preferred for their interpretability, as their decision logic can be easily understood and visualized.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 18,
    "question": "Which term refers to the process of finding the best set of parameters for a machine learning model?",
    "options": {
      "A": "Data cleaning",
      "B": "Model interpretation",
      "C": "Hyperparameter tuning",
      "D": "Feature scaling"
    },
    "answer": "C",
    "explanation": "Hyperparameter tuning (or optimization) involves searching for the optimal hyperparameters that yield the best model performance.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 19,
    "question": "What is a 'dataset' in machine learning?",
    "options": {
      "A": "A single data point",
      "B": "A collection of related data used for training and testing models",
      "C": "The output of a trained model",
      "D": "A set of algorithms used for data processing"
    },
    "answer": "B",
    "explanation": "A dataset is a structured collection of data, typically in tabular form, used as input for ML models.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 20,
    "question": "Which of the following is typically done *after* model training and *before* deployment?",
    "options": {
      "A": "Feature Engineering",
      "B": "Data Collection",
      "C": "Model Evaluation",
      "D": "Defining the problem"
    },
    "answer": "C",
    "explanation": "After a model is trained, its performance needs to be thoroughly evaluated on unseen data before it's put into practical use.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 21,
    "question": "In the context of machine learning, what does 'labeled data' mean?",
    "options": {
      "A": "Data that has been processed and cleaned",
      "B": "Data that comes with corresponding output or target values",
      "C": "Data that is organized into a database",
      "D": "Data that has unique identifiers"
    },
    "answer": "B",
    "explanation": "Labeled data consists of input-output pairs, where the output is the 'label' or 'target' that the model learns to predict.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 22,
    "question": "Which type of problem aims to predict a discrete category, such as 'spam' or 'not spam'?",
    "options": {
      "A": "Regression",
      "B": "Clustering",
      "C": "Classification",
      "D": "Dimensionality Reduction"
    },
    "answer": "C",
    "explanation": "Classification problems involve predicting a categorical output.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 23,
    "question": "What is the main purpose of a 'test set' in machine learning?",
    "options": {
      "A": "To train the model on diverse data",
      "B": "To tune the model's hyperparameters",
      "C": "To provide an unbiased evaluation of the final model's performance on unseen data",
      "D": "To perform feature selection"
    },
    "answer": "C",
    "explanation": "The test set is kept separate from training and validation to give a realistic assessment of how the model will perform in the real world.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 24,
    "question": "Which of these is a common technique for handling categorical features in machine learning models?",
    "options": {
      "A": "Standardization",
      "B": "Normalization",
      "C": "One-hot encoding",
      "D": "Log transformation"
    },
    "answer": "C",
    "explanation": "One-hot encoding converts categorical variables into a numerical format (binary vectors) suitable for most ML algorithms.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 25,
    "question": "What is the goal of 'regression' in machine learning?",
    "options": {
      "A": "To predict a discrete class label",
      "B": "To find hidden groups in data",
      "C": "To predict a continuous numerical value",
      "D": "To reduce the number of features"
    },
    "answer": "C",
    "explanation": "Regression models predict a real-valued output, such as price, temperature, or age.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 26,
    "question": "Which of the following describes 'underfitting'?",
    "options": {
      "A": "The model performs well on test data but poorly on training data",
      "B": "The model is too complex and learns the noise in the training data",
      "C": "The model is too simple and cannot capture the underlying patterns in the training data",
      "D": "The model has high variance"
    },
    "answer": "C",
    "explanation": "Underfitting occurs when a model is too simple and fails to learn the relationships in the training data, resulting in poor performance on both training and test sets.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 27,
    "question": "What is the purpose of 'validation data' (or 'development set')?",
    "options": {
      "A": "For final model evaluation after all training is done",
      "B": "To prevent overfitting during the final training phase",
      "C": "To tune hyperparameters and make design choices during model development",
      "D": "To collect more data for training"
    },
    "answer": "C",
    "explanation": "Validation data is used during the development phase to select the best model hyperparameters and prevent overfitting to the training set.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 28,
    "question": "Which algorithm is based on the idea of finding a hyperplane that best separates different classes in a high-dimensional space?",
    "options": {
      "A": "K-Means",
      "B": "Logistic Regression",
      "C": "Support Vector Machine (SVM)",
      "D": "Linear Regression"
    },
    "answer": "C",
    "explanation": "SVMs aim to find the optimal hyperplane that maximizes the margin between different classes.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 29,
    "question": "What is 'accuracy' as a metric in classification?",
    "options": {
      "A": "The proportion of true positives among all positive predictions",
      "B": "The proportion of actual positives that were correctly identified",
      "C": "The ratio of correctly predicted observations to the total observations",
      "D": "The harmonic mean of precision and recall"
    },
    "answer": "C",
    "explanation": "Accuracy calculates the number of correct predictions divided by the total number of predictions.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 30,
    "question": "Which of these is an example of 'reinforcement learning'?",
    "options": {
      "A": "Predicting stock prices based on historical data",
      "B": "A robot learning to walk by trial and error",
      "C": "Clustering news articles by topic",
      "D": "Identifying fraudulent transactions"
    },
    "answer": "B",
    "explanation": "Reinforcement learning involves an agent learning optimal actions in an environment through rewards and penalties.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 31,
    "question": "What is the purpose of 'cross-validation' in machine learning?",
    "options": {
      "A": "To select features for the model",
      "B": "To prevent overfitting and provide a more robust estimate of model performance",
      "C": "To normalize the data",
      "D": "To impute missing values"
    },
    "answer": "B",
    "explanation": "Cross-validation involves training and testing the model on different subsets of the data multiple times to get a more reliable performance estimate.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 32,
    "question": "Which term describes a model's ability to perform well on new, unseen data?",
    "options": {
      "A": "Memorization",
      "B": "Specialization",
      "C": "Generalization",
      "D": "Optimization"
    },
    "answer": "C",
    "explanation": "Generalization is the core goal of machine learning: to build models that perform well beyond the training data.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 33,
    "question": "What is the 'target variable' (or 'dependent variable') in a supervised learning problem?",
    "options": {
      "A": "An input feature to the model",
      "B": "The variable that the model is trained to predict",
      "C": "A variable used for data preprocessing",
      "D": "A measure of model complexity"
    },
    "answer": "B",
    "explanation": "The target variable is the output or outcome that the supervised learning model aims to predict.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 34,
    "question": "Which of these algorithms is typically used for 'clustering'?",
    "options": {
      "A": "Linear Regression",
      "B": "Decision Tree Classifier",
      "C": "K-Means",
      "D": "Support Vector Regressor"
    },
    "answer": "C",
    "explanation": "K-Means is a popular unsupervised algorithm for partitioning a dataset into K distinct, non-overlapping clusters.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 35,
    "question": "What does 'dimensionality reduction' aim to achieve?",
    "options": {
      "A": "To increase the number of features in a dataset",
      "B": "To reduce the number of samples in a dataset",
      "C": "To reduce the number of input variables/features while preserving important information",
      "D": "To improve model interpretability by making models more complex"
    },
    "answer": "C",
    "explanation": "Dimensionality reduction techniques (like PCA) reduce the number of features, which can help with computational efficiency and mitigate the curse of dimensionality.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 36,
    "question": "Which of the following is an example of an 'ensemble learning' method?",
    "options": {
      "A": "K-Nearest Neighbors",
      "B": "Support Vector Machine",
      "C": "Random Forest",
      "D": "Logistic Regression"
    },
    "answer": "C",
    "explanation": "Random Forest combines multiple decision trees to make a more robust prediction, which is a form of ensemble learning.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 37,
    "question": "What is the primary benefit of using a 'pipeline' in machine learning frameworks (e.g., scikit-learn)?",
    "options": {
      "A": "To allow models to learn without human intervention",
      "B": "To automate the entire data collection process",
      "C": "To chain multiple data preprocessing steps and a model into a single object, ensuring consistent transformations",
      "D": "To perform quantum computing operations"
    },
    "answer": "C",
    "explanation": "Pipelines streamline workflows by ensuring that preprocessing steps applied during training are consistently applied during prediction.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 38,
    "question": "Which term refers to the problem of a model learning noise in the training data rather than the underlying patterns?",
    "options": {
      "A": "Underfitting",
      "B": "Overfitting",
      "C": "Bias",
      "D": "Variance"
    },
    "answer": "B",
    "explanation": "Overfitting implies that the model has learned irrelevant details and noise from the training data, leading to poor generalization.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 39,
    "question": "What is the basic idea behind 'Occam's Razor' in machine learning?",
    "options": {
      "A": "More complex models are always better",
      "B": "Simpler models are generally preferred over complex ones if they explain the data equally well",
      "C": "Models should always be trained on the largest possible dataset",
      "D": "The best model is one that fits the training data perfectly"
    },
    "answer": "B",
    "explanation": "Occam's Razor suggests that among competing hypotheses, the one with the fewest assumptions should be selected. In ML, this translates to preferring simpler models that still perform adequately.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 40,
    "question": "Which type of chart is most suitable for visualizing the correlation between two continuous variables?",
    "options": {
      "A": "Bar Chart",
      "B": "Pie Chart",
      "C": "Scatter Plot",
      "D": "Histogram"
    },
    "answer": "C",
    "explanation": "A scatter plot effectively displays the relationship and potential correlation between two continuous variables.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 41,
    "question": "What does 'EDA' stand for in the context of data science and machine learning?",
    "options": {
      "A": "Early Data Acquisition",
      "B": "Excellent Data Analysis",
      "C": "Exploratory Data Analysis",
      "D": "Efficient Data Algorithms"
    },
    "answer": "C",
    "explanation": "Exploratory Data Analysis (EDA) is a critical step to understand the dataset's characteristics, identify patterns, and detect anomalies.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 42,
    "question": "Which of the following is a common issue faced when dealing with real-world datasets for machine learning?",
    "options": {
      "A": "Perfect data distribution",
      "B": "No missing values",
      "C": "Presence of noise and outliers",
      "D": "Infinite computational resources"
    },
    "answer": "C",
    "explanation": "Real-world datasets are often messy, containing errors, outliers, and irrelevant information (noise).",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 43,
    "question": "What is a 'hyperparameter' in machine learning?",
    "options": {
      "A": "A parameter learned by the model during training",
      "B": "A parameter whose value is set before the learning process begins",
      "C": "The output of the model's prediction",
      "D": "A type of data preprocessing technique"
    },
    "answer": "B",
    "explanation": "Hyperparameters are external to the model and are configured manually (or via optimization) before training starts (e.g., learning rate, number of trees).",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 44,
    "question": "Which term describes the process of converting raw data into a clean and suitable format for machine learning?",
    "options": {
      "A": "Model deployment",
      "B": "Data preprocessing",
      "C": "Feature selection",
      "D": "Algorithm tuning"
    },
    "answer": "B",
    "explanation": "Data preprocessing involves cleaning, transforming, and preparing the raw data to be used by ML algorithms.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 45,
    "question": "What is the 'curse of dimensionality'?",
    "options": {
      "A": "The phenomenon where model accuracy decreases with more training data",
      "B": "The problem of data becoming sparse and difficult to model in high-dimensional spaces",
      "C": "The difficulty of interpreting simple models",
      "D": "The challenge of training models on small datasets"
    },
    "answer": "B",
    "explanation": "The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces, which do not occur in low-dimensional settings.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 46,
    "question": "Which of these is a supervised learning algorithm for both classification and regression?",
    "options": {
      "A": "K-Means",
      "B": "PCA",
      "C": "Decision Tree",
      "D": "DBSCAN"
    },
    "answer": "C",
    "explanation": "Decision Trees can be adapted for both classification (predicting discrete labels) and regression (predicting continuous values).",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 47,
    "question": "What is the primary goal of an 'optimizer' in machine learning?",
    "options": {
      "A": "To speed up data loading",
      "B": "To find the best set of model parameters by minimizing the loss function",
      "C": "To visualize model performance",
      "D": "To perform feature selection"
    },
    "answer": "B",
    "explanation": "Optimizers adjust the model's internal parameters (weights and biases) during training to reduce the error measured by the loss function.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 48,
    "question": "Which type of data is typically used to make predictions on after a model has been trained and evaluated?",
    "options": {
      "A": "Training data",
      "B": "Test data",
      "C": "New, unseen data",
      "D": "Validation data"
    },
    "answer": "C",
    "explanation": "Once a model is deemed satisfactory, it's deployed to make predictions on new data that it has never encountered before.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 49,
    "question": "What does 'imputation' involve in data preprocessing?",
    "options": {
      "A": "Removing outliers from the dataset",
      "B": "Converting numerical data to categorical data",
      "C": "Filling in missing values in the dataset",
      "D": "Scaling numerical features to a smaller range"
    },
    "answer": "C",
    "explanation": "Imputation is the process of replacing missing data with substituted values.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 50,
    "question": "Which of these is a common method for evaluating classification models?",
    "options": {
      "A": "R-squared",
      "B": "Mean Absolute Error (MAE)",
      "C": "Confusion Matrix",
      "D": "Root Mean Squared Error (RMSE)"
    },
    "answer": "C",
    "explanation": "A confusion matrix summarizes the performance of a classification model, showing true positives, true negatives, false positives, and false negatives.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 51,
    "question": "What is the purpose of 'scaling' numerical features in machine learning?",
    "options": {
      "A": "To convert them into categorical data",
      "B": "To make them easier to visualize",
      "C": "To ensure that features with larger values do not disproportionately influence the model",
      "D": "To reduce the number of features"
    },
    "answer": "C",
    "explanation": "Scaling ensures that all numerical features contribute equally to the distance calculations or gradient descent processes, preventing larger-scale features from dominating.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 52,
    "question": "Which concept relates to a model's ability to avoid making extreme errors?",
    "options": {
      "A": "Variance",
      "B": "Robustness",
      "C": "Interpretability",
      "D": "Complexity"
    },
    "answer": "B",
    "explanation": "Robustness refers to the model's ability to maintain its performance even with small changes or noise in the input data.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 53,
    "question": "What is 'supervised learning' primarily used for?",
    "options": {
      "A": "Discovering hidden patterns in unlabeled data",
      "B": "Learning from interactions with an environment through rewards",
      "C": "Predicting an output based on labeled input data",
      "D": "Reducing the dimensionality of datasets"
    },
    "answer": "C",
    "explanation": "Supervised learning models learn a mapping from input features to output labels using a labeled dataset.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 54,
    "question": "Which of the following is NOT a type of machine learning task?",
    "options": {
      "A": "Classification",
      "B": "Regression",
      "C": "Clustering",
      "D": "Data Deletion"
    },
    "answer": "D",
    "explanation": "Data deletion is a data management task, not a machine learning task.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 55,
    "question": "What does a 'model' represent in machine learning?",
    "options": {
      "A": "The raw input data",
      "B": "A mathematical representation that has learned patterns from data and can make predictions",
      "C": "The output predictions themselves",
      "D": "A set of rules defined manually by a human expert"
    },
    "answer": "B",
    "explanation": "A machine learning model is the output of the training process, capable of making predictions or decisions on new data.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 56,
    "question": "Which statistical concept is used to describe the relationship between two variables?",
    "options": {
      "A": "Mean",
      "B": "Median",
      "C": "Correlation",
      "D": "Mode"
    },
    "answer": "C",
    "explanation": "Correlation measures the strength and direction of a linear relationship between two variables.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 57,
    "question": "What is the typical outcome of a 'clustering' algorithm?",
    "options": {
      "A": "A single numerical prediction",
      "B": "A set of decision rules",
      "C": "Groups of similar data points (clusters)",
      "D": "A classification into predefined categories"
    },
    "answer": "C",
    "explanation": "Clustering algorithms aim to group data points such that points in the same group are more similar to each other than to those in other groups.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 58,
    "question": "Which type of data contains both input features and corresponding labels?",
    "options": {
      "A": "Unlabeled data",
      "B": "Raw data",
      "C": "Labeled data",
      "D": "Synthetic data"
    },
    "answer": "C",
    "explanation": "Labeled data is essential for supervised learning tasks, as it provides the correct answers for the model to learn from.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 59,
    "question": "Before training a machine learning model, why is data cleaning important?",
    "options": {
      "A": "It has no impact on model performance",
      "B": "To make the data visually appealing",
      "C": "To remove inconsistencies, errors, and noise that can negatively affect model training",
      "D": "To increase the size of the dataset"
    },
    "answer": "C",
    "explanation": "Dirty data leads to poor model performance and unreliable insights. Cleaning ensures data quality.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 60,
    "question": "What does a 'confusion matrix' help in evaluating?",
    "options": {
      "A": "Regression model errors",
      "B": "Clustering quality",
      "C": "Classification model performance (True Positives, False Positives, etc.)",
      "D": "Dimensionality reduction effectiveness"
    },
    "answer": "C",
    "explanation": "A confusion matrix provides a detailed breakdown of correct and incorrect predictions for each class in a classification problem.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 61,
    "question": "Which metric measures the proportion of positive identifications that were actually correct?",
    "options": {
      "A": "Recall",
      "B": "F1-score",
      "C": "Precision",
      "D": "Accuracy"
    },
    "answer": "C",
    "explanation": "Precision answers: 'Of all the items I predicted as positive, how many are actually positive?'",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 62,
    "question": "What is the 'feature vector' of a data point?",
    "options": {
      "A": "The predicted output for that data point",
      "B": "The collection of all input features for that data point",
      "C": "A randomly generated set of numbers",
      "D": "The label associated with the data point"
    },
    "answer": "B",
    "explanation": "A feature vector is a numerical vector representing an object, where each element of the vector corresponds to a feature of the object.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 63,
    "question": "In a supervised learning task, if the output variable is continuous, what type of problem is it?",
    "options": {
      "A": "Classification",
      "B": "Clustering",
      "C": "Regression",
      "D": "Association"
    },
    "answer": "C",
    "explanation": "Regression deals with predicting continuous numerical values.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 64,
    "question": "What is the primary objective of a 'cost function' (or 'loss function')?",
    "options": {
      "A": "To maximize model interpretability",
      "B": "To quantify the error between predicted and actual values, which is then minimized",
      "C": "To add complexity to the model",
      "D": "To randomly select features for training"
    },
    "answer": "B",
    "explanation": "The cost function is what an optimization algorithm attempts to minimize during model training.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 65,
    "question": "Which of these is a form of 'data augmentation' for image datasets?",
    "options": {
      "A": "Removing duplicate images",
      "B": "Resizing images to a fixed dimension",
      "C": "Applying random rotations or flips to existing images",
      "D": "Converting color images to grayscale"
    },
    "answer": "C",
    "explanation": "Data augmentation involves creating new training examples by applying transformations to existing ones, helping to prevent overfitting and improve generalization.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 66,
    "question": "What does 'model deployment' involve?",
    "options": {
      "A": "Training the model on a new dataset",
      "B": "Making the trained machine learning model available for predictions in a real-world application",
      "C": "Collecting more data for the model",
      "D": "Evaluating the model's performance on the test set"
    },
    "answer": "B",
    "explanation": "Deployment is the final stage where the trained model is integrated into a system to make predictions or decisions on live data.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 67,
    "question": "Which type of learning involves an agent learning through interaction with an environment, receiving rewards or penalties?",
    "options": {
      "A": "Supervised Learning",
      "B": "Unsupervised Learning",
      "C": "Reinforcement Learning",
      "D": "Semi-supervised Learning"
    },
    "answer": "C",
    "explanation": "Reinforcement learning is ideal for problems where an agent needs to learn a sequence of actions to maximize a cumulative reward.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 68,
    "question": "What is the primary role of a 'decision boundary' in a classification model?",
    "options": {
      "A": "To define the limits of the dataset",
      "B": "To separate different classes in the feature space",
      "C": "To indicate the model's confidence in its prediction",
      "D": "To reduce the dimensionality of the data"
    },
    "answer": "B",
    "explanation": "A decision boundary is a hypersurface that partitions the underlying vector space into two or more sets, one for each class.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 69,
    "question": "Which of these refers to the issue where a model performs well on training data but poorly on test data due to learning specific patterns and noise?",
    "options": {
      "A": "Underfitting",
      "B": "Generalization",
      "C": "Overfitting",
      "D": "Bias"
    },
    "answer": "C",
    "explanation": "Overfitting is a common problem where the model becomes too tailored to the training data.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 70,
    "question": "What does the 'F1-score' represent in classification?",
    "options": {
      "A": "The average of true positives and true negatives",
      "B": "The harmonic mean of precision and recall",
      "C": "The total number of correct predictions",
      "D": "The proportion of false positives"
    },
    "answer": "B",
    "explanation": "The F1-score provides a single metric that balances both precision and recall, especially useful for imbalanced datasets.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 71,
    "question": "What is the initial step in almost any machine learning project?",
    "options": {
      "A": "Deploying the model",
      "B": "Data collection and understanding the problem",
      "C": "Hyperparameter tuning",
      "D": "Writing the final report"
    },
    "answer": "B",
    "explanation": "Before any modeling, it's crucial to understand the business problem and gather relevant data.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 72,
    "question": "Which of the following is an example of 'numerical data'?",
    "options": {
      "A": "Colors (Red, Green, Blue)",
      "B": "Customer IDs",
      "C": "Temperature in Celsius",
      "D": "Product Categories"
    },
    "answer": "C",
    "explanation": "Temperature is a continuous, measurable quantity.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 73,
    "question": "What is the common term for the columns in a dataset that are used as input for a machine learning model?",
    "options": {
      "A": "Labels",
      "B": "Outputs",
      "C": "Features",
      "D": "Targets"
    },
    "answer": "C",
    "explanation": "Features are the independent variables or attributes that the model uses to make predictions.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 74,
    "question": "Which of these describes 'batch learning'?",
    "options": {
      "A": "The model learns continuously as new data arrives",
      "B": "The model is trained on a fixed dataset all at once",
      "C": "The model learns by interacting with an environment",
      "D": "The model updates its parameters for each individual data point"
    },
    "answer": "B",
    "explanation": "Batch learning (or offline learning) involves training the model on the entire dataset at once, then deploying it.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 75,
    "question": "What is the primary disadvantage of using a very complex model when a simpler one would suffice?",
    "options": {
      "A": "It might underfit the data",
      "B": "It is always less accurate",
      "C": "Increased risk of overfitting and reduced interpretability",
      "D": "It trains faster"
    },
    "answer": "C",
    "explanation": "Complex models are more prone to overfitting and can be harder to understand or explain.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 76,
    "question": "Which algorithm is a simple, non-parametric classifier that makes predictions based on the majority class of its 'K' nearest training examples?",
    "options": {
      "A": "Linear Regression",
      "B": "Logistic Regression",
      "C": "K-Nearest Neighbors (KNN)",
      "D": "Naive Bayes"
    },
    "answer": "C",
    "explanation": "KNN is an instance-based learning algorithm that classifies new points based on the labels of their closest neighbors.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 77,
    "question": "What is 'data leakage' in machine learning?",
    "options": {
      "A": "When data is accidentally deleted from a database",
      "B": "When information from the test set (or future data) is inadvertently used during model training",
      "C": "When too much data is collected for a project",
      "D": "When data is shared with unauthorized parties"
    },
    "answer": "B",
    "explanation": "Data leakage leads to an overly optimistic evaluation of the model's performance because it learns from information it shouldn't have access to.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 78,
    "question": "Which of the following is typically a **hyperparameter** for a Decision Tree?",
    "options": {
      "A": "Feature importance scores",
      "B": "The maximum depth of the tree",
      "C": "The split points within a node",
      "D": "The Gini impurity value at a node"
    },
    "answer": "B",
    "explanation": "The maximum depth of a decision tree is a parameter that is set by the user before training, rather than learned by the model.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 79,
    "question": "What is the goal of 'feature selection'?",
    "options": {
      "A": "To create new features from existing ones",
      "B": "To choose a subset of relevant features for use in model building",
      "C": "To scale numerical features to a standard range",
      "D": "To handle missing values in features"
    },
    "answer": "B",
    "explanation": "Feature selection aims to reduce the number of input variables to the most relevant ones, improving model performance, reducing overfitting, and speeding up training.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 80,
    "question": "Which evaluation metric is defined as True Positives / (True Positives + False Negatives)?",
    "options": {
      "A": "Precision",
      "B": "Accuracy",
      "C": "Recall (Sensitivity)",
      "D": "Specificity"
    },
    "answer": "C",
    "explanation": "Recall measures the proportion of actual positive cases that were correctly identified.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 81,
    "question": "What is the 'root' node in a Decision Tree?",
    "options": {
      "A": "A node with no outgoing branches",
      "B": "The final decision node",
      "C": "The topmost node representing the entire dataset",
      "D": "A node where a split decision is made"
    },
    "answer": "C",
    "explanation": "The root node is where the decision-making process begins for the entire dataset.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 82,
    "question": "Which type of model outputs probabilities as its direct prediction?",
    "options": {
      "A": "Linear Regression",
      "B": "Logistic Regression",
      "C": "K-Means",
      "D": "Decision Tree"
    },
    "answer": "B",
    "explanation": "Logistic Regression models the probability of a binary outcome using the sigmoid function.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 83,
    "question": "What is the primary challenge when working with imbalanced datasets in classification?",
    "options": {
      "A": "Models train too fast",
      "B": "Models tend to ignore the minority class due to its low frequency",
      "C": "It leads to underfitting automatically",
      "D": "Feature engineering becomes impossible"
    },
    "answer": "B",
    "explanation": "In imbalanced datasets, standard models might achieve high overall accuracy by simply predicting the majority class, but perform poorly on the minority class which is often the class of interest (e.g., fraud, rare disease).",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 84,
    "question": "Which technique helps to visually assess the performance of a binary classification model at various classification thresholds?",
    "options": {
      "A": "Scatter plot",
      "B": "ROC curve (Receiver Operating Characteristic)",
      "C": "Histogram",
      "D": "Box plot"
    },
    "answer": "B",
    "explanation": "The ROC curve plots the True Positive Rate against the False Positive Rate at various threshold settings, helping to evaluate the trade-off between sensitivity and specificity.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 85,
    "question": "What does 'training error' refer to?",
    "options": {
      "A": "The error rate of the model on unseen data",
      "B": "The error rate of the model on the data it was trained on",
      "C": "The error introduced during data preprocessing",
      "D": "The difference between actual and predicted hyperparameters"
    },
    "answer": "B",
    "explanation": "Training error is the error measured by the loss function on the training dataset.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 86,
    "question": "Which of the following is an example of 'categorical data'?",
    "options": {
      "A": "Age in years",
      "B": "Income",
      "C": "Gender (Male, Female)",
      "D": "Height in centimeters"
    },
    "answer": "C",
    "explanation": "Gender falls into distinct categories rather than continuous numerical values.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 87,
    "question": "What is the primary purpose of 'outlier detection'?",
    "options": {
      "A": "To remove all data points that are not perfectly normal",
      "B": "To identify data points that significantly deviate from the majority of the data",
      "C": "To increase the number of features in a dataset",
      "D": "To merge different datasets together"
    },
    "answer": "B",
    "explanation": "Outlier detection aims to find data points that are abnormal or anomalous compared to the rest of the dataset.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 88,
    "question": "Which of these is a common type of data used in machine learning?",
    "options": {
      "A": "Text data",
      "B": "Image data",
      "C": "Numerical data",
      "D": "All of the above"
    },
    "answer": "D",
    "explanation": "Machine learning models are designed to work with various data types, often requiring specific preprocessing steps for each.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 89,
    "question": "What does 'grid search' typically do in hyperparameter tuning?",
    "options": {
      "A": "Randomly selects hyperparameter values",
      "B": "Manually adjusts hyperparameters based on intuition",
      "C": "Exhaustively searches over a specified subset of hyperparameter values for a model",
      "D": "Learns the optimal hyperparameters during training"
    },
    "answer": "C",
    "explanation": "Grid search evaluates every combination of hyperparameters within a predefined grid.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 90,
    "question": "Which term describes the process of making predictions using a trained machine learning model?",
    "options": {
      "A": "Training",
      "B": "Testing",
      "C": "Inference",
      "D": "Validation"
    },
    "answer": "C",
    "explanation": "Inference is the common term for applying a trained model to new data to generate predictions.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 91,
    "question": "What is the simplest form of regression, using a single independent variable to predict a dependent variable?",
    "options": {
      "A": "Polynomial Regression",
      "B": "Multiple Linear Regression",
      "C": "Simple Linear Regression",
      "D": "Logistic Regression"
    },
    "answer": "C",
    "explanation": "Simple Linear Regression models the relationship between two variables by fitting a linear equation to observed data.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 92,
    "question": "Which of these is a non-linear activation function often used in neural networks?",
    "options": {
      "A": "Identity function",
      "B": "Linear function",
      "C": "ReLU (Rectified Linear Unit)",
      "D": "Step function (for simple perceptrons)"
    },
    "answer": "C",
    "explanation": "ReLU is widely used in hidden layers of deep neural networks due to its computational efficiency and ability to mitigate vanishing gradients.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 93,
    "question": "What is the purpose of 'stratified sampling' during train-test split for classification problems?",
    "options": {
      "A": "To ensure equal number of samples in training and testing sets",
      "B": "To maintain the same proportion of target classes in both training and testing sets",
      "C": "To randomly select data points without any specific criteria",
      "D": "To exclude outliers from the split"
    },
    "answer": "B",
    "explanation": "Stratified sampling ensures that the class distribution in the train and test sets reflects that of the original dataset, which is crucial for imbalanced datasets.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 94,
    "question": "Which term describes the data-driven process of making predictions or decisions?",
    "options": {
      "A": "Manual analysis",
      "B": "Traditional programming",
      "C": "Machine Learning",
      "D": "Statistical reporting"
    },
    "answer": "C",
    "explanation": "Machine Learning focuses on enabling systems to learn from data to perform tasks without being explicitly programmed.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 95,
    "question": "What is the 'learning rate' in an optimization algorithm (e.g., Gradient Descent)?",
    "options": {
      "A": "The number of training epochs",
      "B": "The speed at which the model makes predictions",
      "C": "The step size at each iteration while moving toward a minimum of the loss function",
      "D": "The accuracy of the model on the training data"
    },
    "answer": "C",
    "explanation": "The learning rate determines how much the model's weights are adjusted with respect to the gradient of the loss function.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 96,
    "question": "Which plot is useful for seeing the distribution of a single numerical variable?",
    "options": {
      "A": "Scatter plot",
      "B": "Bar chart",
      "C": "Histogram",
      "D": "Line chart"
    },
    "answer": "C",
    "explanation": "A histogram displays the frequency distribution of a continuous variable.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 97,
    "question": "What does the 'bias-variance trade-off' refer to?",
    "options": {
      "A": "A trade-off between model complexity and training time",
      "B": "The balance between a model's ability to fit training data (low bias) and its stability to data fluctuations (low variance)",
      "C": "The choice between using supervised or unsupervised learning",
      "D": "The trade-off between accuracy and precision"
    },
    "answer": "B",
    "explanation": "It's a fundamental concept where reducing bias (simplifying assumptions) often increases variance (sensitivity to data), and vice-versa, affecting generalization.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 98,
    "question": "Which type of variable represents categories without any intrinsic order (e.g., colors)?",
    "options": {
      "A": "Ordinal",
      "B": "Nominal",
      "C": "Interval",
      "D": "Ratio"
    },
    "answer": "B",
    "explanation": "Nominal variables are categorical without any meaningful order.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 99,
    "question": "What is 'supervised learning' characterized by?",
    "options": {
      "A": "Learning without any human intervention",
      "B": "The presence of labeled training data",
      "C": "The absence of predefined target variables",
      "D": "Focus on discovering hidden data structures"
    },
    "answer": "B",
    "explanation": "Supervised learning algorithms require a dataset where each input example is paired with an output label.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 100,
    "question": "Which of these is a common measure of error for regression models?",
    "options": {
      "A": "Accuracy",
      "B": "F1-score",
      "C": "Mean Absolute Error (MAE)",
      "D": "Precision"
    },
    "answer": "C",
    "explanation": "MAE measures the average magnitude of the errors in a set of predictions, without considering their direction.",
    "topic": "Machine Learning",
    "difficulty": "Easy"
  },
  {
    "id": 101,
    "question": "What is the primary challenge that 'regularization' techniques aim to address in machine learning?",
    "options": {
      "A": "Underfitting",
      "B": "Overfitting",
      "C": "Slow training speeds",
      "D": "High dimensionality of data"
    },
    "answer": "B",
    "explanation": "Regularization adds a penalty term to the loss function to discourage overly complex models, thereby reducing overfitting.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 102,
    "question": "In the context of ensemble methods, what is 'Bagging' short for?",
    "options": {
      "A": "Boosting Aggregated Gradients",
      "B": "Bootstrap Aggregating",
      "C": "Binary Automated Grouping",
      "D": "Bayesian Averaging for Generalization"
    },
    "answer": "B",
    "explanation": "Bagging (Bootstrap Aggregating) involves training multiple models on different bootstrap samples of the training data and then averaging their predictions.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 103,
    "question": "Which of the following is true about 'Stochastic Gradient Descent' (SGD)?",
    "options": {
      "A": "It computes the gradient using the entire dataset at each step.",
      "B": "It computes the gradient using a single randomly chosen training example at each step.",
      "C": "It requires less memory than Batch Gradient Descent but is less computationally efficient.",
      "D": "It always converges faster than Batch Gradient Descent."
    },
    "answer": "B",
    "explanation": "SGD updates model parameters based on the gradient of a single randomly chosen training example, making it faster for large datasets but potentially noisier in its convergence.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 104,
    "question": "How does 'feature scaling' (e.g., Standardization or Normalization) benefit algorithms like K-Nearest Neighbors (KNN) or Support Vector Machines (SVM)?",
    "options": {
      "A": "It reduces the number of features.",
      "B": "It improves the interpretability of the model.",
      "C": "It prevents features with larger numerical ranges from dominating the distance calculations.",
      "D": "It converts categorical features into numerical ones."
    },
    "answer": "C",
    "explanation": "Distance-based algorithms are sensitive to the scale of features. Scaling ensures that all features contribute equally to similarity measures.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 105,
    "question": "What is the purpose of 'Principal Component Analysis (PCA)'?",
    "options": {
      "A": "To perform classification on high-dimensional data.",
      "B": "To cluster data points into groups.",
      "C": "To reduce the dimensionality of a dataset by transforming features into a new set of uncorrelated components that capture most of the variance.",
      "D": "To impute missing values in a dataset."
    },
    "answer": "C",
    "explanation": "PCA is a popular linear dimensionality reduction technique used for feature extraction.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 106,
    "question": "Which of the following is a disadvantage of Decision Trees?",
    "options": {
      "A": "They are difficult to interpret.",
      "B": "They are prone to overfitting, especially when deep.",
      "C": "They cannot handle both numerical and categorical data.",
      "D": "They perform poorly on small datasets."
    },
    "answer": "B",
    "explanation": "Decision Trees can easily overfit the training data if they are allowed to grow too deep and capture noise.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 107,
    "question": "In the context of evaluating classification models, what does 'Precision' quantify?",
    "options": {
      "A": "The percentage of actual positives correctly identified.",
      "B": "The percentage of correct predictions out of all predictions made.",
      "C": "The percentage of positive predictions that were correct.",
      "D": "The percentage of actual negatives correctly identified."
    },
    "answer": "C",
    "explanation": "Precision answers: 'When the model predicts positive, how often is it correct?' (TP / (TP + FP)).",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 108,
    "question": "What is the key idea behind 'Ensemble Learning'?",
    "options": {
      "A": "Training a single, very complex model.",
      "B": "Combining predictions from multiple individual models to improve overall performance.",
      "C": "Reducing the dimensionality of the dataset before training.",
      "D": "Using different types of data for different models."
    },
    "answer": "B",
    "explanation": "Ensemble methods leverage the 'wisdom of crowds' by aggregating the decisions of several base learners.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 109,
    "question": "Which of the following is most affected by 'outliers'?",
    "options": {
      "A": "Median",
      "B": "Mode",
      "C": "Mean",
      "D": "Quartiles"
    },
    "answer": "C",
    "explanation": "The mean is sensitive to extreme values, unlike the median or mode, which are more robust to outliers.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 110,
    "question": "What is the 'kernel trick' in Support Vector Machines (SVMs)?",
    "options": {
      "A": "A method to select the best features for SVMs.",
      "B": "A technique to project linearly inseparable data into a higher-dimensional space where it becomes linearly separable, without explicitly calculating the transformation.",
      "C": "A way to reduce the number of support vectors.",
      "D": "A method for handling missing values in SVM training."
    },
    "answer": "B",
    "explanation": "The kernel trick allows SVMs to implicitly map data into higher dimensions, enabling linear separation in that space without the computational cost of the explicit transformation.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 111,
    "question": "Which metric is suitable for evaluating a classification model when the classes are highly imbalanced ?",
    "options": {
      "A": "Accuracy",
      "B": "Precision",
      "C": "Recall",
      "D": "F1-score"
    },
    "answer": "D",
    "explanation": "F1-score is the harmonic mean of precision and recall, providing a balanced measure that is less misleading than accuracy for imbalanced datasets.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 112,
    "question": "What does 'Grid Search Cross-Validation' (GridSearchCV) in scikit-learn achieve?",
    "options": {
      "A": "It trains a model once with predefined hyperparameters.",
      "B": "It only evaluates the model's performance on the test set.",
      "C": "It systematically searches for the best combination of hyperparameters for a given model using cross-validation.",
      "D": "It performs feature selection automatically."
    },
    "answer": "C",
    "explanation": "GridSearchCV automates the process of hyperparameter tuning by exhaustively trying all combinations from a specified grid.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 113,
    "question": "In Gradient Boosting, what does each successive model try to minimize?",
    "options": {
      "A": "The original target variable",
      "B": "The mean of the residuals from the previous model",
      "C": "The variance of the data",
      "D": "The training time"
    },
    "answer": "B",
    "explanation": "Gradient Boosting builds models sequentially, with each new model trying to correct the errors (residuals) of the previous models.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 114,
    "question": "When is 'Logistic Regression' a more appropriate choice than 'Linear Regression'?",
    "options": {
      "A": "When the dependent variable is continuous.",
      "B": "When the dependent variable is categorical (binary or multi-class).",
      "C": "When there are many features in the dataset.",
      "D": "When performing dimensionality reduction."
    },
    "answer": "B",
    "explanation": "Logistic Regression is a classification algorithm used when the output variable is categorical, whereas Linear Regression is for continuous outputs.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 115,
    "question": "What is the primary motivation for using 'Bias-Variance Trade-off' analysis?",
    "options": {
      "A": "To choose the best programming language for machine learning.",
      "B": "To find a balance between a model's complexity and its ability to generalize to unseen data.",
      "C": "To determine the optimal dataset size for training.",
      "D": "To decide whether to use supervised or unsupervised learning."
    },
    "answer": "B",
    "explanation": "Understanding this trade-off helps in selecting models that are complex enough to capture patterns but simple enough to avoid fitting noise.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 116,
    "question": "Which type of model is prone to high bias and low variance?",
    "options": {
      "A": "Overfitted models",
      "B": "Underfitted models (e.g., simple linear models on complex data)",
      "C": "Ensemble models with many base learners",
      "D": "Models with many features"
    },
    "answer": "B",
    "explanation": "Underfitted models are too simple, making strong assumptions (high bias), and are not sensitive to variations in training data (low variance).",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 117,
    "question": "How does 'L1 Regularization' (Lasso) differ from 'L2 Regularization' (Ridge) in its effect on model weights?",
    "options": {
      "A": "L1 tends to shrink coefficients towards zero, potentially leading to feature selection; L2 shrinks them proportionally.",
      "B": "L1 is used for classification, L2 for regression.",
      "C": "L1 increases model complexity, L2 reduces it.",
      "D": "L1 is more computationally expensive than L2."
    },
    "answer": "A",
    "explanation": "Lasso (L1) can drive some coefficients exactly to zero, effectively performing feature selection, while Ridge (L2) only shrinks them towards zero.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 118,
    "question": "What is the main advantage of using 'K-Fold Cross-Validation' over a simple train-test split?",
    "options": {
      "A": "It trains the model faster.",
      "B": "It results in a single, definitive performance metric without variation.",
      "C": "It provides a more robust and less biased estimate of model performance by using all data for both training and validation over multiple iterations.",
      "D": "It completely eliminates the risk of overfitting."
    },
    "answer": "C",
    "explanation": "K-Fold cross-validation uses different subsets of data for training and testing in each fold, leading to a more reliable estimate of generalization performance.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 119,
    "question": "Which of these is a technique for handling 'imbalanced datasets' in classification?",
    "options": {
      "A": "Applying only accuracy as a metric.",
      "B": "Oversampling the minority class or undersampling the majority class.",
      "C": "Ignoring the imbalance and training normally.",
      "D": "Converting the problem into a regression task."
    },
    "answer": "B",
    "explanation": "Techniques like SMOTE (Synthetic Minority Over-sampling Technique) or random undersampling are used to rebalance class distributions.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 120,
    "question": "What does 'bootstrapping' mean in the context of machine learning (e.g., in Bagging)?",
    "options": {
      "A": "Removing outliers from the dataset.",
      "B": "Sampling data with replacement from the original dataset to create new training sets.",
      "C": "Converting categorical features to numerical ones.",
      "D": "Training a model on the entire dataset at once."
    },
    "answer": "B",
    "explanation": "Bootstrapping is a resampling technique where samples are drawn from a dataset with replacement, meaning a single data point can appear multiple times in a bootstrap sample.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 121,
    "question": "What is the 'curse of dimensionality' a challenge for in machine learning?",
    "options": {
      "A": "Only classification algorithms",
      "B": "Algorithms that rely on distance calculations and dense data representations",
      "C": "Only tree-based algorithms",
      "D": "Only models with few features"
    },
    "answer": "B",
    "explanation": "In high-dimensional spaces, data points become very sparse, making concepts like 'distance' and 'neighborhood' less meaningful, impacting algorithms like KNN, SVM, and clustering.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 122,
    "question": "Which type of missing data mechanism implies that the missingness is not related to the missing data itself or to other variables in the dataset?",
    "options": {
      "A": "Missing At Random (MAR)",
      "B": "Missing Completely At Random (MCAR)",
      "C": "Missing Not At Random (MNAR)",
      "D": "Systematic Missingness"
    },
    "answer": "B",
    "explanation": "MCAR is the ideal scenario for handling missing data, as imputation methods are less biased.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 123,
    "question": "When might 'polynomial regression' be preferred over 'linear regression'?",
    "options": {
      "A": "When the relationship between independent and dependent variables is strictly linear.",
      "B": "When the dependent variable is categorical.",
      "C": "When the relationship between variables is non-linear but can be modeled by a polynomial function.",
      "D": "When there are very few data points."
    },
    "answer": "C",
    "explanation": "Polynomial regression can capture non-linear relationships by adding polynomial terms of the independent variables.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 124,
    "question": "What is the purpose of the 'C' hyperparameter in Support Vector Machines (SVMs)?",
    "options": {
      "A": "It controls the number of support vectors.",
      "B": "It determines the type of kernel to use (e.g., linear, RBF).",
      "C": "It controls the trade-off between achieving a low training error and a large margin (regularization parameter).",
      "D": "It sets the maximum number of iterations for training."
    },
    "answer": "C",
    "explanation": "A small C makes the margin wider and allows more misclassifications (stronger regularization), while a large C tries to get all training examples correct but with a smaller margin (less regularization).",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 125,
    "question": "Which of the following describes the output of a 'clustering' algorithm?",
    "options": {
      "A": "A set of continuous predictions for new data.",
      "B": "A trained model that can classify new data into predefined categories.",
      "C": "Assignments of data points to groups based on similarity, without predefined labels.",
      "D": "A ranked list of features by importance."
    },
    "answer": "C",
    "explanation": "Clustering is an unsupervised task that groups similar data points.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 126,
    "question": "What is the primary difference between 'Bagging' and 'Boosting' ensemble methods?",
    "options": {
      "A": "Bagging trains models sequentially, Boosting trains them in parallel.",
      "B": "Bagging uses only decision trees, Boosting uses any base learner.",
      "C": "Bagging builds independent models and averages predictions; Boosting builds models sequentially, with each new model correcting errors of previous ones.",
      "D": "Bagging is for regression, Boosting is for classification."
    },
    "answer": "C",
    "explanation": "Bagging reduces variance by averaging, while Boosting reduces bias by iteratively correcting errors.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 127,
    "question": "When might 'Polynomial Features' be used in Linear Regression?",
    "options": {
      "A": "To handle missing values in features.",
      "B": "To create new features that capture non-linear relationships between variables.",
      "C": "To reduce the dimensionality of the dataset.",
      "D": "To convert categorical features into numerical ones."
    },
    "answer": "B",
    "explanation": "Polynomial features transform existing features into higher powers (e.g., $x_1$, $x_1^2$, $x_1^3$), allowing linear models to fit non-linear data.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 128,
    "question": "Which metric is critical for evaluating how well a model detects all relevant positive cases, especially in medical diagnosis or fraud detection?",
    "options": {
      "A": "Precision",
      "B": "Accuracy",
      "C": "Recall (Sensitivity)",
      "D": "Specificity"
    },
    "answer": "C",
    "explanation": "Recall (True Positive Rate) answers: 'Of all the actual positive cases, how many did the model correctly identify?' (TP / (TP + FN)).",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 129,
    "question": "What is the primary role of 'feature importance' in tree-based models (e.g., Random Forest)?",
    "options": {
      "A": "To calculate the accuracy of the model.",
      "B": "To identify which features contributed most to the model's predictions.",
      "C": "To convert continuous features to discrete bins.",
      "D": "To determine the optimal number of trees in the forest."
    },
    "answer": "B",
    "explanation": "Feature importance scores indicate the relative utility of each feature in predicting the target variable, aiding in interpretability and potential feature selection.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 130,
    "question": "Which technique is commonly used to select a subset of features by training a model with different feature subsets and evaluating their performance?",
    "options": {
      "A": "Principal Component Analysis (PCA)",
      "B": "Linear Discriminant Analysis (LDA)",
      "C": "Wrapper methods (e.g., Recursive Feature Elimination - RFE)",
      "D": "Filter methods (e.g., correlation-based selection)"
    },
    "answer": "C",
    "explanation": "Wrapper methods involve using a specific machine learning algorithm to evaluate the performance of different feature subsets, often being more computationally intensive but yielding better performing subsets.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 131,
    "question": "What is 'Bayesian optimization' primarily used for in machine learning?",
    "options": {
      "A": "Feature engineering",
      "B": "Hyperparameter tuning",
      "C": "Model deployment",
      "D": "Data cleaning"
    },
    "answer": "B",
    "explanation": "Bayesian optimization is a sequential design strategy for finding the global optimum of expensive-to-evaluate black-box functions, commonly applied to hyperparameter optimization.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 132,
    "question": "When dealing with 'Multicollinearity' in regression models, what is the main concern?",
    "options": {
      "A": "The model will be too simple to capture patterns.",
      "B": "It makes the model computationally expensive.",
      "C": "It can lead to unstable and unreliable estimates of regression coefficients, making interpretation difficult.",
      "D": "It always causes the model to overfit."
    },
    "answer": "C",
    "explanation": "Multicollinearity occurs when independent variables are highly correlated, which can inflate the variance of the regression coefficients and make them difficult to interpret.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 133,
    "question": "What does the 'Silhouette Score' measure in clustering evaluation?",
    "options": {
      "A": "The number of clusters in the data.",
      "B": "The density of each cluster.",
      "C": "How similar an object is to its own cluster compared to other clusters (cohesion vs. separation).",
      "D": "The variance within each cluster."
    },
    "answer": "C",
    "explanation": "A higher silhouette score indicates that an object is well-matched to its own cluster and poorly matched to neighboring clusters.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 134,
    "question": "Which of these is a technique used to handle 'class imbalance' by creating synthetic samples for the minority class?",
    "options": {
      "A": "Random Undersampling",
      "B": "SMOTE (Synthetic Minority Over-sampling Technique)",
      "C": "Random Oversampling",
      "D": "Adaboost"
    },
    "answer": "B",
    "explanation": "SMOTE creates synthetic samples of the minority class by taking feature space samples and creating new examples along the line segments joining any two existing samples.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 135,
    "question": "What is 'one-hot encoding' used for?",
    "options": {
      "A": "To convert numerical features into categorical features.",
      "B": "To reduce the number of features in a dataset.",
      "C": "To convert categorical features into a binary numerical representation.",
      "D": "To normalize continuous data."
    },
    "answer": "C",
    "explanation": "One-hot encoding creates binary (0 or 1) columns for each category in a nominal feature.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 136,
    "question": "Which algorithm is a 'generative model' that estimates the joint probability distribution of features and classes?",
    "options": {
      "A": "Support Vector Machine (SVM)",
      "B": "K-Nearest Neighbors (KNN)",
      "C": "Naive Bayes",
      "D": "Decision Tree"
    },
    "answer": "C",
    "explanation": "Naive Bayes is a probabilistic classifier based on Bayes' theorem with the 'naive' assumption of conditional independence between features given the class label.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 137,
    "question": "When evaluating a regression model, what does 'R-squared' ($R^2$) represent?",
    "options": {
      "A": "The average absolute error of the predictions.",
      "B": "The proportion of the variance in the dependent variable that is predictable from the independent variables.",
      "C": "The mean of the squared errors.",
      "D": "The number of features used in the model."
    },
    "answer": "B",
    "explanation": "$R^2$ indicates how well the model explains the variability of the dependent variable.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 138,
    "question": "Which of the following is a key characteristic of 'K-Means' clustering?",
    "options": {
      "A": "It can discover clusters of arbitrary shape.",
      "B": "It requires the number of clusters (K) to be specified beforehand.",
      "C": "It is a hierarchical clustering algorithm.",
      "D": "It works well with categorical data without any preprocessing."
    },
    "answer": "B",
    "explanation": "K-Means is a partition-based clustering algorithm that requires the user to pre-define the number of clusters (K).",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 139,
    "question": "What is the purpose of a 'learning curve' in machine learning?",
    "options": {
      "A": "To visualize the decision boundary of a model.",
      "B": "To show the training and validation (or test) error as a function of the training set size.",
      "C": "To plot the relationship between two features.",
      "D": "To illustrate the convergence of an optimization algorithm."
    },
    "answer": "B",
    "explanation": "Learning curves help diagnose bias and variance issues, indicating whether more data or a more complex/simpler model is needed.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 140,
    "question": "Which type of 'feature selection' method uses statistical tests (e.g., correlation, chi-squared) to score the features independently of any learning algorithm?",
    "options": {
      "A": "Wrapper methods",
      "B": "Embedded methods",
      "C": "Filter methods",
      "D": "Dimensionality reduction methods"
    },
    "answer": "C",
    "explanation": "Filter methods assess feature relevance based on intrinsic properties of the data, independent of the model.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 141,
    "question": "What is the main idea behind 'Adaboost'?",
    "options": {
      "A": "It builds a single, powerful decision tree.",
      "B": "It combines multiple weak learners, where each subsequent learner focuses on the misclassified examples of the previous ones.",
      "C": "It trains multiple models in parallel and averages their outputs.",
      "D": "It uses principal components to improve model performance."
    },
    "answer": "B",
    "explanation": "Adaboost (Adaptive Boosting) assigns higher weights to misclassified samples, forcing subsequent weak learners to pay more attention to them.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 142,
    "question": "When might 'Support Vector Regression' (SVR) be preferred over Linear Regression?",
    "options": {
      "A": "When the relationship between variables is strictly linear.",
      "B": "When the dependent variable is categorical.",
      "C": "When the goal is to find a function that deviates from the actual target values by a margin (epsilon) rather than minimizing squared errors.",
      "D": "When the dataset size is very small."
    },
    "answer": "C",
    "explanation": "SVR aims to find a function that has at most epsilon deviation from the actual target, and anything beyond that margin is considered an error.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 143,
    "question": "What does 'DBSCAN' stand for, and what is its key advantage over K-Means?",
    "options": {
      "A": "Density-Based Spatial Clustering of Applications with Noise; it doesn't require pre-specifying the number of clusters.",
      "B": "Distance-Based Supervised Classification and ANalysis; it is faster.",
      "C": "Deep Bayesian Supervised Clustering Algorithm; it handles missing values.",
      "D": "Data-Based Standard Classification and ANomaly detection; it identifies linear relationships."
    },
    "answer": "A",
    "explanation": "DBSCAN can discover arbitrarily shaped clusters and identifies outliers as noise points, unlike K-Means which creates spherical clusters and requires K.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 144,
    "question": "Which of the following is a disadvantage of using a high 'K' value in K-Nearest Neighbors (KNN)?",
    "options": {
      "A": "Increased sensitivity to noise and outliers.",
      "B": "Higher risk of overfitting.",
      "C": "Smoother decision boundary, potentially leading to underfitting.",
      "D": "Faster prediction time."
    },
    "answer": "C",
    "explanation": "A high K leads to averaging over more neighbors, making the model less sensitive to individual data points, which can smooth out decision boundaries and cause underfitting.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 145,
    "question": "What is the primary objective of the 'Expectation-Maximization (EM)' algorithm?",
    "options": {
      "A": "To find the optimal hyperparameters for a neural network.",
      "B": "To impute missing values in a dataset with high accuracy.",
      "C": "To find maximum likelihood estimates of parameters in probabilistic models, especially when the model depends on unobserved latent variables.",
      "D": "To perform real-time predictions in production systems."
    },
    "answer": "C",
    "explanation": "EM is an iterative algorithm used for problems like Gaussian Mixture Models (GMM) where cluster assignments (latent variables) are unknown.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 146,
    "question": "When would 'Stratified K-Fold Cross-Validation' be particularly important?",
    "options": {
      "A": "For regression problems with highly correlated features.",
      "B": "When dealing with balanced classification datasets.",
      "C": "When dealing with imbalanced classification datasets to ensure class proportions are maintained in each fold.",
      "D": "When the dataset size is extremely large."
    },
    "answer": "C",
    "explanation": "Stratified K-Fold ensures that each fold maintains the same proportion of target classes as the overall dataset, preventing bias in evaluation for imbalanced classes.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 147,
    "question": "Which of the following describes an 'online learning' approach?",
    "options": {
      "A": "The model is trained once and never updated.",
      "B": "The model learns incrementally as new data arrives, adapting its parameters over time.",
      "C": "The model requires all data to be available at once for training.",
      "D": "Training is performed exclusively on cloud servers."
    },
    "answer": "B",
    "explanation": "Online learning is suitable for continuous data streams or when models need to adapt to changing patterns over time.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 148,
    "question": "What is the main idea behind 'Residual Plots' in regression analysis?",
    "options": {
      "A": "To visualize the distribution of the dependent variable.",
      "B": "To show the correlation between independent variables.",
      "C": "To assess the appropriateness of the regression model by plotting residuals against predicted values or independent variables.",
      "D": "To display the feature importance."
    },
    "answer": "C",
    "explanation": "Residual plots help detect non-linearity, heteroscedasticity (non-constant variance of errors), or presence of outliers.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 149,
    "question": "Which metric evaluates the trade-off between True Positive Rate and False Positive Rate across different threshold settings for a binary classifier?",
    "options": {
      "A": "Accuracy",
      "B": "F1-score",
      "C": "ROC AUC (Area Under the Receiver Operating Characteristic Curve)",
      "D": "Mean Absolute Error"
    },
    "answer": "C",
    "explanation": "ROC AUC provides an aggregate measure of performance across all possible classification thresholds.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 150,
    "question": "What is the primary limitation of 'Naive Bayes' algorithm?",
    "options": {
      "A": "It is computationally very expensive.",
      "B": "It assumes strong independence between features, which is rarely true in real-world data.",
      "C": "It cannot handle categorical features.",
      "D": "It is prone to overfitting for large datasets."
    },
    "answer": "B",
    "explanation": "The 'naive' assumption of conditional independence makes it a very simple classifier, but this assumption often doesn't hold true, limiting its performance on complex datasets.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 151,
    "question": "Which type of model is a 'probabilistic graphical model' that represents conditional dependencies between variables using a directed acyclic graph?",
    "options": {
      "A": "Hidden Markov Model (HMM)",
      "B": "Conditional Random Field (CRF)",
      "C": "Bayesian Network",
      "D": "Restricted Boltzmann Machine (RBM)"
    },
    "answer": "C",
    "explanation": "Bayesian Networks are used to model probabilistic relationships among a set of variables.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 152,
    "question": "In the context of 'Anomaly Detection', what does a 'one-class SVM' aim to do?",
    "options": {
      "A": "Classify data into multiple known classes.",
      "B": "Identify the boundary of normal data, considering anything outside it as an anomaly.",
      "C": "Perform regression on anomalous data points.",
      "D": "Cluster anomalous data together."
    },
    "answer": "B",
    "explanation": "One-class SVM learns a decision boundary that separates a set of normal data points from the origin in a high-dimensional feature space.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 153,
    "question": "What is 'Sequential Feature Selection' (SFS)?",
    "options": {
      "A": "A method to randomly select features for a model.",
      "B": "A greedy algorithm that either adds or removes features sequentially based on model performance.",
      "C": "A technique that uses principal components for feature reduction.",
      "D": "A process to convert time-series data into features."
    },
    "answer": "B",
    "explanation": "SFS (e.g., Sequential Forward Selection or Sequential Backward Selection) is a wrapper method that iteratively selects features.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 154,
    "question": "When might 'Hierarchical Clustering' be preferred over 'K-Means'?",
    "options": {
      "A": "When the number of clusters (K) is known beforehand.",
      "B": "When clusters are expected to be spherical.",
      "C": "When there is no prior knowledge of the number of clusters and a dendrogram is desired to visualize cluster relationships.",
      "D": "When dealing with very large datasets for faster computation."
    },
    "answer": "C",
    "explanation": "Hierarchical clustering builds a hierarchy of clusters, represented by a dendrogram, which allows for exploring different numbers of clusters without re-running the algorithm.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 155,
    "question": "What is the primary role of 'Permutation Feature Importance'?",
    "options": {
      "A": "To select the best subset of features before training.",
      "B": "To measure the importance of each feature by assessing the decrease in model performance when that feature's values are randomly shuffled.",
      "C": "To create new interaction features.",
      "D": "To normalize the feature values."
    },
    "answer": "B",
    "explanation": "Permutation importance is a model-agnostic technique that can be used for any black-box model to quantify feature importance.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 156,
    "question": "Which ensemble method trains multiple models independently and combines their predictions through averaging (for regression) or voting (for classification)?",
    "options": {
      "A": "Gradient Boosting",
      "B": "XGBoost",
      "C": "Bagging (e.g., Random Forest)",
      "D": "Adaboost"
    },
    "answer": "C",
    "explanation": "Bagging methods like Random Forest build independent trees and combine their outputs, reducing variance.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 157,
    "question": "What is the primary advantage of 'Mini-Batch Gradient Descent' over 'Stochastic Gradient Descent' (SGD)?",
    "options": {
      "A": "It always converges faster and to a better optimum than SGD.",
      "B": "It offers a balance between the computational efficiency of SGD and the stability of Batch Gradient Descent.",
      "C": "It requires more memory but converges faster per epoch than Batch Gradient Descent.",
      "D": "It can only be used with very small datasets."
    },
    "answer": "B",
    "explanation": "Mini-Batch GD updates parameters using a small batch of samples, reducing noise compared to SGD and being more memory-efficient than Batch GD.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 158,
    "question": "When might 'Stratified Shuffle Split' be preferred over 'Stratified K-Fold' cross-validation?",
    "options": {
      "A": "When the dataset is very small.",
      "B": "When a non-exhaustive number of train/test splits is desired with random permutations.",
      "C": "When reproducibility is not important.",
      "D": "When the classes are perfectly balanced."
    },
    "answer": "B",
    "explanation": "Stratified Shuffle Split provides more control over the number of iterations and the proportion of the train/test split, making it suitable when repeated random sub-sampling is preferred over exhaustive partitioning.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 159,
    "question": "What is 'Feature Interaction' in machine learning?",
    "options": {
      "A": "When two features have identical values.",
      "B": "When the effect of one feature on the target variable depends on the value of another feature.",
      "C": "When two features are perfectly uncorrelated.",
      "D": "When a feature is removed from the dataset."
    },
    "answer": "B",
    "explanation": "Feature interactions are important because they can capture complex relationships that single features cannot explain on their own.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 160,
    "question": "Which algorithm is suitable for finding optimal clusters when clusters are expected to be non-globular or have varying densities?",
    "options": {
      "A": "K-Means",
      "B": "Agglomerative Hierarchical Clustering",
      "C": "DBSCAN",
      "D": "Gaussian Mixture Models (GMM) with EM"
    },
    "answer": "C",
    "explanation": "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is capable of finding clusters of arbitrary shapes and identifying noise points.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 161,
    "question": "What is the concept of 'Cold Start Problem' in Recommender Systems?",
    "options": {
      "A": "When the system recommends too many items.",
      "B": "When there is insufficient information to make accurate recommendations for new users or new items.",
      "C": "When the recommendation engine runs too slowly.",
      "D": "When users stop interacting with the system."
    },
    "answer": "B",
    "explanation": "The cold start problem is common in recommender systems and other data-driven applications where a lack of data for new entities prevents effective modeling.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 162,
    "question": "Which evaluation metric is defined as True Negatives / (True Negatives + False Positives)?",
    "options": {
      "A": "Precision",
      "B": "Recall",
      "C": "Accuracy",
      "D": "Specificity (True Negative Rate)"
    },
    "answer": "D",
    "explanation": "Specificity measures the proportion of actual negative cases that were correctly identified as negative.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 163,
    "question": "What is the main idea behind 'Calibration' in classification models?",
    "options": {
      "A": "To adjust the model's accuracy to 100%.",
      "B": "To ensure that the predicted probabilities of a model accurately reflect the true likelihoods of events.",
      "C": "To reduce the training time of the model.",
      "D": "To convert continuous probabilities into discrete class labels."
    },
    "answer": "B",
    "explanation": "A well-calibrated model means that if it predicts a class with 80% probability, then that class actually occurs 80% of the time.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 164,
    "question": "What is the 'Elbow Method' used for in K-Means clustering?",
    "options": {
      "A": "To visualize the clusters.",
      "B": "To determine the optimal number of clusters (K).",
      "C": "To initialize the cluster centroids.",
      "D": "To evaluate the quality of a specific cluster."
    },
    "answer": "B",
    "explanation": "The elbow method plots the within-cluster sum of squares (WCSS) against the number of clusters, looking for the 'elbow' point where adding more clusters no longer significantly decreases WCSS.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 165,
    "question": "Which of these is an 'embedded method' for feature selection?",
    "options": {
      "A": "Univariate statistical tests (e.g., F-score)",
      "B": "Recursive Feature Elimination (RFE)",
      "C": "Lasso Regression (L1 regularization)",
      "D": "Principal Component Analysis (PCA)"
    },
    "answer": "C",
    "explanation": "Embedded methods perform feature selection as part of the model training process, e.g., Lasso's ability to zero out coefficients.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 166,
    "question": "What is the primary distinction between 'Parametric' and 'Non-Parametric' machine learning algorithms?",
    "options": {
      "A": "Parametric models are always more accurate.",
      "B": "Parametric models make assumptions about the underlying data distribution, while non-parametric models do not.",
      "C": "Non-parametric models are faster to train.",
      "D": "Parametric models require more data."
    },
    "answer": "B",
    "explanation": "Parametric models (e.g., Linear Regression) have a fixed number of parameters, while non-parametric models (e.g., KNN, Decision Trees) can adapt their structure based on the data.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 167,
    "question": "Which technique is used to handle 'concept drift' in machine learning models deployed in production?",
    "options": {
      "A": "Freezing model weights.",
      "B": "Regular retraining of the model with new data.",
      "C": "Ignoring performance degradation over time.",
      "D": "Reducing the learning rate to zero."
    },
    "answer": "B",
    "explanation": "Concept drift refers to changes in the relationship between input features and the target variable over time, requiring models to be periodically retrained or adapted.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 168,
    "question": "What is the 'Margin' in Support Vector Machines (SVMs)?",
    "options": {
      "A": "The distance from a data point to the nearest cluster centroid.",
      "B": "The width of the decision boundary between classes, defined by the distance to the support vectors.",
      "C": "The sum of squared errors in regression.",
      "D": "The number of misclassified points."
    },
    "answer": "B",
    "explanation": "SVMs aim to find the hyperplane that maximizes this margin, leading to better generalization.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 169,
    "question": "When might 'Isolation Forest' be a suitable algorithm?",
    "options": {
      "A": "For linear regression problems.",
      "B": "For classification tasks with highly imbalanced classes.",
      "C": "For anomaly/outlier detection in large datasets.",
      "D": "For clustering data into a predefined number of groups."
    },
    "answer": "C",
    "explanation": "Isolation Forest is an ensemble-based anomaly detection algorithm that works by explicitly isolating anomalies, which are few and different, unlike normal data points.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 170,
    "question": "What is the primary challenge addressed by 'Active Learning'?",
    "options": {
      "A": "Too much labeled data.",
      "B": "The high cost of obtaining labeled data for supervised learning.",
      "C": "The need for real-time predictions.",
      "D": "Overfitting in deep neural networks."
    },
    "answer": "B",
    "explanation": "Active learning aims to intelligently query a human oracle for labels on specific data points that would most improve the model's performance, thereby reducing labeling costs.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 171,
    "question": "In the context of 'Reinforcement Learning', what is a 'Reward Function'?",
    "options": {
      "A": "A function that determines the optimal policy for the agent.",
      "B": "A signal that indicates how good or bad an action taken by the agent was in a given state.",
      "C": "The probability of transitioning from one state to another.",
      "D": "The initial state of the environment."
    },
    "answer": "B",
    "explanation": "The reward function guides the agent's learning process by providing feedback on its actions.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 172,
    "question": "Which evaluation metric is less sensitive to class imbalance than accuracy and focuses on the minority class?",
    "options": {
      "A": "Mean Squared Error",
      "B": "R-squared",
      "C": "F1-score",
      "D": "Root Mean Squared Error"
    },
    "answer": "C",
    "explanation": "F1-score balances precision and recall, providing a better indicator of performance when dealing with skewed class distributions.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 173,
    "question": "What is the role of 'Hyperparameter Optimization'?",
    "options": {
      "A": "To remove irrelevant features from the dataset.",
      "B": "To find the best set of hyperparameters for a machine learning model to maximize its performance.",
      "C": "To train the model on larger datasets.",
      "D": "To convert numerical features into categorical features."
    },
    "answer": "B",
    "explanation": "Hyperparameter optimization is crucial for achieving optimal model performance by searching the hyperparameter space.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 174,
    "question": "Which type of 'bias' refers to the systematic error introduced by the way data is collected or measured?",
    "options": {
      "A": "Algorithm bias",
      "B": "Selection bias",
      "C": "Confirmation bias",
      "D": "Underfitting bias"
    },
    "answer": "B",
    "explanation": "Selection bias occurs when the dataset used for training is not representative of the true population or future data the model will encounter.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 175,
    "question": "What is the 'bootstrap aggregating' process in Random Forest?",
    "options": {
      "A": "Aggregating all possible decision trees into one.",
      "B": "Training each decision tree on a random subset of features.",
      "C": "Training each decision tree on a bootstrap sample (sampling with replacement) of the training data.",
      "D": "Selecting the best decision tree from a large pool."
    },
    "answer": "C",
    "explanation": "Bootstrap aggregating in Random Forest involves creating multiple bootstrapped datasets for training individual trees, leading to diverse trees.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 176,
    "question": "Which regression metric is more robust to outliers?",
    "options": {
      "A": "Mean Squared Error (MSE)",
      "B": "Root Mean Squared Error (RMSE)",
      "C": "Mean Absolute Error (MAE)",
      "D": "R-squared ($R^2$)"
    },
    "answer": "C",
    "explanation": "MAE uses absolute differences, so large errors don't get squared, making it less sensitive to outliers than MSE or RMSE.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 177,
    "question": "What is the purpose of 'Batch Normalization' in neural networks (and sometimes in other ML contexts)?",
    "options": {
      "A": "To normalize the input data to the entire network.",
      "B": "To prevent overfitting by randomly dropping out neurons.",
      "C": "To normalize the activations of previous layers, improving training stability and speed.",
      "D": "To ensure that all features have the same scale before feeding into the network."
    },
    "answer": "C",
    "explanation": "Batch Normalization addresses the 'internal covariate shift' problem, allowing higher learning rates and making networks less sensitive to initial weights.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 178,
    "question": "Which algorithm is known for its 'Lazy Learning' approach?",
    "options": {
      "A": "Linear Regression",
      "B": "Support Vector Machine",
      "C": "K-Nearest Neighbors (KNN)",
      "D": "Decision Tree"
    },
    "answer": "C",
    "explanation": "Lazy learning algorithms like KNN do not learn a generalization function during the training phase. Instead, they simply store the training data and generalize only when a query is made.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 179,
    "question": "What is the 'Inductive Bias' of a machine learning algorithm?",
    "options": {
      "A": "The dataset used for training.",
      "B": "The assumptions made by the learning algorithm to generalize from training data to unseen data.",
      "C": "The error rate on the test set.",
      "D": "The process of hyperparameter tuning."
    },
    "answer": "B",
    "explanation": "Inductive bias refers to the set of assumptions that a learning algorithm uses to predict outputs of given inputs that it has not encountered.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 180,
    "question": "When might 'Mean Absolute Percentage Error (MAPE)' be a problematic metric for regression?",
    "options": {
      "A": "When dealing with very large target values.",
      "B": "When the target variable can take zero or near-zero values.",
      "C": "When the predictions are always very accurate.",
      "D": "When the model is underfitting."
    },
    "answer": "B",
    "explanation": "MAPE involves division by the actual value, which can lead to undefined or extremely large errors when the actual value is zero or close to zero.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 181,
    "question": "Which technique is useful for visualizing high-dimensional data in 2D or 3D, preserving local structure?",
    "options": {
      "A": "PCA",
      "B": "t-SNE (t-Distributed Stochastic Neighbor Embedding)",
      "C": "Linear Discriminant Analysis (LDA)",
      "D": "Factor Analysis"
    },
    "answer": "B",
    "explanation": "t-SNE is a non-linear dimensionality reduction technique particularly well-suited for visualizing high-dimensional datasets.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 182,
    "question": "What is 'Model Stacking' in ensemble learning?",
    "options": {
      "A": "Training all base models on the same dataset.",
      "B": "Combining predictions from multiple diverse models using another learning algorithm (a meta-learner).",
      "C": "Arranging models in a sequential pipeline.",
      "D": "Training a single model on an augmented dataset."
    },
    "answer": "B",
    "explanation": "Stacking involves training a 'meta-model' or 'blender' that learns to combine the predictions of several base models.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 183,
    "question": "In the context of Decision Trees, what is 'Information Gain' used for?",
    "options": {
      "A": "To measure the accuracy of the tree.",
      "B": "To determine the best feature to split a node at each step of tree construction.",
      "C": "To calculate the total number of leaves in the tree.",
      "D": "To prune the tree after it's grown."
    },
    "answer": "B",
    "explanation": "Information Gain quantifies the reduction in entropy (or impurity) achieved by splitting a dataset on a particular feature.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 184,
    "question": "When comparing two classification models, if Model A has high precision and low recall, and Model B has low precision and high recall, which scenario favors Model A?",
    "options": {
      "A": "When false negatives are very costly (e.g., detecting a disease).",
      "B": "When false positives are very costly (e.g., spam detection where legitimate emails must not be marked as spam).",
      "C": "When both false positives and false negatives are equally costly.",
      "D": "When the dataset is perfectly balanced."
    },
    "answer": "B",
    "explanation": "High precision means fewer false positives. If misclassifying a non-spam as spam is very bad, then precision is more important.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 185,
    "question": "What is 'Synthetic Data Generation' primarily used for?",
    "options": {
      "A": "To replace all real data in a dataset.",
      "B": "To create new, artificial data points to augment datasets, especially when real data is scarce or sensitive.",
      "C": "To perform data visualization.",
      "D": "To remove redundant features."
    },
    "answer": "B",
    "explanation": "Synthetic data can help overcome data scarcity, privacy concerns, and class imbalance issues.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 186,
    "question": "Which of these is a 'discriminative model'?",
    "options": {
      "A": "Gaussian Mixture Model (GMM)",
      "B": "Linear Discriminant Analysis (LDA)",
      "C": "Naive Bayes",
      "D": "Hidden Markov Model (HMM)"
    },
    "answer": "B",
    "explanation": "Discriminative models directly model the conditional probability $P(Y|X)$, focusing on learning the boundary between classes (e.g., Logistic Regression, SVM, LDA). Generative models (like Naive Bayes, GMM, HMM) model the joint probability $P(X,Y)$.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 187,
    "question": "What is the 'Purity' metric used for in clustering evaluation?",
    "options": {
      "A": "To assess how compact the clusters are.",
      "B": "To measure how well the clusters align with known class labels (if available), indicating the proportion of points correctly assigned to their 'true' class.",
      "C": "To determine the number of outliers in a cluster.",
      "D": "To measure the density of points within a cluster."
    },
    "answer": "B",
    "explanation": "Purity is a simple, intuitive measure but can be misleading as it tends to increase with the number of clusters.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 188,
    "question": "When developing a machine learning model, why is it crucial to keep a 'test set' completely separate and untouched until final evaluation?",
    "options": {
      "A": "To speed up the training process.",
      "B": "To prevent data leakage and provide an unbiased estimate of the model's true generalization performance.",
      "C": "To make the model simpler.",
      "D": "To allow for more feature engineering to be performed."
    },
    "answer": "B",
    "explanation": "If the test set is used during model development (even for hyperparameter tuning), it loses its ability to serve as an unbiased evaluation of unseen data.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 189,
    "question": "What is the primary purpose of 'Shapley values' in model interpretability?",
    "options": {
      "A": "To calculate the accuracy of a model.",
      "B": "To determine the optimal hyperparameters for a model.",
      "C": "To fairly attribute the contribution of each feature to a model's prediction for a specific instance.",
      "D": "To visualize decision trees."
    },
    "answer": "C",
    "explanation": "Shapley values (from game theory) provide a principled way to explain individual predictions by assigning a contribution value to each feature.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 190,
    "question": "In Time Series Forecasting, which of these is a common issue that needs to be addressed?",
    "options": {
      "A": "Perfectly independent observations",
      "B": "Seasonality and trends",
      "C": "Absence of any outliers",
      "D": "Static data distribution"
    },
    "answer": "B",
    "explanation": "Time series data often exhibits patterns like seasonality (repeating cycles) and trends (long-term increase/decrease) that need to be captured by models.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 191,
    "question": "Which of the following describes 'Semi-Supervised Learning'?",
    "options": {
      "A": "Learning entirely from labeled data.",
      "B": "Learning entirely from unlabeled data.",
      "C": "Learning from a combination of a small amount of labeled data and a large amount of unlabeled data.",
      "D": "Learning through rewards and penalties."
    },
    "answer": "C",
    "explanation": "Semi-supervised learning is useful when obtaining labeled data is expensive, but unlabeled data is abundant.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 192,
    "question": "What is 'Feature Scaling' also known as in some contexts?",
    "options": {
      "A": "Dimensionality Reduction",
      "B": "Feature Engineering",
      "C": "Data Normalization / Standardization",
      "D": "One-hot Encoding"
    },
    "answer": "C",
    "explanation": "Normalization (Min-Max Scaling) and Standardization (Z-score scaling) are common methods of feature scaling.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 193,
    "question": "Which type of 'bias' in data refers to the historical or societal prejudices that are reflected in the data used to train the model?",
    "options": {
      "A": "Measurement bias",
      "B": "Algorithmic bias",
      "C": "Cognitive bias",
      "D": "Reporting bias"
    },
    "answer": "B",
    "explanation": "Algorithmic bias (or historical bias) in this context refers to the unintended reflection of societal biases present in the training data, leading to unfair or discriminatory outcomes.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 194,
    "question": "What is the core idea behind 'Local Interpretable Model-agnostic Explanations (LIME)'?",
    "options": {
      "A": "To globally explain the entire model's behavior.",
      "B": "To explain the predictions of any black-box machine learning model by locally approximating it with an interpretable model.",
      "C": "To find the most important features in a dataset.",
      "D": "To improve the accuracy of complex models."
    },
    "answer": "B",
    "explanation": "LIME focuses on explaining individual predictions by creating a simple, interpretable model around the prediction point.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 195,
    "question": "In the context of 'K-Means', what is a 'centroid'?",
    "options": {
      "A": "A data point that is an outlier.",
      "B": "The actual target label for a cluster.",
      "C": "The mean position of all data points within a cluster.",
      "D": "A randomly selected data point."
    },
    "answer": "C",
    "explanation": "Centroids are the central points of each cluster, which K-Means iteratively updates to minimize the within-cluster sum of squares.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 196,
    "question": "What is the main limitation of using 'Accuracy' as an evaluation metric for imbalanced classification problems?",
    "options": {
      "A": "It is computationally expensive to calculate.",
      "B": "It gives a misleadingly high score if the majority class is predicted most of the time.",
      "C": "It cannot be used for multi-class classification.",
      "D": "It only works for perfectly balanced datasets."
    },
    "answer": "B",
    "explanation": "In imbalanced datasets, a model can achieve high accuracy by simply predicting the majority class, even if it performs poorly on the minority class.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 197,
    "question": "Which of the following statements is true about 'Decision Boundaries' in SVMs with a Gaussian RBF kernel?",
    "options": {
      "A": "They are always linear.",
      "B": "They can be non-linear and complex, adapting to the data distribution.",
      "C": "They are only defined in the original feature space.",
      "D": "They only work for binary classification."
    },
    "answer": "B",
    "explanation": "The RBF kernel allows SVMs to create complex, non-linear decision boundaries by implicitly mapping data into an infinite-dimensional space.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 198,
    "question": "When performing 'Feature Engineering', which approach involves combining existing features to create a new, more informative feature?",
    "options": {
      "A": "Feature Scaling",
      "B": "Feature Interaction (e.g., product of two features)",
      "C": "Feature Imputation",
      "D": "Feature Removal"
    },
    "answer": "B",
    "explanation": "Feature interaction terms capture how two or more features relate to each other in predicting the target, often improving model performance.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 199,
    "question": "What is the purpose of 'Conformal Prediction' in machine learning?",
    "options": {
      "A": "To select the best hyperparameters for a model.",
      "B": "To provide statistically rigorous prediction intervals or sets with a guaranteed confidence level for individual predictions.",
      "C": "To visualize high-dimensional data.",
      "D": "To detect adversarial attacks on models."
    },
    "answer": "B",
    "explanation": "Conformal prediction adds a measure of reliability to predictions, distinguishing it from traditional point predictions or average confidence intervals.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 200,
    "question": "Which algorithm is a 'Boosting' method that is known for its high performance and robust handling of various data types, often used in Kaggle competitions?",
    "options": {
      "A": "Random Forest",
      "B": "AdaBoost",
      "C": "Gradient Boosting Machines (e.g., XGBoost, LightGBM)",
      "D": "K-Nearest Neighbors"
    },
    "answer": "C",
    "explanation": "XGBoost, LightGBM, and CatBoost are highly optimized implementations of Gradient Boosting Machines, renowned for their speed and accuracy.",
    "topic": "Machine Learning",
    "difficulty": "Medium"
  },
  {
    "id": 201,
    "question": "Which of the following is most effective in combating the 'curse of dimensionality' in datasets with many features?",
    "options": {
      "A": "Increasing the size of the training set significantly.",
      "B": "Applying complex non-linear models without any feature preprocessing.",
      "C": "Dimensionality reduction techniques (e.g., PCA, LDA) or feature selection methods.",
      "D": "Using simpler models like Linear Regression."
    },
    "answer": "C",
    "explanation": "Dimensionality reduction or feature selection helps to mitigate the sparsity issues and computational burden associated with high-dimensional data.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 202,
    "question": "In the context of evaluating a classification model, what does 'AU-PRC' (Area Under the Precision-Recall Curve) represent, and when is it preferred over AU-ROC?",
    "options": {
      "A": "It's the same as AU-ROC; no preference.",
      "B": "It measures the trade-off between sensitivity and specificity; preferred for balanced datasets.",
      "C": "It summarizes the trade-off between precision and recall; preferred for highly imbalanced datasets where the positive class is rare.",
      "D": "It indicates the model's calibration; preferred for regression tasks."
    },
    "answer": "C",
    "explanation": "AU-PRC provides a more informative view for imbalanced datasets because it focuses on the positive class and avoids the influence of easily predicted negative samples.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 203,
    "question": "What is the 'No Free Lunch' theorem in machine learning?",
    "options": {
      "A": "It states that a model trained on one dataset cannot perform well on another.",
      "B": "It asserts that no single machine learning algorithm is universally superior across all possible problems; performance is problem-dependent.",
      "C": "It implies that all models require extensive feature engineering.",
      "D": "It suggests that overfitting is unavoidable in complex models."
    },
    "answer": "B",
    "explanation": "The theorem highlights that the effectiveness of an algorithm is specific to the problem it's applied to, meaning there's no 'one-size-fits-all' best algorithm.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 204,
    "question": "Why might 'Bayesian Optimization' be preferred over 'Grid Search' or 'Random Search' for hyperparameter tuning, especially with expensive models?",
    "options": {
      "A": "It is guaranteed to find the global optimum faster.",
      "B": "It explores the hyperparameter space randomly, making it more robust.",
      "C": "It intelligently builds a probabilistic model of the objective function to guide the search, reducing the number of evaluations needed.",
      "D": "It does not require cross-validation."
    },
    "answer": "C",
    "explanation": "Bayesian optimization uses a surrogate model (often a Gaussian Process) to estimate the objective function, focusing exploration on promising regions and exploitation on known good areas.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 205,
    "question": "What is the primary objective of a 'Graph Neural Network (GNN)'?",
    "options": {
      "A": "To process sequential data like text or speech.",
      "B": "To learn representations on data structured as graphs (e.g., social networks, molecular structures) by propagating and aggregating information among nodes.",
      "C": "To generate realistic images from noise.",
      "D": "To classify tabular data with high accuracy."
    },
    "answer": "B",
    "explanation": "GNNs extend deep learning to non-Euclidean data structures, capturing relationships and features within connected data.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 206,
    "question": "In the context of 'Reinforcement Learning', what is the 'Policy' of an agent?",
    "options": {
      "A": "The current state of the environment.",
      "B": "The value function predicting future rewards.",
      "C": "A mapping from states to actions, defining the agent's behavior.",
      "D": "The immediate reward received after an action."
    },
    "answer": "C",
    "explanation": "The policy dictates what action the agent should take in any given state to maximize its cumulative reward.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 207,
    "question": "What is 'Adversarial Machine Learning' primarily concerned with?",
    "options": {
      "A": "Training models to detect highly correlated features.",
      "B": "Developing robust models that are resilient to malicious attacks designed to fool them (e.g., adversarial examples).",
      "C": "Building models that compete against each other to improve performance.",
      "D": "Optimizing models for faster training on adversarial hardware."
    },
    "answer": "B",
    "explanation": "Adversarial ML studies how to make models more secure against subtly perturbed inputs that lead to misclassifications.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 208,
    "question": "Which type of 'bias' in machine learning models refers to the simplifying assumptions made about the data distribution that might lead to systematic errors?",
    "options": {
      "A": "Algorithmic bias (ethical)",
      "B": "Measurement bias",
      "C": "Inductive bias (statistical)",
      "D": "Sampling bias"
    },
    "answer": "C",
    "explanation": "Inductive bias refers to the inherent assumptions made by a learning algorithm, which guides it to prefer certain hypotheses over others.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 209,
    "question": "In 'Natural Language Processing (NLP)', what is the main purpose of 'word embeddings' like Word2Vec or GloVe?",
    "options": {
      "A": "To convert words into one-hot encoded vectors.",
      "B": "To represent words as dense, low-dimensional vectors that capture semantic relationships.",
      "C": "To remove stop words from text documents.",
      "D": "To classify documents into predefined categories."
    },
    "answer": "B",
    "explanation": "Word embeddings allow neural networks to process words efficiently while preserving their contextual and semantic meanings, enabling tasks like semantic similarity and analogy.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 210,
    "question": "What is the 'Structural Risk Minimization (SRM)' principle, as applied in Support Vector Machines (SVMs)?",
    "options": {
      "A": "Minimizing the training error only.",
      "B": "Minimizing a bound on the generalization error, balancing model complexity with empirical risk.",
      "C": "Maximizing the complexity of the model to reduce bias.",
      "D": "Selecting the simplest model that achieves perfect training accuracy."
    },
    "answer": "B",
    "explanation": "SRM aims to find a balance between fitting the training data well (empirical risk) and controlling the complexity of the model to improve generalization performance on unseen data.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 211,
    "question": "Why is 'Autoencoder' considered an unsupervised or self-supervised learning technique?",
    "options": {
      "A": "It requires explicit labels for reconstruction.",
      "B": "It learns to reconstruct its input, using the input itself as the supervision signal.",
      "C": "It relies on external rewards for learning.",
      "D": "It performs classification on labeled data."
    },
    "answer": "B",
    "explanation": "Autoencoders are neural networks trained to reconstruct their input, effectively learning a compressed representation (encoding) of the data without external labels.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 212,
    "question": "What is the primary use case for 'Latent Dirichlet Allocation (LDA)'?",
    "options": {
      "A": "Image classification",
      "B": "Dimensionality reduction for numerical data",
      "C": "Topic modeling for text documents",
      "D": "Anomaly detection in time series"
    },
    "answer": "C",
    "explanation": "LDA is a generative probabilistic model for collections of discrete data such as text corpora, used to discover abstract 'topics' that occur in a collection of documents.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 213,
    "question": "Which approach is best suited for handling 'concept drift' when the underlying data distribution changes gradually over time?",
    "options": {
      "A": "Training a model once and never updating it.",
      "B": "Regularly retraining the model on entirely new, fresh datasets.",
      "C": "Using adaptive learning algorithms or incremental updates that can adjust to changes.",
      "D": "Only using static, historical data for training."
    },
    "answer": "C",
    "explanation": "Adaptive models, online learning, or retraining strategies that incorporate recent data are essential for maintaining performance in dynamic environments.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 214,
    "question": "What is the 'kernel density estimation (KDE)' technique primarily used for?",
    "options": {
      "A": "Classification of discrete data.",
      "B": "Estimating the probability density function of a random variable.",
      "C": "Calculating the correlation between features.",
      "D": "Performing linear regression."
    },
    "answer": "B",
    "explanation": "KDE is a non-parametric way to estimate the probability density function of a random variable, often used for data visualization or density-based clustering.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 215,
    "question": "Explain the 'multi-armed bandit' problem in reinforcement learning.",
    "options": {
      "A": "A problem where an agent must choose actions that lead to immediate high rewards only.",
      "B": "A problem where an agent must balance exploration (trying new actions) and exploitation (using known best actions) to maximize cumulative reward.",
      "C": "A problem specifically designed for image classification tasks.",
      "D": "A problem where the environment is fully observable and deterministic."
    },
    "answer": "B",
    "explanation": "The multi-armed bandit problem is a classic example illustrating the exploration-exploitation dilemma in sequential decision-making.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 216,
    "question": "What is the significance of 'Convexity' in the context of optimization for machine learning models?",
    "options": {
      "A": "It ensures that the model will always overfit the training data.",
      "B": "It guarantees that gradient descent (or similar optimization algorithms) will converge to the global minimum, not just a local minimum.",
      "C": "It means the loss function is highly complex and difficult to minimize.",
      "D": "It implies that the model has low variance."
    },
    "answer": "B",
    "explanation": "For convex loss functions, any local minimum is also the global minimum, simplifying the optimization process significantly.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 217,
    "question": "Which of these algorithms can handle 'outliers' most robustly without specific outlier removal preprocessing?",
    "options": {
      "A": "Linear Regression (OLS)",
      "B": "K-Means Clustering",
      "C": "Decision Tree / Random Forest",
      "D": "Support Vector Machines (SVM) with soft margin"
    },
    "answer": "D",
    "explanation": "SVMs with a soft margin parameter (C) are designed to tolerate some misclassifications (including those caused by outliers) to achieve a wider, more generalizable margin. Tree-based methods are also robust, but SVM explicitly models the margin.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 218,
    "question": "What is 'Causal Inference' in machine learning?",
    "options": {
      "A": "Predicting the future based on past observations.",
      "B": "Determining the effect of a treatment or intervention (cause) on an outcome, going beyond mere correlation.",
      "C": "Classifying data points into distinct categories.",
      "D": "Reducing the dimensionality of a dataset."
    },
    "answer": "B",
    "explanation": "Causal inference aims to understand 'why' something happens, not just 'what' happens, by establishing cause-and-effect relationships.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 219,
    "question": "Which technique is most effective for visualizing the structure and relationships within a large, complex dataset (e.g., in anomaly detection or fraud detection) where clusters might be non-globular and densities vary?",
    "options": {
      "A": "Pair plots (scatter plot matrix)",
      "B": "Box plots",
      "C": "UMAP (Uniform Manifold Approximation and Projection) or t-SNE",
      "D": "Histograms for individual features"
    },
    "answer": "C",
    "explanation": "UMAP and t-SNE are powerful non-linear dimensionality reduction algorithms that excel at preserving local and global data structure, making them ideal for complex data visualization and outlier identification in high dimensions.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 220,
    "question": "What is the primary challenge addressed by 'Zero-Shot Learning'?",
    "options": {
      "A": "Training models with very few labeled examples for a given class.",
      "B": "Enabling models to recognize or categorize objects/concepts they have never seen during training.",
      "C": "Reducing the computational cost of training large models.",
      "D": "Handling missing data in the input features."
    },
    "answer": "B",
    "explanation": "Zero-shot learning typically relies on auxiliary information (e.g., semantic descriptions, attribute vectors) to connect unseen classes to known ones.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 221,
    "question": "In Time Series Forecasting, what does 'Stationarity' refer to?",
    "options": {
      "A": "The absence of any trends or seasonality in the data.",
      "B": "The property where the statistical properties (mean, variance, autocorrelation) of the series do not change over time.",
      "C": "The ability of the model to predict future values with 100% accuracy.",
      "D": "The presence of strong correlation between consecutive data points."
    },
    "answer": "B",
    "explanation": "Many time series models assume stationarity, and non-stationary series often need to be transformed (e.g., differencing) to become stationary.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 222,
    "question": "What is the concept of 'Ensemble Diversity' in ensemble learning?",
    "options": {
      "A": "Using only one type of base learner for all models.",
      "B": "Ensuring that the individual base models in an ensemble make different types of errors or focus on different aspects of the data, leading to better overall performance.",
      "C": "Training all models on the exact same data.",
      "D": "Measuring the number of features used by each base model."
    },
    "answer": "B",
    "explanation": "Diversity is key for ensemble methods to work well; if all models make the same errors, combining them won't help much.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 223,
    "question": "Which technique is specifically designed to handle 'concept drift' by giving more weight to recent data points?",
    "options": {
      "A": "Batch Gradient Descent",
      "B": "Exponentially Weighted Moving Average (EWMA) for model updates or data weighting.",
      "C": "Cross-validation with a fixed split.",
      "D": "Using extremely simple models."
    },
    "answer": "B",
    "explanation": "EWMA-based approaches give more importance to recent observations, allowing the model to adapt more quickly to changes in the data distribution.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 224,
    "question": "What is the primary role of the 'epsilon-insensitive loss function' in Support Vector Regression (SVR)?",
    "options": {
      "A": "To classify data points into distinct categories.",
      "B": "To penalize errors that fall outside a certain margin (epsilon), effectively creating a 'tube' around the regression line within which errors are not penalized.",
      "C": "To measure the mean squared error for all predictions.",
      "D": "To perform feature selection."
    },
    "answer": "B",
    "explanation": "This loss function makes SVR robust to noise within the epsilon margin, focusing on finding a fit that is 'epsilon-close' to the data points.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 225,
    "question": "What is the 'Hawthorne Effect' in the context of data collection and its impact on machine learning?",
    "options": {
      "A": "The tendency of models to overfit small datasets.",
      "B": "The phenomenon where individuals modify their behavior in response to being observed, potentially biasing collected data.",
      "C": "The effect of choosing incorrect hyperparameters on model performance.",
      "D": "The computational cost associated with large datasets."
    },
    "answer": "B",
    "explanation": "This effect can lead to misleading insights if the observed behavior during data collection differs from natural behavior.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 226,
    "question": "Which of these is a technique used in 'Model Distillation' or 'Knowledge Distillation'?",
    "options": {
      "A": "Training a small 'student' model to mimic the behavior of a larger, more complex 'teacher' model.",
      "B": "Combining multiple models into one large model.",
      "C": "Extracting features from a pre-trained model for a new task.",
      "D": "Regularizing the weights of a neural network."
    },
    "answer": "A",
    "explanation": "Knowledge distillation allows transferring knowledge from a complex model to a simpler one, often resulting in improved performance for the smaller model.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 227,
    "question": "What does 'Counterfactual Explanations' aim to provide in terms of model interpretability?",
    "options": {
      "A": "A global understanding of how the model works.",
      "B": "An explanation of why a specific prediction was made by identifying the smallest changes to the input features that would change the prediction to a desired outcome.",
      "C": "The statistical significance of each feature.",
      "D": "A summary of the training data distribution."
    },
    "answer": "B",
    "explanation": "Counterfactuals answer 'What if?' questions, helping users understand what inputs would have led to a different model output.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 228,
    "question": "Which of the following is a key characteristic of 'Transfer Learning' beyond just using pre-trained weights?",
    "options": {
      "A": "Training an entirely new model from scratch.",
      "B": "Leveraging knowledge learned from a source task (often with large data) to improve performance on a related but different target task (often with limited data).",
      "C": "Reducing the dimensionality of the input data.",
      "D": "Enabling models to learn without any human supervision."
    },
    "answer": "B",
    "explanation": "Transfer learning is particularly effective in deep learning, allowing models to benefit from vast datasets used for pre-training (e.g., ImageNet) and then fine-tuned for specific tasks.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 229,
    "question": "What is the primary challenge of 'One-Shot Learning'?",
    "options": {
      "A": "Having an abundance of labeled data.",
      "B": "Training a model to recognize new classes from a single example per class.",
      "C": "Dealing with highly correlated features.",
      "D": "Building a model that runs very fast."
    },
    "answer": "B",
    "explanation": "One-shot learning is a difficult problem often addressed using techniques like Siamese networks or meta-learning, which aim to learn a similarity metric.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 230,
    "question": "Which type of 'Recommender System' relies on the similarity between users (e.g., users who liked item A also liked item B)?",
    "options": {
      "A": "Content-Based Filtering",
      "B": "Collaborative Filtering (User-User or Item-Item)",
      "C": "Hybrid Recommender System",
      "D": "Knowledge-Based Recommender System"
    },
    "answer": "B",
    "explanation": "Collaborative filtering makes recommendations by leveraging the preferences of a community of users, either based on similar users or similar items.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 231,
    "question": "In the context of 'Time Series Forecasting', what is 'Autocorrelation'?",
    "options": {
      "A": "The correlation between two different time series.",
      "B": "The correlation of a time series with a lagged version of itself.",
      "C": "The relationship between a time series and external variables.",
      "D": "The mean value of a time series over a specific period."
    },
    "answer": "B",
    "explanation": "Autocorrelation helps in identifying patterns like seasonality and trends in time series data.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 232,
    "question": "What is 'Model Agnosticism' in the context of XAI (Explainable AI)?",
    "options": {
      "A": "The ability to explain only simple, white-box models.",
      "B": "The ability of an explanation method to be applied to any machine learning model, regardless of its internal architecture.",
      "C": "A lack of trust in machine learning models.",
      "D": "The process of selecting the best model for a task."
    },
    "answer": "B",
    "explanation": "Model-agnostic methods treat the ML model as a black box and can explain predictions from various models (e.g., LIME, SHAP).",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 233,
    "question": "Which technique is most suitable for 'Outlier Detection' in high-dimensional datasets where defining a 'distance' is challenging due to sparsity?",
    "options": {
      "A": "Z-score method",
      "B": "Isolation Forest",
      "C": "K-Means Clustering (for outlier detection)",
      "D": "Interquartile Range (IQR) method"
    },
    "answer": "B",
    "explanation": "Isolation Forest is effective in high dimensions because it isolates anomalies rather than trying to profile normal data, and it does so by randomly partitioning data.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 234,
    "question": "What does the 'SVD (Singular Value Decomposition)' technique achieve in dimensionality reduction (e.g., in Latent Semantic Analysis)?",
    "options": {
      "A": "It performs clustering on high-dimensional data.",
      "B": "It factorizes a matrix into three simpler matrices, effectively finding underlying latent factors and reducing dimensionality.",
      "C": "It converts categorical features to numerical ones.",
      "D": "It selects the most important features based on correlation."
    },
    "answer": "B",
    "explanation": "SVD is a powerful matrix factorization technique used for various tasks, including dimensionality reduction and noise reduction.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 235,
    "question": "Which loss function is particularly useful for 'robust regression' when the dataset contains significant outliers?",
    "options": {
      "A": "Mean Squared Error (MSE)",
      "B": "Mean Absolute Error (MAE)",
      "C": "Huber Loss or Quantile Loss",
      "D": "Cross-Entropy Loss"
    },
    "answer": "C",
    "explanation": "Huber loss is less sensitive to outliers than MSE because it's quadratic for small errors but linear for large errors. Quantile loss is also robust as it allows focusing on specific quantiles.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 236,
    "question": "What is 'Adversarial Training' in machine learning?",
    "options": {
      "A": "Training models to compete against each other to find the best solution.",
      "B": "A technique to make models more robust to adversarial attacks by training them on adversarial examples alongside legitimate data.",
      "C": "Training models in a competitive environment to achieve faster convergence.",
      "D": "A method for hyperparameter optimization using a competitive strategy."
    },
    "answer": "B",
    "explanation": "Adversarial training is a defense mechanism against adversarial attacks, improving model robustness.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 237,
    "question": "Which of these is a limitation of 'Gaussian Mixture Models (GMM)' for clustering compared to DBSCAN?",
    "options": {
      "A": "GMM can model arbitrarily shaped clusters.",
      "B": "GMM always requires the number of clusters to be specified in advance and assumes clusters are Gaussian-distributed.",
      "C": "GMM cannot handle noisy data.",
      "D": "GMM is computationally less expensive."
    },
    "answer": "B",
    "explanation": "While GMM can model elliptical clusters, it assumes a Gaussian distribution for each component and requires 'K' beforehand, making it less flexible than DBSCAN for complex shapes or varying densities.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 238,
    "question": "What is the primary role of 'SMOTE (Synthetic Minority Over-sampling Technique)'?",
    "options": {
      "A": "To undersample the majority class in imbalanced datasets.",
      "B": "To create new, synthetic examples of the minority class to balance the dataset.",
      "C": "To remove noise from the dataset.",
      "D": "To perform feature selection for imbalanced data."
    },
    "answer": "B",
    "explanation": "SMOTE addresses class imbalance by generating synthetic data points between existing minority class instances.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 239,
    "question": "Which measure is used to evaluate the similarity between two probability distributions?",
    "options": {
      "A": "Euclidean Distance",
      "B": "Cosine Similarity",
      "C": "Kullback-Leibler (KL) Divergence",
      "D": "Jaccard Index"
    },
    "answer": "C",
    "explanation": "KL Divergence (or relative entropy) quantifies how one probability distribution diverges from a second, expected probability distribution.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 240,
    "question": "What is the main challenge of 'online learning' regarding model stability?",
    "options": {
      "A": "Models cannot learn from new data.",
      "B": "Models might suffer from 'catastrophic forgetting' where they forget previously learned information when learning new data.",
      "C": "Online learning is always stable and never needs retraining.",
      "D": "It requires more computational power than batch learning."
    },
    "answer": "B",
    "explanation": "Catastrophic forgetting is a significant hurdle in sequential learning settings, especially for neural networks.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 241,
    "question": "What is 'Model Bias' in the context of fairness and ethics in AI?",
    "options": {
      "A": "The mathematical bias of the learning algorithm.",
      "B": "The systematic and unfair preference for or against certain groups, often due to biased training data or algorithmic design.",
      "C": "The error due to underfitting.",
      "D": "The variance in model predictions."
    },
    "answer": "B",
    "explanation": "This type of bias leads to discriminatory outcomes, affecting specific demographics or attributes.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 242,
    "question": "Which method helps to ensure that a machine learning model's predictions remain consistent even if input features are slightly perturbed?",
    "options": {
      "A": "Dimensionality reduction",
      "B": "Data augmentation and adversarial training",
      "C": "One-hot encoding",
      "D": "Standardization"
    },
    "answer": "B",
    "explanation": "Data augmentation increases the diversity of training data, and adversarial training specifically exposes the model to perturbed inputs to improve its robustness.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 243,
    "question": "In the context of 'Reinforcement Learning', what is the difference between 'On-Policy' and 'Off-Policy' learning?",
    "options": {
      "A": "On-policy learns from a fixed dataset, off-policy learns online.",
      "B": "On-policy learns the value function for the policy currently being followed, while off-policy learns for a policy different from the one used to generate experiences.",
      "C": "On-policy uses a deterministic policy, off-policy uses a stochastic policy.",
      "D": "On-policy is model-free, off-policy is model-based."
    },
    "answer": "B",
    "explanation": "This distinction is crucial for algorithms like SARSA (on-policy) vs. Q-learning (off-policy), influencing how they update their value estimates.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 244,
    "question": "What is the 'Simulated Annealing' algorithm often used for in machine learning?",
    "options": {
      "A": "Clustering data into arbitrary shapes.",
      "B": "Global optimization of complex objective functions, especially in combinatorial optimization problems or for finding global minima in non-convex landscapes.",
      "C": "Dimensionality reduction for sparse data.",
      "D": "Preprocessing image data."
    },
    "answer": "B",
    "explanation": "Simulated annealing is a metaheuristic that attempts to find a good approximation to the global optimum of a given function in a large search space, inspired by the annealing process in metallurgy.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 245,
    "question": "When might 'Lasso (L1) Regression' be a more suitable choice than 'Ridge (L2) Regression'?",
    "options": {
      "A": "When multicollinearity is not an issue.",
      "B": "When you have a large number of features and suspect that only a subset of them are truly relevant (feature selection is desired).",
      "C": "When all features are expected to contribute to the model.",
      "D": "When computational speed is the only concern."
    },
    "answer": "B",
    "explanation": "Lasso's ability to drive coefficients to exactly zero makes it a powerful tool for automatic feature selection.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 246,
    "question": "What is 'Domain Adaptation' in machine learning?",
    "options": {
      "A": "Adapting a model to a new programming language.",
      "B": "Adjusting a model trained on a source domain to perform well on a target domain with a different data distribution.",
      "C": "Adapting the model to different types of data (e.g., text to images).",
      "D": "Adapting a single model to multiple tasks simultaneously."
    },
    "answer": "B",
    "explanation": "Domain adaptation is crucial when training data (source domain) differs significantly from the data encountered during deployment (target domain), preventing performance degradation.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 247,
    "question": "What is the primary purpose of 'Convex Hull' in geometric algorithms, and how is it sometimes relevant to machine learning?",
    "options": {
      "A": "To find the mean of a set of points.",
      "B": "To calculate the smallest convex polygon (or polyhedron) that encloses a given set of points; sometimes used in clustering visualization or outlier detection.",
      "C": "To transform non-linear data into linear data.",
      "D": "To classify points into convex regions."
    },
    "answer": "B",
    "explanation": "Points outside the convex hull are often considered outliers, and the hull can describe the 'shape' of a cluster.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 248,
    "question": "Which algorithm is suitable for 'one-class classification' to detect anomalies when you primarily have data for the 'normal' class?",
    "options": {
      "A": "Logistic Regression",
      "B": "Support Vector Machine (binary classifier)",
      "C": "One-Class SVM",
      "D": "K-Means"
    },
    "answer": "C",
    "explanation": "One-Class SVM is specifically designed to learn the boundary of a single class, effectively identifying observations that deviate from this learned normal profile.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 249,
    "question": "In the context of 'Explainable AI (XAI)', what is the difference between 'Local' and 'Global' interpretability?",
    "options": {
      "A": "Local explains simple models, Global explains complex models.",
      "B": "Local explanations focus on why a specific prediction for an individual instance was made; Global explanations describe the overall behavior of the model across all predictions.",
      "C": "Local is for classification, Global is for regression.",
      "D": "Local is for training, Global is for testing."
    },
    "answer": "B",
    "explanation": "Local methods (like LIME, SHAP) explain individual predictions, while global methods (e.g., permutation importance across the dataset, partial dependence plots) explain the model's general decision-making.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 250,
    "question": "What is 'Model Quantization' primarily used for in deploying machine learning models?",
    "options": {
      "A": "To increase the accuracy of the model.",
      "B": "To reduce the model's size and computational requirements by using lower precision numbers (e.g., 8-bit integers instead of 32-bit floats) for weights and activations.",
      "C": "To train the model on quantum computers.",
      "D": "To encrypt the model for security purposes."
    },
    "answer": "B",
    "explanation": "Quantization is a key technique for deploying models to resource-constrained environments like mobile devices or edge devices, often with minimal impact on accuracy.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 251,
    "question": "Which type of 'bias' in a model refers to its tendency to be too simple and make strong assumptions about the data, leading to systematic errors on both training and test data?",
    "options": {
      "A": "Variance",
      "B": "Underfitting (as a result of high bias)",
      "C": "Overfitting",
      "D": "Data leakage"
    },
    "answer": "B",
    "explanation": "High bias models make overly simplistic assumptions, failing to capture the true underlying relationships in the data.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 252,
    "question": "In 'Reinforcement Learning', what is the 'Value Function'?",
    "options": {
      "A": "A function that maps states to optimal actions.",
      "B": "A function that estimates how good it is for an agent to be in a particular state or to perform a particular action in a state, based on expected future rewards.",
      "C": "A measure of immediate reward received.",
      "D": "The probability distribution over possible states."
    },
    "answer": "B",
    "explanation": "Value functions are central to many RL algorithms, as they allow the agent to evaluate the long-term desirability of states and actions.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 253,
    "question": "What is the concept of 'Ensemble Pruning'?",
    "options": {
      "A": "Removing irrelevant features before ensemble training.",
      "B": "Selecting a subset of base models from a larger ensemble to improve performance or reduce complexity.",
      "C": "Reducing the depth of individual trees in a forest.",
      "D": "Automatically generating more base models."
    },
    "answer": "B",
    "explanation": "Ensemble pruning aims to identify and remove redundant or poorly performing base models from an ensemble, improving efficiency and sometimes performance.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 254,
    "question": "Which statistical test is often used to compare the performance of two machine learning models on the same dataset?",
    "options": {
      "A": "Chi-Squared Test",
      "B": "ANOVA",
      "C": "Paired t-test or McNemar's Test (for classification)",
      "D": "Correlation test"
    },
    "answer": "C",
    "explanation": "For comparing two models on the same dataset, statistical tests for dependent samples (like paired t-test for regression errors or McNemar's for classification) are appropriate.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 255,
    "question": "What is 'Meta-Learning' (or 'Learning to Learn')?",
    "options": {
      "A": "Learning about metadata of a dataset.",
      "B": "Training a model that can learn new tasks or adapt to new environments quickly with minimal new data, by learning an effective learning process itself.",
      "C": "Learning from errors in previous model predictions.",
      "D": "Using a combination of different learning algorithms."
    },
    "answer": "B",
    "explanation": "Meta-learning aims to enable rapid adaptation to new tasks, often by training on a variety of tasks to learn how to learn.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 256,
    "question": "When is the 'adjusted R-squared' a more appropriate metric than R-squared for regression models?",
    "options": {
      "A": "When the model is underfitting.",
      "B": "When comparing models with different numbers of features.",
      "C": "When the dataset is very small.",
      "D": "When the relationship is non-linear."
    },
    "answer": "B",
    "explanation": "Adjusted R-squared penalizes the inclusion of unnecessary features, making it a better choice for model comparison when complexity varies.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 257,
    "question": "What is 'Active Learning' commonly used for in practical applications?",
    "options": {
      "A": "To speed up model inference on edge devices.",
      "B": "To reduce the manual effort and cost of labeling large datasets by strategically selecting which unlabeled data points to query for labels.",
      "C": "To train models with no labeled data at all.",
      "D": "To continuously retrain models in real-time."
    },
    "answer": "B",
    "explanation": "Active learning is highly beneficial in domains where expert labeling is expensive and time-consuming, like medical imaging or complex text annotation.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 258,
    "question": "Which problem does 'Knowledge Distillation' primarily aim to solve?",
    "options": {
      "A": "Overfitting in small models.",
      "B": "Deploying large, complex 'teacher' models efficiently by transferring their knowledge to smaller, more efficient 'student' models.",
      "C": "The vanishing gradient problem in deep networks.",
      "D": "Generating new, synthetic data points."
    },
    "answer": "B",
    "explanation": "Distillation allows deploying more compact and faster models with comparable performance to their larger counterparts.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 259,
    "question": "What is the primary characteristic of a 'Generative Adversarial Network (GAN)'?",
    "options": {
      "A": "It performs accurate image classification.",
      "B": "It learns to generate new data instances that resemble the training data by pitting two neural networks (generator and discriminator) against each other.",
      "C": "It is a type of autoencoder used for dimensionality reduction.",
      "D": "It's specifically designed for time series forecasting."
    },
    "answer": "B",
    "explanation": "GANs are a powerful class of generative models used for tasks like image synthesis, style transfer, and data augmentation.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 260,
    "question": "In Reinforcement Learning, what is a 'Markov Decision Process (MDP)'?",
    "options": {
      "A": "A specific algorithm for training reinforcement learning agents.",
      "B": "A mathematical framework for modeling sequential decision-making in situations where outcomes are partly random and partly under the control of a decision maker.",
      "C": "A method for clustering states based on their similarity.",
      "D": "A way to visualize the agent's reward history."
    },
    "answer": "B",
    "explanation": "MDPs are a fundamental concept in reinforcement learning, providing the formal structure for defining the environment and agent interactions.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 261,
    "question": "Which statistical concept underpins the 'Central Limit Theorem'?",
    "options": {
      "A": "The distribution of individual samples.",
      "B": "The distribution of sample means approaches a normal distribution as the sample size increases, regardless of the population's distribution.",
      "C": "The variance of a single random variable.",
      "D": "The relationship between two categorical variables."
    },
    "answer": "B",
    "explanation": "The Central Limit Theorem is fundamental in statistics and machine learning for hypothesis testing and confidence interval estimation.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 262,
    "question": "When dealing with 'Multicollinearity' in linear regression, why is 'Variance Inflation Factor (VIF)' a useful diagnostic tool?",
    "options": {
      "A": "It measures the correlation between the dependent variable and each independent variable.",
      "B": "It quantifies how much the variance of an estimated regression coefficient is inflated due to collinearity with other independent variables.",
      "C": "It indicates the optimal number of features to select.",
      "D": "It is used to normalize the feature scales."
    },
    "answer": "B",
    "explanation": "High VIF values suggest severe multicollinearity, indicating that the coefficient estimates are unstable and unreliable.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 263,
    "question": "What is the purpose of 'Differential Privacy' in machine learning?",
    "options": {
      "A": "To encrypt the model's internal parameters.",
      "B": "To ensure that model training and predictions do not reveal specific information about individual data points in the training set.",
      "C": "To prevent data leakage during model deployment.",
      "D": "To make models more robust to adversarial attacks."
    },
    "answer": "B",
    "explanation": "Differential privacy adds noise to the data or algorithm during training to protect individual privacy while still allowing for useful model training.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 264,
    "question": "Which ensemble method is particularly effective for small datasets and prone to overfitting, where the base models are typically deep decision trees?",
    "options": {
      "A": "Random Forest",
      "B": "AdaBoost (weak learners)",
      "C": "Gradient Boosting (e.g., XGBoost, LightGBM)",
      "D": "Bagging (general)"
    },
    "answer": "C",
    "explanation": "Gradient Boosting, especially implementations like XGBoost, often use deep trees as base learners (strong learners) and rely on the boosting process to control overfitting, making them powerful even with smaller datasets where bagging might struggle to get enough diverse samples.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 265,
    "question": "What is 'Concept Drift' in machine learning?",
    "options": {
      "A": "A sudden change in the model's performance during training.",
      "B": "The phenomenon where the statistical properties of the target variable, or the relationship between input features and the target variable, change over time.",
      "C": "The gradual movement of model weights towards zero during regularization.",
      "D": "The process of a model adapting to new features."
    },
    "answer": "B",
    "explanation": "Concept drift is a major challenge in real-world ML systems, requiring continuous monitoring and adaptation strategies.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 266,
    "question": "When might 'Transfer Learning' be less effective or even detrimental?",
    "options": {
      "A": "When the source and target tasks are very similar.",
      "B": "When the source and target domains are significantly different or irrelevant.",
      "C": "When the target dataset is very large.",
      "D": "When the pre-trained model is very small."
    },
    "answer": "B",
    "explanation": "If the knowledge learned from the source task is not relevant to the target task, transferring it can lead to 'negative transfer' and worse performance than training from scratch.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 267,
    "question": "What is the core idea behind 'Federated Learning'?",
    "options": {
      "A": "Training a single model on a centralized server.",
      "B": "Training a machine learning model on decentralized datasets located on multiple client devices without directly sharing the raw data, only model updates.",
      "C": "Using a federated network of GPUs for faster training.",
      "D": "A method for hyperparameter optimization across distributed systems."
    },
    "answer": "B",
    "explanation": "Federated learning addresses privacy and data sovereignty concerns by keeping data localized while still benefiting from distributed model training.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 268,
    "question": "Which type of 'bias' in data refers to the overrepresentation or underrepresentation of certain groups or characteristics in the dataset compared to the real-world population?",
    "options": {
      "A": "Algorithmic bias",
      "B": "Measurement bias",
      "C": "Sampling bias (or selection bias)",
      "D": "Reporting bias"
    },
    "answer": "C",
    "explanation": "Sampling bias can lead to models that perform poorly or unfairly on underrepresented groups.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 269,
    "question": "What is 'Multi-Task Learning'?",
    "options": {
      "A": "Training separate models for each task.",
      "B": "Training a single model to perform multiple related tasks simultaneously, leveraging shared representations to improve performance on all tasks.",
      "C": "Breaking down a complex task into multiple simpler sub-tasks.",
      "D": "Using multiple CPUs for parallel task execution."
    },
    "answer": "B",
    "explanation": "Multi-task learning can lead to better generalization, especially when individual tasks have limited data, by sharing knowledge across tasks.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 270,
    "question": "In the context of 'Reinforcement Learning', what is the 'Exploration-Exploitation' trade-off?",
    "options": {
      "A": "The balance between using known good actions to maximize immediate reward vs. trying new actions to discover potentially better rewards.",
      "B": "The trade-off between model complexity and training time.",
      "C": "The balance between bias and variance in model learning.",
      "D": "The choice between supervised and unsupervised learning."
    },
    "answer": "A",
    "explanation": "This is a fundamental dilemma in RL: too much exploitation leads to suboptimal performance, too much exploration leads to inefficient learning.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 271,
    "question": "What is the 'Maximum Likelihood Estimation (MLE)' principle in machine learning and statistics?",
    "options": {
      "A": "To find the simplest model that explains the data.",
      "B": "To find the model parameters that maximize the probability of observing the given training data.",
      "C": "To minimize the mean squared error of predictions.",
      "D": "To select features that are most correlated with the target variable."
    },
    "answer": "B",
    "explanation": "MLE is a widely used method for estimating the parameters of a statistical model given observations, by finding the parameter values that make the observed data most probable.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 272,
    "question": "Which technique is most effective for 'imputation' of missing values in a complex, non-linear dataset, considering relationships between multiple variables?",
    "options": {
      "A": "Mean imputation",
      "B": "Median imputation",
      "C": "K-Nearest Neighbors (KNN) imputation or MICE (Multiple Imputation by Chained Equations)",
      "D": "Deleting rows with missing values"
    },
    "answer": "C",
    "explanation": "KNN imputation fills missing values based on the values of the K nearest neighbors. MICE builds a separate prediction model for each variable with missing values, making it more sophisticated for complex relationships.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 273,
    "question": "What is 'Curriculum Learning' in the context of deep learning?",
    "options": {
      "A": "Training a model on a fixed, large curriculum of data.",
      "B": "Training a model by starting with easier examples and gradually increasing the complexity of the training data.",
      "C": "A method to randomly select training examples.",
      "D": "Learning new skills in a structured, sequential manner."
    },
    "answer": "B",
    "explanation": "Inspired by how humans learn, curriculum learning can accelerate training and improve final model performance, especially in complex tasks.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 274,
    "question": "What is the primary motivation for using 'Probabilistic Graphical Models' (e.g., Bayesian Networks, HMMs)?",
    "options": {
      "A": "To reduce the dimensionality of high-dimensional data.",
      "B": "To model complex relationships between many variables using a graph-based representation, often involving uncertainty and probabilistic inference.",
      "C": "To classify data points into distinct categories with high accuracy.",
      "D": "To perform non-linear regression on large datasets."
    },
    "answer": "B",
    "explanation": "PGMs provide a flexible and interpretable way to represent and reason about uncertainty and dependencies in complex systems.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 275,
    "question": "When might a 'Survival Analysis' model be appropriate in machine learning?",
    "options": {
      "A": "For predicting the exact time an event will occur (e.g., when a customer will churn).",
      "B": "For modeling the time until an event occurs, especially when some observations are censored (event has not yet occurred by the end of the study).",
      "C": "For classifying whether an event will occur or not.",
      "D": "For clustering customers based on their event history."
    },
    "answer": "B",
    "explanation": "Survival analysis specifically handles censored data, where the event of interest may not have been observed for all subjects during the study period.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 276,
    "question": "What is the concept of 'Homoscedasticity' in regression analysis?",
    "options": {
      "A": "The errors (residuals) of the model have a constant variance across all levels of the independent variables.",
      "B": "The independent variables are perfectly correlated.",
      "C": "The residuals are not normally distributed.",
      "D": "The model is underfitting the data."
    },
    "answer": "A",
    "explanation": "Homoscedasticity is an assumption of linear regression. Its violation (heteroscedasticity) can lead to inefficient and biased parameter estimates.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 277,
    "question": "Which method is commonly used for 'Hyperparameter Tuning' that samples from a distribution over hyperparameters, often finding good values faster than grid search?",
    "options": {
      "A": "Grid Search",
      "B": "Manual Tuning",
      "C": "Random Search",
      "D": "Bayesian Optimization"
    },
    "answer": "C",
    "explanation": "Random search is often more efficient than grid search, especially in high-dimensional hyperparameter spaces, because it's more likely to explore different combinations of individual hyperparameters.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 278,
    "question": "What is 'Model Debugging' in machine learning?",
    "options": {
      "A": "Deploying the model to production.",
      "B": "The process of identifying and fixing errors, biases, or unexpected behaviors in a machine learning model's predictions or training process.",
      "C": "Making the model run faster.",
      "D": "Collecting more training data for the model."
    },
    "answer": "B",
    "explanation": "Debugging ML models often involves analyzing predictions, feature importances, and intermediate activations to understand 'why' a model behaves as it does.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 279,
    "question": "Which of these is a disadvantage of 'K-Means' clustering that 'DBSCAN' can overcome?",
    "options": {
      "A": "Difficulty handling large datasets.",
      "B": "Inability to find spherical clusters.",
      "C": "Requires specifying the number of clusters (K) beforehand and struggles with non-globular clusters and varying densities.",
      "D": "Sensitivity to outliers."
    },
    "answer": "C",
    "explanation": "DBSCAN doesn't require K and can discover clusters of arbitrary shapes and identify noise points.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 280,
    "question": "What is 'Synthetic Minority Oversampling Technique for Regression (SMOTER)' used for?",
    "options": {
      "A": "To create synthetic samples for the majority class in regression.",
      "B": "To handle class imbalance in classification problems.",
      "C": "To create synthetic samples for under-represented regions or values in regression tasks to balance the distribution of the target variable.",
      "D": "To convert regression problems into classification problems."
    },
    "answer": "C",
    "explanation": "SMOTER is an adaptation of SMOTE for regression problems to address imbalance or sparsity in the target variable's distribution.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 281,
    "question": "In 'Reinforcement Learning', what is the 'Bellman Equation'?",
    "options": {
      "A": "An equation used to calculate the immediate reward.",
      "B": "A fundamental equation that decomposes the value function into the immediate reward and the discounted future rewards from the next state.",
      "C": "An equation for updating policy probabilities.",
      "D": "A measure of the state-action space."
    },
    "answer": "B",
    "explanation": "The Bellman equation is a recursive relationship that describes the optimal value function, forming the basis for many dynamic programming and RL algorithms.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 282,
    "question": "What is the main idea behind 'Quantile Regression'?",
    "options": {
      "A": "To predict the mean of the dependent variable.",
      "B": "To predict specific quantiles (e.g., median, 10th percentile, 90th percentile) of the dependent variable's distribution, rather than just the mean.",
      "C": "To classify data into different quantile groups.",
      "D": "To remove outliers from the dependent variable."
    },
    "answer": "B",
    "explanation": "Quantile regression is robust to outliers and can provide a more complete picture of the relationship between variables, especially when the effect of predictors varies across different parts of the response distribution.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 283,
    "question": "What is the 'Simplicity-Complexity Trade-off' in machine learning model design?",
    "options": {
      "A": "The choice between using a CPU or GPU.",
      "B": "The balance between a model being too simple (high bias, underfitting) and too complex (high variance, overfitting).",
      "C": "The trade-off between training speed and prediction speed.",
      "D": "The choice between using labeled or unlabeled data."
    },
    "answer": "B",
    "explanation": "This trade-off is another way of looking at the bias-variance trade-off, focusing on model complexity as the driving factor.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 284,
    "question": "Which of these is a key challenge in 'Online Anomaly Detection' in streaming data?",
    "options": {
      "A": "All data is available at once.",
      "B": "The need for real-time processing and adaptation to changing data patterns (concept drift) without storing all historical data.",
      "C": "Anomalies always appear in distinct clusters.",
      "D": "The absence of any false positives."
    },
    "answer": "B",
    "explanation": "Online anomaly detection requires algorithms that can learn incrementally and adapt to evolving normal behavior to avoid flagging legitimate new patterns as anomalies.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 285,
    "question": "What is 'Kernel PCA' (KPCA)?",
    "options": {
      "A": "A linear dimensionality reduction technique.",
      "B": "A non-linear extension of PCA that maps data into a higher-dimensional space using kernel functions before performing PCA.",
      "C": "A method for clustering data in the original feature space.",
      "D": "A technique for feature selection based on kernel similarity."
    },
    "answer": "B",
    "explanation": "KPCA allows PCA to find non-linear components by implicitly transforming the data into a higher-dimensional space where it might be linearly separable.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 286,
    "question": "In 'Reinforcement Learning', what is the difference between 'Model-Based' and 'Model-Free' learning?",
    "options": {
      "A": "Model-based learns a policy, model-free learns a value function.",
      "B": "Model-based explicitly learns or approximates the environment's dynamics (transition and reward functions), while model-free learns directly from interactions without an explicit model of the environment.",
      "C": "Model-based uses neural networks, model-free uses decision trees.",
      "D": "Model-based is for continuous actions, model-free is for discrete actions."
    },
    "answer": "B",
    "explanation": "Model-based RL can be more sample-efficient but requires accurate environmental models. Model-free RL is simpler to implement but often requires more experience.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 287,
    "question": "What is the primary motivation for 'Data-Centric AI'?",
    "options": {
      "A": "To focus primarily on developing new, complex model architectures.",
      "B": "To shift focus from model-centric improvements (algorithm, hyperparameter tuning) to systematically improving the quality and consistency of the data used for training.",
      "C": "To train models exclusively on synthetic data.",
      "D": "To reduce the amount of data needed for training."
    },
    "answer": "B",
    "explanation": "Data-Centric AI emphasizes that improving data quality and consistency can often yield greater gains in model performance than solely iterating on model architectures.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 288,
    "question": "Which evaluation metric is typically used to assess the quality of 'clustering' results when true labels are available?",
    "options": {
      "A": "Accuracy",
      "B": "Silhouette Score (often used when labels are not available)",
      "C": "Adjusted Rand Index (ARI) or Normalized Mutual Information (NMI)",
      "D": "Mean Squared Error"
    },
    "answer": "C",
    "explanation": "ARI and NMI are external validation metrics that measure the agreement between the clustering results and the true class labels, adjusting for chance.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 289,
    "question": "What is 'Homomorphic Encryption' and its relevance to machine learning?",
    "options": {
      "A": "A method for compressing machine learning models.",
      "B": "A type of encryption that allows computations to be performed on encrypted data without decrypting it, enabling privacy-preserving machine learning.",
      "C": "A technique to make models more robust to adversarial attacks.",
      "D": "A method for securing model deployment endpoints."
    },
    "answer": "B",
    "explanation": "Homomorphic encryption is a powerful tool for privacy-preserving ML, enabling computations on sensitive data while it remains encrypted.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 290,
    "question": "Which of these is a common 'Sampling Bias' issue that can arise during data collection for machine learning?",
    "options": {
      "A": "The model learns too slowly.",
      "B": "The collected data does not accurately represent the real-world population or future data distributions, leading to biased model predictions.",
      "C": "Too many features are collected, causing overfitting.",
      "D": "The model's predictions are not interpretable."
    },
    "answer": "B",
    "explanation": "Sampling bias can severely compromise the generalization ability and fairness of a machine learning model.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 291,
    "question": "What is 'Adversarial Examples' in the context of Deep Learning?",
    "options": {
      "A": "Examples that are intentionally mislabeled in the training data.",
      "B": "Inputs to a machine learning model that are intentionally designed by an attacker to cause the model to make a mistake (e.g., misclassify) while being imperceptible to humans.",
      "C": "Examples used for testing the model's performance on real-world data.",
      "D": "Examples that are naturally occurring outliers in the dataset."
    },
    "answer": "B",
    "explanation": "Adversarial examples highlight the brittleness of many state-of-the-art models and are a significant concern for security-critical applications.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 292,
    "question": "When would 'Synthetic Minority Over-sampling Technique (SMOTE)' be a good choice for handling class imbalance?",
    "options": {
      "A": "When the minority class is very noisy with many outliers.",
      "B": "When you want to remove instances from the majority class.",
      "C": "When the minority class is small but well-defined, and you need to increase its representation without simply duplicating existing samples.",
      "D": "When the dataset is perfectly balanced."
    },
    "answer": "C",
    "explanation": "SMOTE creates new, distinct synthetic examples, enriching the minority class representation without causing perfect duplicates, which can help decision boundaries.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 293,
    "question": "What is the 'Permutation Test' in hypothesis testing, and how is it relevant to feature importance?",
    "options": {
      "A": "A test for normality of residuals.",
      "B": "A non-parametric test that assesses the significance of an effect (e.g., feature importance) by permuting data and comparing the observed statistic to a distribution of statistics from permuted data.",
      "C": "A test to compare the means of two independent groups.",
      "D": "A test for multicollinearity."
    },
    "answer": "B",
    "explanation": "Permutation tests are robust and model-agnostic, making them valuable for assessing the true importance of features beyond model-specific metrics.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 294,
    "question": "What is 'Lifelong Learning' (or Continual Learning) in AI?",
    "options": {
      "A": "Training a model for a very long time on a single dataset.",
      "B": "Enabling models to continuously learn new tasks and adapt to new information without forgetting previously acquired knowledge.",
      "C": "Learning exclusively in a supervised manner throughout its lifetime.",
      "D": "A method to optimize the learning rate."
    },
    "answer": "B",
    "explanation": "Lifelong learning aims to mimic human learning, where knowledge from past tasks helps in learning new tasks and is retained over time.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 295,
    "question": "Which of these is a common strategy to mitigate the 'Cold Start Problem' in Recommender Systems?",
    "options": {
      "A": "Relying solely on collaborative filtering.",
      "B": "Using content-based filtering (leveraging item attributes), hybrid approaches, or incorporating user/item demographics.",
      "C": "Ignoring new users/items until they have sufficient interaction data.",
      "D": "Only recommending popular items."
    },
    "answer": "B",
    "explanation": "To recommend for new entities, systems need to use information other than past interactions, such as item descriptions or user profiles.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 296,
    "question": "What is the primary challenge that 'Explainable AI (XAI)' seeks to address?",
    "options": {
      "A": "Improving model accuracy.",
      "B": "Making complex machine learning models understandable and transparent to humans, addressing their 'black box' nature.",
      "C": "Reducing the computational cost of training models.",
      "D": "Automating feature engineering."
    },
    "answer": "B",
    "explanation": "XAI is crucial for building trust, ensuring fairness, and enabling debugging and regulatory compliance in ML systems.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 297,
    "question": "When would 'Genetic Algorithms' (or other evolutionary algorithms) be particularly useful in machine learning?",
    "options": {
      "A": "For simple linear regression problems.",
      "B": "For optimizing hyperparameters or model architectures in complex, non-convex search spaces where traditional gradient-based methods struggle.",
      "C": "For real-time predictions on streaming data.",
      "D": "For standard supervised classification tasks with well-behaved data."
    },
    "answer": "B",
    "explanation": "Genetic algorithms are a class of optimization algorithms inspired by natural selection, capable of exploring vast search spaces for solutions to difficult problems.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 298,
    "question": "What is 'Fairness-Aware Machine Learning' primarily concerned with?",
    "options": {
      "A": "Making models easier to understand.",
      "B": "Ensuring that machine learning models do not produce discriminatory or unfair outcomes across different demographic groups or sensitive attributes.",
      "C": "Achieving the highest possible accuracy on all datasets.",
      "D": "Minimizing the training time of models."
    },
    "answer": "B",
    "explanation": "Fairness in AI is a critical area, addressing issues like disparate impact or treatment based on protected characteristics.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 299,
    "question": "Which concept describes the situation where adding more data to a training set does not significantly improve a model's performance?",
    "options": {
      "A": "Overfitting",
      "B": "Underfitting",
      "C": "Sufficient data (or hitting the 'performance plateau')",
      "D": "High variance"
    },
    "answer": "C",
    "explanation": "If a model has reached a performance plateau, it suggests that the model architecture or features might be the limiting factor, not the amount of data.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  },
  {
    "id": 300,
    "question": "What is 'Probabilistic Programming' in the context of machine learning?",
    "options": {
      "A": "A programming paradigm for writing deterministic algorithms.",
      "B": "A programming paradigm that allows users to specify probabilistic models using code and perform inference on them, often leveraging Bayesian methods.",
      "C": "A method for optimizing the execution of machine learning programs.",
      "D": "A way to convert machine learning models into executable programs."
    },
    "answer": "B",
    "explanation": "Probabilistic programming languages (PPLs) allow flexible creation of complex probabilistic models and automate the challenging task of statistical inference.",
    "topic": "Machine Learning",
    "difficulty": "Hard"
  }
]
